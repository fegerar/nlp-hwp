{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61679521",
   "metadata": {},
   "source": [
    "# Augustus: Enhancing Epigraphic Language Models with POS Tagging, Material Classification, and Generative Capabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472a1b1",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b136772-1920-4977-b5f3-506915b9810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\digil\\Anaconda3\\envs\\edr\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "AdamW,\n",
    "get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from cltk.tokenizers.lat.lat import LatinWordTokenizer as WordTokenizer # Not used in provided core logic\n",
    "from cltk.tokenizers.lat.lat import LatinPunktSentenceTokenizer as SentenceTokenizer # Not used\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, BertConfig\n",
    "import wandb\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2530293-c2ea-415c-bffc-968aeaa81522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd696b26",
   "metadata": {},
   "source": [
    "## Century Classification Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d296f5-818f-4b13-a0b8-84099ebf54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD_LABELS = {\n",
    "    \"Before_5C_BC\": 0,  # year <= -501\n",
    "    \"C5_BC\": 1,         # -500 to -401\n",
    "    \"C4_BC\": 2,         # -400 to -301\n",
    "    \"C3_BC\": 3,         # -300 to -201\n",
    "    \"C2_BC\": 4,         # -200 to -101\n",
    "    \"C1_BC\": 5,         # -100 to -1\n",
    "    \"C1_AD\": 6,         # 0 to 100 (representative_year = 0 included here)\n",
    "    \"C2_AD\": 7,         # 101 to 200\n",
    "    \"C3_AD\": 8,         # 201 to 300\n",
    "    \"C4_AD\": 9,         # 301 to 400\n",
    "    \"C5_AD\": 10,        # 401 to 500\n",
    "    \"After_5C_AD\": 11,  # year >= 501\n",
    "    \"Unknown\": -1\n",
    "}\n",
    "NUM_DATE_LABELS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b320b78",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b4e76-64c3-49e0-82a8-2581310e30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatinBERTForMultiTask(nn.Module):\n",
    "    def __init__(self, bert_path, num_date_labels, num_material_labels, num_pos_labels):\n",
    "        super(LatinBERTForMultiTask, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_path, add_pooling_layer=True)\n",
    "        config = self.bert.config\n",
    "\n",
    "        self.cls_mlm = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "        self.num_date_labels = num_date_labels\n",
    "        self.dropout_date = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier_date = nn.Linear(config.hidden_size, self.num_date_labels)\n",
    "\n",
    "        self.num_material_labels = num_material_labels\n",
    "        self.dropout_material = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier_material = nn.Linear(config.hidden_size, self.num_material_labels)\n",
    "\n",
    "        self.num_pos_labels = num_pos_labels\n",
    "        self.dropout_pos = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier_pos = nn.Linear(config.hidden_size, self.num_pos_labels)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None, \n",
    "        attention_mask=None, \n",
    "        token_type_ids=None, \n",
    "        masked_lm_labels=None, \n",
    "        date_labels=None, \n",
    "        material_labels=None,\n",
    "        pos_labels=None\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs.last_hidden_state \n",
    "        pooled_output = outputs.pooler_output     \n",
    "\n",
    "        mlm_loss, date_loss, material_loss, pos_loss = None, None, None, None\n",
    "        mlm_logits, date_logits, material_logits, pos_logits = None, None, None, None\n",
    "        loss_fct = CrossEntropyLoss() \n",
    "        loss_fct_token = CrossEntropyLoss(ignore_index=-100) \n",
    "\n",
    "        mlm_logits = self.cls_mlm(sequence_output)\n",
    "        if masked_lm_labels is not None:\n",
    "            mlm_loss = loss_fct_token(mlm_logits.view(-1, self.bert.config.vocab_size), masked_lm_labels.view(-1))\n",
    "\n",
    "        if self.num_date_labels > 0 and pooled_output is not None:\n",
    "            pooled_output_date = self.dropout_date(pooled_output)\n",
    "            date_logits = self.classifier_date(pooled_output_date)\n",
    "            if date_labels is not None:\n",
    "                date_loss = loss_fct(date_logits.view(-1, self.num_date_labels), date_labels.view(-1))\n",
    "\n",
    "        if self.num_material_labels > 0 and pooled_output is not None:\n",
    "            pooled_output_material = self.dropout_material(pooled_output)\n",
    "            material_logits = self.classifier_material(pooled_output_material)\n",
    "            if material_labels is not None:\n",
    "                material_loss = loss_fct(material_logits.view(-1, self.num_material_labels), material_labels.view(-1))\n",
    "\n",
    "        if self.num_pos_labels > 0 and sequence_output is not None:\n",
    "            sequence_output_pos = self.dropout_pos(sequence_output)\n",
    "            pos_logits = self.classifier_pos(sequence_output_pos)\n",
    "            if pos_labels is not None:\n",
    "                pos_loss = loss_fct_token(pos_logits.view(-1, self.num_pos_labels), pos_labels.view(-1))\n",
    "\n",
    "        return {\n",
    "            \"mlm_loss\": mlm_loss,\n",
    "            \"mlm_logits\": mlm_logits,\n",
    "            \"date_loss\": date_loss,\n",
    "            \"date_logits\": date_logits,\n",
    "            \"material_loss\": material_loss,\n",
    "            \"material_logits\": material_logits,\n",
    "            \"pos_loss\": pos_loss,\n",
    "            \"pos_logits\": pos_logits,\n",
    "            \"hidden_states\": outputs.hidden_states,\n",
    "            \"pooler_output\": pooled_output\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a349d5",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39654425-f4bd-4727-9e66-d2f02ed8dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatinTokenizer():\n",
    "    UNK_TOKEN_IN_CORPUS = \"<unk>\"\n",
    "    MODEL_UNK_TOKEN = \"[UNK]\"\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        self.vocab = {}\n",
    "        self.reverseVocab = {}\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.vocab[\"[PAD]\"] = 0\n",
    "        self.vocab[self.MODEL_UNK_TOKEN] = 1 \n",
    "        self.vocab[\"[CLS]\"] = 2\n",
    "        self.vocab[\"[SEP]\"] = 3\n",
    "        self.vocab[\"[MASK]\"] = 4\n",
    "\n",
    "        for key in self.encoder._subtoken_string_to_id:\n",
    "            subword_id_in_our_vocab = self.encoder._subtoken_string_to_id[key] + 5\n",
    "            self.vocab[key] = subword_id_in_our_vocab\n",
    "            self.reverseVocab[subword_id_in_our_vocab] = key\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token == \"[PAD]\":\n",
    "                ids.append(0)\n",
    "            elif token == self.MODEL_UNK_TOKEN: \n",
    "                ids.append(1)\n",
    "            elif token == \"[CLS]\":\n",
    "                ids.append(2)\n",
    "            elif token == \"[SEP]\":\n",
    "                ids.append(3)\n",
    "            elif token == \"[MASK]\":\n",
    "                ids.append(4)\n",
    "            else:\n",
    "                ids.append(self.vocab.get(token, self.vocab[self.MODEL_UNK_TOKEN]))\n",
    "        return ids\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids_list):\n",
    "        tokens = []\n",
    "        for id_val in ids_list:\n",
    "            if id_val == 0:\n",
    "                tokens.append(\"[PAD]\")\n",
    "            elif id_val == 1:\n",
    "                tokens.append(self.MODEL_UNK_TOKEN) \n",
    "            elif id_val == 2:\n",
    "                tokens.append(\"[CLS]\")\n",
    "            elif id_val == 3:\n",
    "                tokens.append(\"[SEP]\")\n",
    "            elif id_val == 4:\n",
    "                tokens.append(\"[MASK]\")\n",
    "            else:\n",
    "                tokens.append(self.reverseVocab.get(id_val, self.MODEL_UNK_TOKEN))\n",
    "        return tokens\n",
    "    \n",
    "    def tokenize_word(self, word):\n",
    "        if self.UNK_TOKEN_IN_CORPUS in word:\n",
    "            return [self.MODEL_UNK_TOKEN]\n",
    "        elif word in {\"[PAD]\", self.MODEL_UNK_TOKEN, \"[CLS]\", \"[SEP]\", \"[MASK]\"}:\n",
    "            return [word]\n",
    "        else:\n",
    "            try:\n",
    "                sub_ids_from_encoder = self.encoder.encode(word)\n",
    "                subword_strings = []\n",
    "                for sub_id in sub_ids_from_encoder:\n",
    "                    our_subword_id = sub_id + 5\n",
    "                    subword_string = self.reverseVocab.get(our_subword_id)\n",
    "                    if subword_string is not None:\n",
    "                        subword_strings.append(subword_string)\n",
    "                    else:\n",
    "                        subword_strings.append(self.MODEL_UNK_TOKEN)\n",
    "                return subword_strings\n",
    "            except Exception:\n",
    "                return [self.MODEL_UNK_TOKEN]\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        raw_words = text.split()\n",
    "        for word in raw_words:\n",
    "            tokens.extend(self.tokenize_word(word))\n",
    "        return tokens\n",
    "\n",
    "    def __call__(self, text, max_length=128, padding=True, truncation=True,\n",
    "                 return_attention_mask=True, add_special_tokens=True):\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            tokens = self.tokenize(text)\n",
    "        else:\n",
    "            tokens = list(text)\n",
    "\n",
    "        if add_special_tokens:\n",
    "            if not tokens or tokens[0] != \"[CLS]\":\n",
    "                tokens = [\"[CLS]\"] + tokens\n",
    "            if not tokens or tokens[-1] != \"[SEP]\":\n",
    "                 tokens = tokens + [\"[SEP]\"]\n",
    "\n",
    "        if truncation and len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length - 1] + [\"[SEP]\"]\n",
    "\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        attention_mask_list = [1] * len(input_ids) if return_attention_mask else None\n",
    "\n",
    "        if padding:\n",
    "            pad_length = max_length - len(input_ids)\n",
    "            if pad_length > 0 :\n",
    "                input_ids = input_ids + [self.vocab[\"[PAD]\"]] * pad_length\n",
    "                if attention_mask_list:\n",
    "                    attention_mask_list = attention_mask_list + [0] * pad_length\n",
    "            if len(input_ids) > max_length: \n",
    "                input_ids = input_ids[:max_length]\n",
    "                if attention_mask_list:\n",
    "                    attention_mask_list = attention_mask_list[:max_length]\n",
    "\n",
    "        result = {\"input_ids\": input_ids}\n",
    "        if return_attention_mask:\n",
    "            result[\"attention_mask\"] = attention_mask_list\n",
    "        return result\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "def get_period_label_for_record(record_dating_info):\n",
    "    year_from_raw = record_dating_info.get(\"year_from\")\n",
    "    year_to_raw = record_dating_info.get(\"year_to\")\n",
    "\n",
    "    try:\n",
    "        year_from = int(year_from_raw) if year_from_raw is not None and str(year_from_raw).strip() != \"\" else None\n",
    "    except ValueError: year_from = None\n",
    "    try:\n",
    "        year_to = int(year_to_raw) if year_to_raw is not None and str(year_to_raw).strip() != \"\" else None\n",
    "    except ValueError: year_to = None\n",
    "\n",
    "    representative_year = None\n",
    "    if year_from is not None and year_to is not None:\n",
    "        if year_from > year_to: return PERIOD_LABELS[\"Unknown\"]\n",
    "        representative_year = (year_from + year_to) / 2.0 \n",
    "    elif year_from is not None:\n",
    "        representative_year = float(year_from)\n",
    "    elif year_to is not None:\n",
    "        representative_year = float(year_to)\n",
    "    else:\n",
    "        return PERIOD_LABELS[\"Unknown\"]\n",
    "\n",
    "    if representative_year is None: return PERIOD_LABELS[\"Unknown\"] \n",
    "\n",
    "    if representative_year <= -501: return PERIOD_LABELS[\"Before_5C_BC\"]\n",
    "    elif -500 <= representative_year <= -401: return PERIOD_LABELS[\"C5_BC\"]\n",
    "    elif -400 <= representative_year <= -301: return PERIOD_LABELS[\"C4_BC\"]\n",
    "    elif -300 <= representative_year <= -201: return PERIOD_LABELS[\"C3_BC\"]\n",
    "    elif -200 <= representative_year <= -101: return PERIOD_LABELS[\"C2_BC\"]\n",
    "    elif -100 <= representative_year <= -1: return PERIOD_LABELS[\"C1_BC\"]\n",
    "    elif 0 <= representative_year <= 100: return PERIOD_LABELS[\"C1_AD\"] \n",
    "    elif 101 <= representative_year <= 200: return PERIOD_LABELS[\"C2_AD\"]\n",
    "    elif 201 <= representative_year <= 300: return PERIOD_LABELS[\"C3_AD\"]\n",
    "    elif 301 <= representative_year <= 400: return PERIOD_LABELS[\"C4_AD\"]\n",
    "    elif 401 <= representative_year <= 500: return PERIOD_LABELS[\"C5_AD\"]\n",
    "    elif representative_year >= 501: return PERIOD_LABELS[\"After_5C_AD\"]\n",
    "    else:\n",
    "        return PERIOD_LABELS[\"Unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8cadf",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298b24a-4623-45de-8bdd-267d3e181fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpigraphDataset(Dataset):\n",
    "    def __init__(self, source_data_file, tokenizer, max_length, max_unk_percentage, material_to_id, pos_to_id):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        self.material_to_id = material_to_id\n",
    "        self.pos_to_id = pos_to_id\n",
    "        self.max_unk_percentage = max_unk_percentage\n",
    "        self.unk_token_id = tokenizer.vocab[tokenizer.MODEL_UNK_TOKEN]\n",
    "\n",
    "        num_records_in_file, num_skipped_missing_fields = 0, 0\n",
    "        num_examples_before_filtering, num_filtered_unk = 0, 0\n",
    "        num_filtered_date, num_filtered_material, num_filtered_pos_mismatch = 0, 0, 0\n",
    "\n",
    "        print(f\"Reading and processing data from {source_data_file}...\")\n",
    "        with open(source_data_file, 'r', encoding='utf-8') as f:\n",
    "            all_data = json.load(f)\n",
    "        num_records_in_file = len(all_data)\n",
    "\n",
    "        for record in all_data:\n",
    "            record_id = record.get(\"record_number\")\n",
    "            text = record.get(\"parsed_field\", \"\").strip() \n",
    "            if not record_id or not text:\n",
    "                num_skipped_missing_fields += 1\n",
    "                continue\n",
    "            \n",
    "            num_examples_before_filtering += 1\n",
    "            \n",
    "            date_label = get_period_label_for_record(record.get(\"dating\", {}))\n",
    "            material_raw = record.get(\"material\", \"Unknown\").lower().strip()\n",
    "            material_label = self.material_to_id.get(material_raw, self.material_to_id[\"Unknown\"])\n",
    "            \n",
    "            if date_label == PERIOD_LABELS[\"Unknown\"]:\n",
    "                num_filtered_date += 1\n",
    "                continue\n",
    "\n",
    "            pos_tags_data = record.get(\"pos_tags\", [])\n",
    "            \n",
    "            subwords, pos_ids = [], []\n",
    "\n",
    "            if pos_tags_data and isinstance(pos_tags_data, list):\n",
    "                words = [item[0] for item in pos_tags_data]\n",
    "                tags = [item[1] for item in pos_tags_data]\n",
    "                \n",
    "                for i, word in enumerate(words):\n",
    "                    word_subtokens = self.tokenizer.tokenize_word(word)\n",
    "                    tag = tags[i]\n",
    "                    tag_id = self.pos_to_id.get(tag, self.pos_to_id[\"Unknown\"])\n",
    "                    subwords.extend(word_subtokens)\n",
    "                    pos_ids.extend([tag_id] * len(word_subtokens))\n",
    "            else:\n",
    "                num_filtered_pos_mismatch += 1\n",
    "                subwords = self.tokenizer.tokenize(text)\n",
    "                pos_ids = [-100] * len(subwords)\n",
    "                \n",
    "            subwords = [\"[CLS]\"] + subwords\n",
    "            pos_ids = [-100] + pos_ids\n",
    "            \n",
    "            if len(subwords) > self.max_length - 1: \n",
    "                subwords = subwords[:self.max_length - 1]\n",
    "                pos_ids = pos_ids[:self.max_length - 1]\n",
    "            \n",
    "            subwords.append(\"[SEP]\")\n",
    "            pos_ids.append(-100)\n",
    "\n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(subwords)\n",
    "\n",
    "            unk_count = input_ids.count(self.unk_token_id)\n",
    "            unk_percent = (unk_count / len(input_ids)) * 100 if input_ids else 0\n",
    "            if unk_percent > self.max_unk_percentage:\n",
    "                num_filtered_unk += 1\n",
    "                continue\n",
    "                \n",
    "            self.examples.append({\n",
    "                \"record_id\": record_id,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"date_label\": date_label,\n",
    "                \"material_label\": material_label,\n",
    "                \"pos_labels\": pos_ids\n",
    "            })\n",
    "\n",
    "        print(f\"Total records read: {num_records_in_file}\")\n",
    "        print(f\"  Skipped (no ID/text): {num_skipped_missing_fields}\")\n",
    "        print(f\"  Records before filtering: {num_examples_before_filtering}\")\n",
    "        print(f\"  Filtered (UNK > {self.max_unk_percentage}%): {num_filtered_unk}\")\n",
    "        print(f\"  Filtered (Unknown date): {num_filtered_date}\")\n",
    "        print(f\"  Records with missing/malformed 'pos_tags' field: {num_filtered_pos_mismatch}\")\n",
    "        print(f\"Loaded {len(self.examples)} examples after all filtering.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d02ec",
   "metadata": {},
   "source": [
    "## Data Collator for Multi-Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d55a75-fdc2-4b6c-aa9f-7f968abcdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatinDataCollatorForMultiTask:\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm_probability = mlm_probability\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        input_ids_list = [e[\"input_ids\"] for e in examples]\n",
    "        pos_labels_list = [e[\"pos_labels\"] for e in examples]\n",
    "        date_labels_list = [e[\"date_label\"] for e in examples]\n",
    "        material_labels_list = [e[\"material_label\"] for e in examples]\n",
    "\n",
    "        batch_input_ids = self._pad_sequences(input_ids_list, pad_value=self.tokenizer.vocab[\"[PAD]\"])\n",
    "        batch_pos_labels = self._pad_sequences(pos_labels_list, pad_value=-100) # Pad with -100 for loss ignore\n",
    "        batch_size, seq_length = batch_input_ids.size()\n",
    "\n",
    "        attention_mask = (batch_input_ids != self.tokenizer.vocab[\"[PAD]\"]).long()\n",
    "\n",
    "        mlm_labels = batch_input_ids.clone()\n",
    "        special_tokens_mask = torch.zeros_like(batch_input_ids, dtype=torch.bool)\n",
    "        for special_id in [self.tokenizer.vocab[tok] for tok in [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]]:\n",
    "            special_tokens_mask |= (batch_input_ids == special_id)\n",
    "            \n",
    "        probability_matrix = torch.full(mlm_labels.shape, self.mlm_probability)\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        mlm_labels[~masked_indices] = -100  \n",
    "\n",
    "        indices_replaced = torch.bernoulli(torch.full(mlm_labels.shape, 0.8)).bool() & masked_indices\n",
    "        batch_input_ids[indices_replaced] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        indices_random = torch.bernoulli(torch.full(mlm_labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(5, self.tokenizer.get_vocab_size(), mlm_labels.shape, dtype=torch.long)\n",
    "        batch_input_ids[indices_random] = random_words[indices_random]\n",
    "\n",
    "        batch_date_labels = torch.tensor(date_labels_list, dtype=torch.long)\n",
    "        batch_material_labels = torch.tensor(material_labels_list, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": batch_input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"masked_lm_labels\": mlm_labels,      \n",
    "            \"date_labels\": batch_date_labels,         \n",
    "            \"material_labels\": batch_material_labels, \n",
    "            \"pos_labels\": batch_pos_labels        \n",
    "        }\n",
    "\n",
    "    def _pad_sequences(self, sequences, pad_value):\n",
    "        max_len = max(len(seq) for seq in sequences) if sequences else 0\n",
    "        if max_len == 0: return torch.empty((len(sequences), 0), dtype=torch.long)\n",
    "        \n",
    "        padded_sequences = torch.full((len(sequences), max_len), pad_value, dtype=torch.long)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            padded_sequences[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
    "        return padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84557a65",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d3e61-0b4d-45f2-8414-a654a3d38628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataloader, args_config, pos_class_names, material_class_names):\n",
    "    model.eval()\n",
    "    total_eval_loss = 0.0\n",
    "    total_eval_mlm_loss, total_eval_date_loss, total_eval_material_loss, total_eval_pos_loss = 0.0, 0.0, 0.0, 0.0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    all_mlm_preds, all_mlm_labels = [], []\n",
    "    all_date_preds, all_date_labels = [], []\n",
    "    all_material_preds, all_material_labels = [], []\n",
    "    all_pos_preds, all_pos_labels = [], []\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            mlm_loss = outputs.get(\"mlm_loss\")\n",
    "            date_loss = outputs.get(\"date_loss\")\n",
    "            material_loss = outputs.get(\"material_loss\")\n",
    "            pos_loss = outputs.get(\"pos_loss\")\n",
    "\n",
    "            current_batch_loss = 0\n",
    "            if mlm_loss is not None: total_eval_mlm_loss += mlm_loss.item(); current_batch_loss += mlm_loss\n",
    "            if date_loss is not None: total_eval_date_loss += date_loss.item(); current_batch_loss += args_config[\"date_loss_weight\"] * date_loss\n",
    "            if material_loss is not None: total_eval_material_loss += material_loss.item(); current_batch_loss += args_config[\"material_loss_weight\"] * material_loss\n",
    "            if pos_loss is not None: total_eval_pos_loss += pos_loss.item(); current_batch_loss += args_config[\"pos_loss_weight\"] * pos_loss\n",
    "            total_eval_loss += current_batch_loss.item()\n",
    "\n",
    "            if outputs.get(\"mlm_logits\") is not None:\n",
    "                mask = (batch[\"masked_lm_labels\"] != -100)\n",
    "                all_mlm_preds.extend(torch.argmax(outputs[\"mlm_logits\"], dim=-1)[mask].cpu().tolist())\n",
    "                all_mlm_labels.extend(batch[\"masked_lm_labels\"][mask].cpu().tolist())\n",
    "            \n",
    "            if outputs.get(\"date_logits\") is not None: \n",
    "                all_date_preds.extend(torch.argmax(outputs[\"date_logits\"], dim=-1).cpu().tolist())\n",
    "                all_date_labels.extend(batch[\"date_labels\"].cpu().tolist())\n",
    "            \n",
    "            if outputs.get(\"material_logits\") is not None:\n",
    "                all_material_preds.extend(torch.argmax(outputs[\"material_logits\"], dim=-1).cpu().tolist())\n",
    "                all_material_labels.extend(batch[\"material_labels\"].cpu().tolist())\n",
    "\n",
    "            if outputs.get(\"pos_logits\") is not None:\n",
    "                mask = (batch[\"pos_labels\"] != -100)\n",
    "                all_pos_preds.extend(torch.argmax(outputs[\"pos_logits\"], dim=-1)[mask].cpu().tolist())\n",
    "                all_pos_labels.extend(batch[\"pos_labels\"][mask].cpu().tolist())\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    avg_loss = lambda total_loss: total_loss / nb_eval_steps if nb_eval_steps > 0 else 0\n",
    "    results = {\n",
    "        \"eval_loss_combined\": avg_loss(total_eval_loss),\n",
    "        \"eval_mlm_loss\": avg_loss(total_eval_mlm_loss),\n",
    "        \"eval_date_loss\": avg_loss(total_eval_date_loss),\n",
    "        \"eval_material_loss\": avg_loss(total_eval_material_loss),\n",
    "        \"eval_pos_loss\": avg_loss(total_eval_pos_loss),\n",
    "        \"eval_mlm_perplexity\": torch.exp(torch.tensor(avg_loss(total_eval_mlm_loss))).item()\n",
    "    }\n",
    "\n",
    "    def get_cls_metrics(labels, preds, num_labels):\n",
    "        if not labels: return 0, 0, 0, 0\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', labels=list(range(num_labels)), zero_division=0)\n",
    "        return accuracy, p, r, f1\n",
    "    \n",
    "    results[\"eval_mlm_accuracy\"] = accuracy_score(all_mlm_labels, all_mlm_preds) if all_mlm_labels else 0\n",
    "    results[\"eval_date_accuracy\"], results[\"eval_date_precision\"], results[\"eval_date_recall\"], results[\"eval_date_f1\"] = \\\n",
    "        get_cls_metrics(all_date_labels, all_date_preds, args_config[\"num_date_labels\"])\n",
    "    results[\"eval_material_accuracy\"], results[\"eval_material_precision\"], results[\"eval_material_recall\"], results[\"eval_material_f1\"] = \\\n",
    "        get_cls_metrics(all_material_labels, all_material_preds, args_config[\"num_material_labels\"])\n",
    "    results[\"eval_pos_accuracy\"], results[\"eval_pos_precision\"], results[\"eval_pos_recall\"], results[\"eval_pos_f1\"] = \\\n",
    "        get_cls_metrics(all_pos_labels, all_pos_preds, args_config[\"num_pos_labels\"])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b9413",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ff14a-93f4-4159-9a59-22da095d9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_training_example(args, tokenizer, input_ids_batch, labels_batch, logits_batch, epoch, step, global_step):\n",
    "    if not hasattr(log_training_example, \"log_file_handler\"):\n",
    "        log_file_path = \"training_examples.txt\"\n",
    "        log_training_example.log_file_handler = open(log_file_path, \"a\", encoding=\"utf-8\")\n",
    "        print(f\"Logging training examples to: {log_file_path}\")\n",
    "        if os.path.getsize(log_file_path) == 0:\n",
    "            log_training_example.log_file_handler.write(\"Epoch | Step (Batch) | Global Step | Original | Masked Input | Target | Prediction\\n\")\n",
    "            log_training_example.log_file_handler.write(\"----------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    num_examples_to_log = min(2, input_ids_batch.size(0))\n",
    "    for i in range(num_examples_to_log):\n",
    "        input_sequence_ids = input_ids_batch[i].tolist()\n",
    "        label_sequence_ids = labels_batch[i].tolist() \n",
    "        logit_sequence = logits_batch[i] \n",
    "\n",
    "        reconstructed_original_ids = []\n",
    "        for k_idx, l_id in enumerate(label_sequence_ids):\n",
    "            if l_id != -100: reconstructed_original_ids.append(l_id)\n",
    "            else: reconstructed_original_ids.append(input_sequence_ids[k_idx])\n",
    "\n",
    "        original_tokens = tokenizer.convert_ids_to_tokens(reconstructed_original_ids)\n",
    "        masked_input_tokens = tokenizer.convert_ids_to_tokens(input_sequence_ids)\n",
    "\n",
    "        log_training_example.log_file_handler.write(f\"Epoch {epoch} | Step {step} | GStep {global_step}\\n\")\n",
    "        log_training_example.log_file_handler.write(f\"  Original Approx: {' '.join(original_tokens)}\\n\")\n",
    "        log_training_example.log_file_handler.write(f\"  Masked Input:    {' '.join(masked_input_tokens)}\\n\")\n",
    "\n",
    "        masked_positions = [idx for idx, token_id in enumerate(label_sequence_ids) if token_id != -100]\n",
    "        if not masked_positions:\n",
    "            log_training_example.log_file_handler.write(\"  No tokens masked in this example for MLM prediction.\\n\")\n",
    "        else:\n",
    "            for pos in masked_positions:\n",
    "                target_token_id = label_sequence_ids[pos]\n",
    "                target_token = tokenizer.convert_ids_to_tokens([target_token_id])[0]\n",
    "                predicted_token_id = torch.argmax(logit_sequence[pos]).item()\n",
    "                predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "                top_3_tokens_ids = torch.topk(logit_sequence[pos], 3).indices.tolist()\n",
    "                top_3_tokens = tokenizer.convert_ids_to_tokens(top_3_tokens_ids)\n",
    "                log_training_example.log_file_handler.write(\n",
    "                    f\"  Pos {pos}: Target='{target_token}' ({target_token_id}), Predicted='{predicted_token}' ({predicted_token_id}), Top-3: {top_3_tokens}\\n\"\n",
    "                )\n",
    "        log_training_example.log_file_handler.write(\"----------------------------------------------------------------------------------\\n\")\n",
    "    log_training_example.log_file_handler.flush()\n",
    "\n",
    "def close_log_file():\n",
    "    if hasattr(log_training_example, \"log_file_handler\") and log_training_example.log_file_handler:\n",
    "        print(\"Closing training examples log file.\")\n",
    "        log_training_example.log_file_handler.close()\n",
    "        delattr(log_training_example, \"log_file_handler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609857f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4698d-920d-4ce6-9fcd-87e8cb0bc845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_dataset, eval_dataset, tokenizer, pos_class_names, material_class_names):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=args[\"batch_size\"], shuffle=True,\n",
    "        collate_fn=LatinDataCollatorForMultiTask(tokenizer, mlm_probability=args[\"mlm_probability\"])\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, batch_size=args[\"batch_size\"], shuffle=False,\n",
    "        collate_fn=LatinDataCollatorForMultiTask(tokenizer, mlm_probability=args[\"mlm_probability\"])\n",
    "    )\n",
    "\n",
    "    t_total = len(train_dataloader) // args.get(\"gradient_accumulation_steps\", 1) * args[\"num_train_epochs\"]\n",
    "    optimizer = AdamW(model.parameters(), lr=args[\"learning_rate\"], eps=args[\"adam_epsilon\"])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args[\"warmup_steps\"], num_training_steps=t_total)\n",
    "\n",
    "    print(\"***** Running training *****\")\n",
    "    print(f\"  Num examples = {len(train_dataset)}\")\n",
    "    print(f\"  Num Epochs = {args['num_train_epochs']}\")\n",
    "    print(f\"  Batch size = {args['batch_size']}\")\n",
    "    print(f\"  Total optimization steps = {t_total}\")\n",
    "\n",
    "    global_step = 0\n",
    "    total_train_loss_accumulator = 0.0\n",
    "    logging_loss_accumulators = defaultdict(float)\n",
    "    model.zero_grad()\n",
    "    best_eval_loss = float('inf')\n",
    "\n",
    "    for epoch in range(int(args[\"num_train_epochs\"])):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            for key, value in batch.items():\n",
    "                batch[key] = value.to(device)\n",
    "\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            mlm_loss = outputs.get(\"mlm_loss\")\n",
    "            date_loss = outputs.get(\"date_loss\")\n",
    "            material_loss = outputs.get(\"material_loss\")\n",
    "            pos_loss = outputs.get(\"pos_loss\")\n",
    "\n",
    "            combined_loss = 0\n",
    "            if mlm_loss is not None: \n",
    "                combined_loss += mlm_loss\n",
    "                logging_loss_accumulators[\"mlm\"] += mlm_loss.item()\n",
    "            if date_loss is not None: \n",
    "                weighted_loss = args[\"date_loss_weight\"] * date_loss\n",
    "                combined_loss += weighted_loss\n",
    "                logging_loss_accumulators[\"date\"] += weighted_loss.item()\n",
    "            if material_loss is not None: \n",
    "                weighted_loss = args[\"material_loss_weight\"] * material_loss\n",
    "                combined_loss += weighted_loss\n",
    "                logging_loss_accumulators[\"material\"] += weighted_loss.item()\n",
    "            if pos_loss is not None: \n",
    "                weighted_loss = args[\"pos_loss_weight\"] * pos_loss\n",
    "                combined_loss += weighted_loss\n",
    "                logging_loss_accumulators[\"pos\"] += weighted_loss.item()\n",
    "\n",
    "            if isinstance(combined_loss, torch.Tensor):\n",
    "                combined_loss.backward()\n",
    "                total_train_loss_accumulator += combined_loss.item()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            if args[\"logging_steps\"] > 0 and global_step % args[\"logging_steps\"] == 0:\n",
    "                avg_combined_loss = (total_train_loss_accumulator - getattr(train, 'last_logged_loss', 0)) / args[\"logging_steps\"]\n",
    "                train.last_logged_loss = total_train_loss_accumulator\n",
    "                \n",
    "                print(f\"Epoch {epoch}, GStep {global_step}, AvgCombinedLoss: {avg_combined_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "                for task in [\"mlm\", \"date\", \"material\", \"pos\"]:\n",
    "                    avg_loss = logging_loss_accumulators[task] / args[\"logging_steps\"]\n",
    "                    print(f\"  - Avg {task.upper()} Loss: {avg_loss:.4f}\")\n",
    "                    logging_loss_accumulators[task] = 0.0\n",
    "                \n",
    "                if outputs.get(\"mlm_logits\") is not None:\n",
    "                    log_training_example(args, tokenizer, batch[\"input_ids\"].cpu(), batch[\"masked_lm_labels\"].cpu(), outputs[\"mlm_logits\"].cpu().detach(), epoch, step, global_step)\n",
    "\n",
    "            if args[\"save_steps\"] > 0 and global_step % args[\"save_steps\"] == 0:\n",
    "                output_dir = os.path.join(args[\"output_dir\"], f\"checkpoint-{global_step}\")\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "                model.bert.config.to_json_file(os.path.join(output_dir, \"config.json\"))\n",
    "                print(f\"Saving model checkpoint to {output_dir}\")\n",
    "\n",
    "        eval_metrics = evaluate(model, eval_dataloader, args, pos_class_names, material_class_names)\n",
    "        print(f\"--- Evaluation results after epoch {epoch} ---\")\n",
    "        for key, value in eval_metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "        if eval_metrics['eval_loss_combined'] < best_eval_loss:\n",
    "            best_eval_loss = eval_metrics['eval_loss_combined']\n",
    "            best_model_dir = os.path.join(args[\"output_dir\"], \"best_model\")\n",
    "            os.makedirs(best_model_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(best_model_dir, \"pytorch_model.bin\"))\n",
    "            model.bert.config.to_json_file(os.path.join(best_model_dir, \"config.json\"))\n",
    "            print(f\"Saving best model with eval combined loss {best_eval_loss:.4f} to {best_model_dir}\")\n",
    "\n",
    "    close_log_file()\n",
    "    final_model_dir = os.path.join(args[\"output_dir\"], \"final_model\")\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(final_model_dir, \"pytorch_model.bin\"))\n",
    "    model.bert.config.to_json_file(os.path.join(final_model_dir, \"config.json\"))\n",
    "    print(f\"Saving final model to {final_model_dir}\")\n",
    "    return global_step, total_train_loss_accumulator / global_step if global_step > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1242b",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0761000-bd49-4d33-a695-7ddbf42cf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_missing_words(model, tokenizer, text, max_predictions=3):\n",
    "    model.eval()\n",
    "    model_bert_component = model.bert if hasattr(model, 'bert') else model\n",
    "    max_len = model_bert_component.config.max_position_embeddings\n",
    "\n",
    "    encoded_input = tokenizer(text, max_length=max_len, padding=True, truncation=True, return_attention_mask=True, add_special_tokens=True)\n",
    "    input_ids = torch.tensor([encoded_input[\"input_ids\"]]).to(device)\n",
    "    attention_mask = torch.tensor([encoded_input[\"attention_mask\"]]).to(device)\n",
    "    mask_token_id = tokenizer.vocab[\"[MASK]\"]\n",
    "    mask_positions = (input_ids[0] == mask_token_id).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "    if not mask_positions:\n",
    "        return \"No [MASK] tokens found in the text to predict.\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        prediction_scores = outputs[\"mlm_logits\"]\n",
    "\n",
    "    results = []\n",
    "    original_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "    for mask_pos in mask_positions:\n",
    "        logits_for_mask = prediction_scores[0, mask_pos]\n",
    "        top_k = torch.topk(logits_for_mask, max_predictions)\n",
    "        predicted_tokens = tokenizer.convert_ids_to_tokens(top_k.indices.tolist())\n",
    "        results.append({\n",
    "            \"position\": mask_pos,\n",
    "            \"predictions\": [{\"token\": tok, \"score\": score} for tok, score in zip(predicted_tokens, top_k.values.tolist())]\n",
    "        })\n",
    "    return {\"full_input\": \" \".join(original_tokens), \"predictions_for_masks\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4ab74",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2e400-41ce-42b2-b497-0b6371f88731",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"bert_path\": \"bert_model/\",\n",
    "    \"tokenizer_path\": \"latin-bert/models/subword_tokenizer_latin/latin.subword.encoder\",\n",
    "    \"source_data_file\": \"data/final_results.json\",\n",
    "    \"output_dir\": \"runs/multitask_mlm_date_material_pos\",\n",
    "\n",
    "    \"max_seq_length\": 128,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 7e-5,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"num_train_epochs\": 10.0,\n",
    "    \"warmup_steps\": 8000,\n",
    "    \"mlm_probability\": 0.15,\n",
    "    \n",
    "    \"date_loss_weight\": 0.5,\n",
    "    \"material_loss_weight\": 0.2, \n",
    "    \"pos_loss_weight\": 0.7,\n",
    "    \n",
    "    \"logging_steps\": 100,\n",
    "    \"save_steps\": 500,\n",
    "    \"seed\": 42,\n",
    "    \"eval_split\": 0.1,\n",
    "    \"max_unk_percentage\": 50.0,\n",
    "    \n",
    "    \"predict\": False,\n",
    "    \"predict_text\": \"dominus [MASK] servum et <unk> est in horto.\",\n",
    "    \n",
    "    \"num_date_labels\": NUM_DATE_LABELS,\n",
    "    \"num_material_labels\": 0,\n",
    "    \"num_pos_labels\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16d9ab",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945a9fd-e8c2-4d17-a19f-0f0c6d841611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning data/final_results.json to build vocabularies for Material and POS...\n",
      "Found 497 unique materials.\n",
      "Found 16 unique POS tags.\n",
      "WARNING:tensorflow:From c:\\Users\\digil\\Anaconda3\\envs\\edr\\lib\\site-packages\\tensor2tensor\\data_generators\\text_encoder.py:938: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "Vocabulary size (for MLM): 32900 tokens\n",
      "\n",
      "Running in TRAINING mode.\n",
      "Reading and processing data from data/final_results.json...\n",
      "Total records read: 103542\n",
      "  Skipped (no ID/text): 821\n",
      "  Records before filtering: 102721\n",
      "  Filtered (UNK > 50.0%): 114\n",
      "  Filtered (Unknown date): 12567\n",
      "  Records with missing/malformed 'pos_tags' field: 0\n",
      "Loaded 90040 examples after all filtering.\n",
      "Training on 81036 examples, evaluating on 9004 examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\digil\\Anaconda3\\envs\\edr\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 81036\n",
      "  Num Epochs = 10.0\n",
      "  Batch size = 16\n",
      "  Total optimization steps = 50650.0\n",
      "Epoch 0, GStep 100, AvgCombinedLoss: 14.8735, LR: 8.75e-07\n",
      "  - Avg MLM Loss: 10.4666\n",
      "  - Avg DATE Loss: 1.1944\n",
      "  - Avg MATERIAL Loss: 1.2471\n",
      "  - Avg POS Loss: 1.9653\n",
      "Logging training examples to: training_examples.txt\n",
      "Epoch 0, GStep 200, AvgCombinedLoss: 14.3780, LR: 1.75e-06\n",
      "  - Avg MLM Loss: 10.4041\n",
      "  - Avg DATE Loss: 1.0037\n",
      "  - Avg MATERIAL Loss: 1.2101\n",
      "  - Avg POS Loss: 1.7602\n",
      "Epoch 0, GStep 300, AvgCombinedLoss: 13.5705, LR: 2.62e-06\n",
      "  - Avg MLM Loss: 10.2423\n",
      "  - Avg DATE Loss: 0.7947\n",
      "  - Avg MATERIAL Loss: 1.0988\n",
      "  - Avg POS Loss: 1.4348\n",
      "Epoch 0, GStep 400, AvgCombinedLoss: 12.6711, LR: 3.50e-06\n",
      "  - Avg MLM Loss: 9.7703\n",
      "  - Avg DATE Loss: 0.7222\n",
      "  - Avg MATERIAL Loss: 0.8984\n",
      "  - Avg POS Loss: 1.2802\n",
      "Epoch 0, GStep 500, AvgCombinedLoss: 11.7182, LR: 4.37e-06\n",
      "  - Avg MLM Loss: 9.1706\n",
      "  - Avg DATE Loss: 0.6819\n",
      "  - Avg MATERIAL Loss: 0.6704\n",
      "  - Avg POS Loss: 1.1953\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-500\n",
      "Epoch 0, GStep 600, AvgCombinedLoss: 10.9133, LR: 5.25e-06\n",
      "  - Avg MLM Loss: 8.6204\n",
      "  - Avg DATE Loss: 0.6611\n",
      "  - Avg MATERIAL Loss: 0.5838\n",
      "  - Avg POS Loss: 1.0480\n",
      "Epoch 0, GStep 700, AvgCombinedLoss: 10.2649, LR: 6.12e-06\n",
      "  - Avg MLM Loss: 8.1021\n",
      "  - Avg DATE Loss: 0.6483\n",
      "  - Avg MATERIAL Loss: 0.5368\n",
      "  - Avg POS Loss: 0.9777\n",
      "Epoch 0, GStep 800, AvgCombinedLoss: 9.6512, LR: 7.00e-06\n",
      "  - Avg MLM Loss: 7.6846\n",
      "  - Avg DATE Loss: 0.6224\n",
      "  - Avg MATERIAL Loss: 0.4647\n",
      "  - Avg POS Loss: 0.8795\n",
      "Epoch 0, GStep 900, AvgCombinedLoss: 9.1667, LR: 7.87e-06\n",
      "  - Avg MLM Loss: 7.3252\n",
      "  - Avg DATE Loss: 0.6236\n",
      "  - Avg MATERIAL Loss: 0.4112\n",
      "  - Avg POS Loss: 0.8067\n",
      "Epoch 0, GStep 1000, AvgCombinedLoss: 8.7676, LR: 8.75e-06\n",
      "  - Avg MLM Loss: 7.0081\n",
      "  - Avg DATE Loss: 0.5940\n",
      "  - Avg MATERIAL Loss: 0.4291\n",
      "  - Avg POS Loss: 0.7365\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-1000\n",
      "Epoch 0, GStep 1100, AvgCombinedLoss: 8.3658, LR: 9.63e-06\n",
      "  - Avg MLM Loss: 6.6744\n",
      "  - Avg DATE Loss: 0.6014\n",
      "  - Avg MATERIAL Loss: 0.4089\n",
      "  - Avg POS Loss: 0.6811\n",
      "Epoch 0, GStep 1200, AvgCombinedLoss: 8.0959, LR: 1.05e-05\n",
      "  - Avg MLM Loss: 6.4995\n",
      "  - Avg DATE Loss: 0.5725\n",
      "  - Avg MATERIAL Loss: 0.3896\n",
      "  - Avg POS Loss: 0.6343\n",
      "Epoch 0, GStep 1300, AvgCombinedLoss: 7.6988, LR: 1.14e-05\n",
      "  - Avg MLM Loss: 6.1435\n",
      "  - Avg DATE Loss: 0.5768\n",
      "  - Avg MATERIAL Loss: 0.3878\n",
      "  - Avg POS Loss: 0.5908\n",
      "Epoch 0, GStep 1400, AvgCombinedLoss: 7.6536, LR: 1.22e-05\n",
      "  - Avg MLM Loss: 6.1354\n",
      "  - Avg DATE Loss: 0.5644\n",
      "  - Avg MATERIAL Loss: 0.3751\n",
      "  - Avg POS Loss: 0.5788\n",
      "Epoch 0, GStep 1500, AvgCombinedLoss: 7.2696, LR: 1.31e-05\n",
      "  - Avg MLM Loss: 5.7798\n",
      "  - Avg DATE Loss: 0.5505\n",
      "  - Avg MATERIAL Loss: 0.3838\n",
      "  - Avg POS Loss: 0.5556\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-1500\n",
      "Epoch 0, GStep 1600, AvgCombinedLoss: 7.0669, LR: 1.40e-05\n",
      "  - Avg MLM Loss: 5.6260\n",
      "  - Avg DATE Loss: 0.5381\n",
      "  - Avg MATERIAL Loss: 0.3721\n",
      "  - Avg POS Loss: 0.5307\n",
      "Epoch 0, GStep 1700, AvgCombinedLoss: 6.8511, LR: 1.49e-05\n",
      "  - Avg MLM Loss: 5.4559\n",
      "  - Avg DATE Loss: 0.5304\n",
      "  - Avg MATERIAL Loss: 0.3538\n",
      "  - Avg POS Loss: 0.5109\n",
      "Epoch 0, GStep 1800, AvgCombinedLoss: 6.6896, LR: 1.57e-05\n",
      "  - Avg MLM Loss: 5.3283\n",
      "  - Avg DATE Loss: 0.4972\n",
      "  - Avg MATERIAL Loss: 0.3601\n",
      "  - Avg POS Loss: 0.5040\n",
      "Epoch 0, GStep 1900, AvgCombinedLoss: 6.5573, LR: 1.66e-05\n",
      "  - Avg MLM Loss: 5.1931\n",
      "  - Avg DATE Loss: 0.5112\n",
      "  - Avg MATERIAL Loss: 0.3513\n",
      "  - Avg POS Loss: 0.5018\n",
      "Epoch 0, GStep 2000, AvgCombinedLoss: 6.4897, LR: 1.75e-05\n",
      "  - Avg MLM Loss: 5.1254\n",
      "  - Avg DATE Loss: 0.5235\n",
      "  - Avg MATERIAL Loss: 0.3630\n",
      "  - Avg POS Loss: 0.4778\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-2000\n",
      "Epoch 0, GStep 2100, AvgCombinedLoss: 6.3041, LR: 1.84e-05\n",
      "  - Avg MLM Loss: 4.9792\n",
      "  - Avg DATE Loss: 0.4894\n",
      "  - Avg MATERIAL Loss: 0.3497\n",
      "  - Avg POS Loss: 0.4857\n",
      "Epoch 0, GStep 2200, AvgCombinedLoss: 6.2804, LR: 1.93e-05\n",
      "  - Avg MLM Loss: 4.9483\n",
      "  - Avg DATE Loss: 0.5083\n",
      "  - Avg MATERIAL Loss: 0.3534\n",
      "  - Avg POS Loss: 0.4704\n",
      "Epoch 0, GStep 2300, AvgCombinedLoss: 6.3453, LR: 2.01e-05\n",
      "  - Avg MLM Loss: 5.0121\n",
      "  - Avg DATE Loss: 0.4874\n",
      "  - Avg MATERIAL Loss: 0.3714\n",
      "  - Avg POS Loss: 0.4744\n",
      "Epoch 0, GStep 2400, AvgCombinedLoss: 6.2842, LR: 2.10e-05\n",
      "  - Avg MLM Loss: 4.9842\n",
      "  - Avg DATE Loss: 0.5048\n",
      "  - Avg MATERIAL Loss: 0.3458\n",
      "  - Avg POS Loss: 0.4493\n",
      "Epoch 0, GStep 2500, AvgCombinedLoss: 6.0107, LR: 2.19e-05\n",
      "  - Avg MLM Loss: 4.7281\n",
      "  - Avg DATE Loss: 0.4831\n",
      "  - Avg MATERIAL Loss: 0.3511\n",
      "  - Avg POS Loss: 0.4485\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-2500\n",
      "Epoch 0, GStep 2600, AvgCombinedLoss: 5.8590, LR: 2.27e-05\n",
      "  - Avg MLM Loss: 4.6103\n",
      "  - Avg DATE Loss: 0.4840\n",
      "  - Avg MATERIAL Loss: 0.3315\n",
      "  - Avg POS Loss: 0.4331\n",
      "Epoch 0, GStep 2700, AvgCombinedLoss: 5.8649, LR: 2.36e-05\n",
      "  - Avg MLM Loss: 4.6305\n",
      "  - Avg DATE Loss: 0.4709\n",
      "  - Avg MATERIAL Loss: 0.3380\n",
      "  - Avg POS Loss: 0.4255\n",
      "Epoch 0, GStep 2800, AvgCombinedLoss: 5.7771, LR: 2.45e-05\n",
      "  - Avg MLM Loss: 4.5465\n",
      "  - Avg DATE Loss: 0.4598\n",
      "  - Avg MATERIAL Loss: 0.3406\n",
      "  - Avg POS Loss: 0.4302\n",
      "Epoch 0, GStep 2900, AvgCombinedLoss: 5.7431, LR: 2.54e-05\n",
      "  - Avg MLM Loss: 4.5540\n",
      "  - Avg DATE Loss: 0.4674\n",
      "  - Avg MATERIAL Loss: 0.3105\n",
      "  - Avg POS Loss: 0.4112\n",
      "Epoch 0, GStep 3000, AvgCombinedLoss: 5.5743, LR: 2.62e-05\n",
      "  - Avg MLM Loss: 4.3491\n",
      "  - Avg DATE Loss: 0.4686\n",
      "  - Avg MATERIAL Loss: 0.3332\n",
      "  - Avg POS Loss: 0.4233\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-3000\n",
      "Epoch 0, GStep 3100, AvgCombinedLoss: 5.7215, LR: 2.71e-05\n",
      "  - Avg MLM Loss: 4.4997\n",
      "  - Avg DATE Loss: 0.4684\n",
      "  - Avg MATERIAL Loss: 0.3337\n",
      "  - Avg POS Loss: 0.4197\n",
      "Epoch 0, GStep 3200, AvgCombinedLoss: 5.5718, LR: 2.80e-05\n",
      "  - Avg MLM Loss: 4.3438\n",
      "  - Avg DATE Loss: 0.4764\n",
      "  - Avg MATERIAL Loss: 0.3432\n",
      "  - Avg POS Loss: 0.4083\n",
      "Epoch 0, GStep 3300, AvgCombinedLoss: 5.3830, LR: 2.89e-05\n",
      "  - Avg MLM Loss: 4.1804\n",
      "  - Avg DATE Loss: 0.4745\n",
      "  - Avg MATERIAL Loss: 0.3165\n",
      "  - Avg POS Loss: 0.4116\n",
      "Epoch 0, GStep 3400, AvgCombinedLoss: 5.3406, LR: 2.97e-05\n",
      "  - Avg MLM Loss: 4.1343\n",
      "  - Avg DATE Loss: 0.4646\n",
      "  - Avg MATERIAL Loss: 0.3357\n",
      "  - Avg POS Loss: 0.4060\n",
      "Epoch 0, GStep 3500, AvgCombinedLoss: 5.2714, LR: 3.06e-05\n",
      "  - Avg MLM Loss: 4.0751\n",
      "  - Avg DATE Loss: 0.4621\n",
      "  - Avg MATERIAL Loss: 0.3407\n",
      "  - Avg POS Loss: 0.3934\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-3500\n",
      "Epoch 0, GStep 3600, AvgCombinedLoss: 5.3178, LR: 3.15e-05\n",
      "  - Avg MLM Loss: 4.1303\n",
      "  - Avg DATE Loss: 0.4614\n",
      "  - Avg MATERIAL Loss: 0.3395\n",
      "  - Avg POS Loss: 0.3866\n",
      "Epoch 0, GStep 3700, AvgCombinedLoss: 5.3517, LR: 3.24e-05\n",
      "  - Avg MLM Loss: 4.1714\n",
      "  - Avg DATE Loss: 0.4620\n",
      "  - Avg MATERIAL Loss: 0.3346\n",
      "  - Avg POS Loss: 0.3837\n",
      "Epoch 0, GStep 3800, AvgCombinedLoss: 5.2216, LR: 3.32e-05\n",
      "  - Avg MLM Loss: 4.0506\n",
      "  - Avg DATE Loss: 0.4763\n",
      "  - Avg MATERIAL Loss: 0.3103\n",
      "  - Avg POS Loss: 0.3844\n",
      "Epoch 0, GStep 3900, AvgCombinedLoss: 5.1494, LR: 3.41e-05\n",
      "  - Avg MLM Loss: 3.9875\n",
      "  - Avg DATE Loss: 0.4434\n",
      "  - Avg MATERIAL Loss: 0.3479\n",
      "  - Avg POS Loss: 0.3705\n",
      "Epoch 0, GStep 4000, AvgCombinedLoss: 5.0407, LR: 3.50e-05\n",
      "  - Avg MLM Loss: 3.8635\n",
      "  - Avg DATE Loss: 0.4666\n",
      "  - Avg MATERIAL Loss: 0.3281\n",
      "  - Avg POS Loss: 0.3825\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-4000\n",
      "Epoch 0, GStep 4100, AvgCombinedLoss: 5.0953, LR: 3.59e-05\n",
      "  - Avg MLM Loss: 3.9017\n",
      "  - Avg DATE Loss: 0.4756\n",
      "  - Avg MATERIAL Loss: 0.3345\n",
      "  - Avg POS Loss: 0.3835\n",
      "Epoch 0, GStep 4200, AvgCombinedLoss: 4.9365, LR: 3.67e-05\n",
      "  - Avg MLM Loss: 3.8061\n",
      "  - Avg DATE Loss: 0.4368\n",
      "  - Avg MATERIAL Loss: 0.3210\n",
      "  - Avg POS Loss: 0.3726\n",
      "Epoch 0, GStep 4300, AvgCombinedLoss: 4.8733, LR: 3.76e-05\n",
      "  - Avg MLM Loss: 3.7356\n",
      "  - Avg DATE Loss: 0.4487\n",
      "  - Avg MATERIAL Loss: 0.3097\n",
      "  - Avg POS Loss: 0.3793\n",
      "Epoch 0, GStep 4400, AvgCombinedLoss: 4.8375, LR: 3.85e-05\n",
      "  - Avg MLM Loss: 3.6895\n",
      "  - Avg DATE Loss: 0.4595\n",
      "  - Avg MATERIAL Loss: 0.3186\n",
      "  - Avg POS Loss: 0.3698\n",
      "Epoch 0, GStep 4500, AvgCombinedLoss: 4.8128, LR: 3.94e-05\n",
      "  - Avg MLM Loss: 3.6883\n",
      "  - Avg DATE Loss: 0.4389\n",
      "  - Avg MATERIAL Loss: 0.3133\n",
      "  - Avg POS Loss: 0.3724\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-4500\n",
      "Epoch 0, GStep 4600, AvgCombinedLoss: 4.6709, LR: 4.02e-05\n",
      "  - Avg MLM Loss: 3.5392\n",
      "  - Avg DATE Loss: 0.4556\n",
      "  - Avg MATERIAL Loss: 0.3139\n",
      "  - Avg POS Loss: 0.3623\n",
      "Epoch 0, GStep 4700, AvgCombinedLoss: 4.7470, LR: 4.11e-05\n",
      "  - Avg MLM Loss: 3.5865\n",
      "  - Avg DATE Loss: 0.4573\n",
      "  - Avg MATERIAL Loss: 0.3279\n",
      "  - Avg POS Loss: 0.3753\n",
      "Epoch 0, GStep 4800, AvgCombinedLoss: 4.6333, LR: 4.20e-05\n",
      "  - Avg MLM Loss: 3.5086\n",
      "  - Avg DATE Loss: 0.4369\n",
      "  - Avg MATERIAL Loss: 0.3237\n",
      "  - Avg POS Loss: 0.3641\n",
      "Epoch 0, GStep 4900, AvgCombinedLoss: 4.5789, LR: 4.29e-05\n",
      "  - Avg MLM Loss: 3.4807\n",
      "  - Avg DATE Loss: 0.4422\n",
      "  - Avg MATERIAL Loss: 0.3051\n",
      "  - Avg POS Loss: 0.3508\n",
      "Epoch 0, GStep 5000, AvgCombinedLoss: 4.5003, LR: 4.37e-05\n",
      "  - Avg MLM Loss: 3.3791\n",
      "  - Avg DATE Loss: 0.4247\n",
      "  - Avg MATERIAL Loss: 0.3342\n",
      "  - Avg POS Loss: 0.3623\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-5000\n",
      "--- Evaluation results after epoch 0 ---\n",
      "  eval_loss_combined: 4.4610\n",
      "  eval_mlm_loss: 3.3569\n",
      "  eval_date_loss: 0.8837\n",
      "  eval_material_loss: 1.6254\n",
      "  eval_pos_loss: 0.4817\n",
      "  eval_mlm_perplexity: 28.6991\n",
      "  eval_mlm_accuracy: 0.5152\n",
      "  eval_date_accuracy: 0.6653\n",
      "  eval_date_precision: 0.6576\n",
      "  eval_date_recall: 0.6653\n",
      "  eval_date_f1: 0.6325\n",
      "  eval_material_accuracy: 0.5667\n",
      "  eval_material_precision: 0.4290\n",
      "  eval_material_recall: 0.5667\n",
      "  eval_material_f1: 0.4651\n",
      "  eval_pos_accuracy: 0.8385\n",
      "  eval_pos_precision: 0.8411\n",
      "  eval_pos_recall: 0.8385\n",
      "  eval_pos_f1: 0.8360\n",
      "Saving best model with eval combined loss 4.4610 to runs/multitask_mlm_date_material_pos\\best_model\n",
      "Epoch 1, GStep 5100, AvgCombinedLoss: 4.6387, LR: 4.46e-05\n",
      "  - Avg MLM Loss: 3.5199\n",
      "  - Avg DATE Loss: 0.4513\n",
      "  - Avg MATERIAL Loss: 0.3124\n",
      "  - Avg POS Loss: 0.3551\n",
      "Epoch 1, GStep 5200, AvgCombinedLoss: 4.4926, LR: 4.55e-05\n",
      "  - Avg MLM Loss: 3.4043\n",
      "  - Avg DATE Loss: 0.4254\n",
      "  - Avg MATERIAL Loss: 0.3158\n",
      "  - Avg POS Loss: 0.3471\n",
      "Epoch 1, GStep 5300, AvgCombinedLoss: 4.5578, LR: 4.64e-05\n",
      "  - Avg MLM Loss: 3.4519\n",
      "  - Avg DATE Loss: 0.4436\n",
      "  - Avg MATERIAL Loss: 0.3218\n",
      "  - Avg POS Loss: 0.3405\n",
      "Epoch 1, GStep 5400, AvgCombinedLoss: 4.6041, LR: 4.72e-05\n",
      "  - Avg MLM Loss: 3.5091\n",
      "  - Avg DATE Loss: 0.4307\n",
      "  - Avg MATERIAL Loss: 0.3173\n",
      "  - Avg POS Loss: 0.3470\n",
      "Epoch 1, GStep 5500, AvgCombinedLoss: 4.4232, LR: 4.81e-05\n",
      "  - Avg MLM Loss: 3.3441\n",
      "  - Avg DATE Loss: 0.4294\n",
      "  - Avg MATERIAL Loss: 0.3112\n",
      "  - Avg POS Loss: 0.3385\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-5500\n",
      "Epoch 1, GStep 5600, AvgCombinedLoss: 4.3856, LR: 4.90e-05\n",
      "  - Avg MLM Loss: 3.2656\n",
      "  - Avg DATE Loss: 0.4440\n",
      "  - Avg MATERIAL Loss: 0.3263\n",
      "  - Avg POS Loss: 0.3498\n",
      "Epoch 1, GStep 5700, AvgCombinedLoss: 4.3104, LR: 4.99e-05\n",
      "  - Avg MLM Loss: 3.2324\n",
      "  - Avg DATE Loss: 0.4188\n",
      "  - Avg MATERIAL Loss: 0.3135\n",
      "  - Avg POS Loss: 0.3457\n",
      "Epoch 1, GStep 5800, AvgCombinedLoss: 4.3310, LR: 5.07e-05\n",
      "  - Avg MLM Loss: 3.2397\n",
      "  - Avg DATE Loss: 0.4409\n",
      "  - Avg MATERIAL Loss: 0.3099\n",
      "  - Avg POS Loss: 0.3405\n",
      "Epoch 1, GStep 5900, AvgCombinedLoss: 4.4128, LR: 5.16e-05\n",
      "  - Avg MLM Loss: 3.3529\n",
      "  - Avg DATE Loss: 0.4186\n",
      "  - Avg MATERIAL Loss: 0.3047\n",
      "  - Avg POS Loss: 0.3366\n",
      "Epoch 1, GStep 6000, AvgCombinedLoss: 4.3159, LR: 5.25e-05\n",
      "  - Avg MLM Loss: 3.2408\n",
      "  - Avg DATE Loss: 0.4347\n",
      "  - Avg MATERIAL Loss: 0.3003\n",
      "  - Avg POS Loss: 0.3401\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-6000\n",
      "Epoch 1, GStep 6100, AvgCombinedLoss: 4.2599, LR: 5.34e-05\n",
      "  - Avg MLM Loss: 3.1776\n",
      "  - Avg DATE Loss: 0.4372\n",
      "  - Avg MATERIAL Loss: 0.3173\n",
      "  - Avg POS Loss: 0.3278\n",
      "Epoch 1, GStep 6200, AvgCombinedLoss: 4.1696, LR: 5.42e-05\n",
      "  - Avg MLM Loss: 3.1125\n",
      "  - Avg DATE Loss: 0.4274\n",
      "  - Avg MATERIAL Loss: 0.2963\n",
      "  - Avg POS Loss: 0.3334\n",
      "Epoch 1, GStep 6300, AvgCombinedLoss: 4.1490, LR: 5.51e-05\n",
      "  - Avg MLM Loss: 3.1010\n",
      "  - Avg DATE Loss: 0.4091\n",
      "  - Avg MATERIAL Loss: 0.3022\n",
      "  - Avg POS Loss: 0.3367\n",
      "Epoch 1, GStep 6400, AvgCombinedLoss: 4.1643, LR: 5.60e-05\n",
      "  - Avg MLM Loss: 3.0811\n",
      "  - Avg DATE Loss: 0.4406\n",
      "  - Avg MATERIAL Loss: 0.3111\n",
      "  - Avg POS Loss: 0.3315\n",
      "Epoch 1, GStep 6500, AvgCombinedLoss: 4.1116, LR: 5.69e-05\n",
      "  - Avg MLM Loss: 3.0394\n",
      "  - Avg DATE Loss: 0.4275\n",
      "  - Avg MATERIAL Loss: 0.3060\n",
      "  - Avg POS Loss: 0.3386\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-6500\n",
      "Epoch 1, GStep 6600, AvgCombinedLoss: 4.0495, LR: 5.77e-05\n",
      "  - Avg MLM Loss: 3.0006\n",
      "  - Avg DATE Loss: 0.4081\n",
      "  - Avg MATERIAL Loss: 0.3180\n",
      "  - Avg POS Loss: 0.3229\n",
      "Epoch 1, GStep 6700, AvgCombinedLoss: 4.0821, LR: 5.86e-05\n",
      "  - Avg MLM Loss: 3.0097\n",
      "  - Avg DATE Loss: 0.4342\n",
      "  - Avg MATERIAL Loss: 0.3076\n",
      "  - Avg POS Loss: 0.3307\n",
      "Epoch 1, GStep 6800, AvgCombinedLoss: 4.0626, LR: 5.95e-05\n",
      "  - Avg MLM Loss: 2.9892\n",
      "  - Avg DATE Loss: 0.4282\n",
      "  - Avg MATERIAL Loss: 0.3131\n",
      "  - Avg POS Loss: 0.3321\n",
      "Epoch 1, GStep 6900, AvgCombinedLoss: 4.0624, LR: 6.04e-05\n",
      "  - Avg MLM Loss: 2.9879\n",
      "  - Avg DATE Loss: 0.4451\n",
      "  - Avg MATERIAL Loss: 0.3037\n",
      "  - Avg POS Loss: 0.3257\n",
      "Epoch 1, GStep 7000, AvgCombinedLoss: 4.0683, LR: 6.12e-05\n",
      "  - Avg MLM Loss: 3.0178\n",
      "  - Avg DATE Loss: 0.4247\n",
      "  - Avg MATERIAL Loss: 0.3064\n",
      "  - Avg POS Loss: 0.3194\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-7000\n",
      "Epoch 1, GStep 7100, AvgCombinedLoss: 4.0086, LR: 6.21e-05\n",
      "  - Avg MLM Loss: 2.9496\n",
      "  - Avg DATE Loss: 0.4211\n",
      "  - Avg MATERIAL Loss: 0.3114\n",
      "  - Avg POS Loss: 0.3264\n",
      "Epoch 1, GStep 7200, AvgCombinedLoss: 3.9902, LR: 6.30e-05\n",
      "  - Avg MLM Loss: 2.8848\n",
      "  - Avg DATE Loss: 0.4480\n",
      "  - Avg MATERIAL Loss: 0.3344\n",
      "  - Avg POS Loss: 0.3231\n",
      "Epoch 1, GStep 7300, AvgCombinedLoss: 3.9793, LR: 6.39e-05\n",
      "  - Avg MLM Loss: 2.9082\n",
      "  - Avg DATE Loss: 0.4391\n",
      "  - Avg MATERIAL Loss: 0.3053\n",
      "  - Avg POS Loss: 0.3267\n",
      "Epoch 1, GStep 7400, AvgCombinedLoss: 3.9767, LR: 6.48e-05\n",
      "  - Avg MLM Loss: 2.9196\n",
      "  - Avg DATE Loss: 0.4315\n",
      "  - Avg MATERIAL Loss: 0.3075\n",
      "  - Avg POS Loss: 0.3181\n",
      "Epoch 1, GStep 7500, AvgCombinedLoss: 3.8942, LR: 6.56e-05\n",
      "  - Avg MLM Loss: 2.8508\n",
      "  - Avg DATE Loss: 0.4096\n",
      "  - Avg MATERIAL Loss: 0.2998\n",
      "  - Avg POS Loss: 0.3340\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-7500\n",
      "Epoch 1, GStep 7600, AvgCombinedLoss: 4.0088, LR: 6.65e-05\n",
      "  - Avg MLM Loss: 2.9069\n",
      "  - Avg DATE Loss: 0.4476\n",
      "  - Avg MATERIAL Loss: 0.3274\n",
      "  - Avg POS Loss: 0.3270\n",
      "Epoch 1, GStep 7700, AvgCombinedLoss: 3.8275, LR: 6.74e-05\n",
      "  - Avg MLM Loss: 2.7916\n",
      "  - Avg DATE Loss: 0.3998\n",
      "  - Avg MATERIAL Loss: 0.3084\n",
      "  - Avg POS Loss: 0.3277\n",
      "Epoch 1, GStep 7800, AvgCombinedLoss: 3.9128, LR: 6.82e-05\n",
      "  - Avg MLM Loss: 2.8406\n",
      "  - Avg DATE Loss: 0.4383\n",
      "  - Avg MATERIAL Loss: 0.3110\n",
      "  - Avg POS Loss: 0.3228\n",
      "Epoch 1, GStep 7900, AvgCombinedLoss: 3.9762, LR: 6.91e-05\n",
      "  - Avg MLM Loss: 2.9149\n",
      "  - Avg DATE Loss: 0.4263\n",
      "  - Avg MATERIAL Loss: 0.3110\n",
      "  - Avg POS Loss: 0.3240\n",
      "Epoch 1, GStep 8000, AvgCombinedLoss: 3.8447, LR: 7.00e-05\n",
      "  - Avg MLM Loss: 2.7902\n",
      "  - Avg DATE Loss: 0.4172\n",
      "  - Avg MATERIAL Loss: 0.3141\n",
      "  - Avg POS Loss: 0.3232\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-8000\n",
      "Epoch 1, GStep 8100, AvgCombinedLoss: 3.8255, LR: 6.98e-05\n",
      "  - Avg MLM Loss: 2.7996\n",
      "  - Avg DATE Loss: 0.4078\n",
      "  - Avg MATERIAL Loss: 0.2921\n",
      "  - Avg POS Loss: 0.3260\n",
      "Epoch 1, GStep 8200, AvgCombinedLoss: 3.8298, LR: 6.97e-05\n",
      "  - Avg MLM Loss: 2.7634\n",
      "  - Avg DATE Loss: 0.4545\n",
      "  - Avg MATERIAL Loss: 0.3004\n",
      "  - Avg POS Loss: 0.3116\n",
      "Epoch 1, GStep 8300, AvgCombinedLoss: 3.8035, LR: 6.95e-05\n",
      "  - Avg MLM Loss: 2.7534\n",
      "  - Avg DATE Loss: 0.4353\n",
      "  - Avg MATERIAL Loss: 0.3039\n",
      "  - Avg POS Loss: 0.3109\n",
      "Epoch 1, GStep 8400, AvgCombinedLoss: 3.8092, LR: 6.93e-05\n",
      "  - Avg MLM Loss: 2.7646\n",
      "  - Avg DATE Loss: 0.4200\n",
      "  - Avg MATERIAL Loss: 0.2993\n",
      "  - Avg POS Loss: 0.3252\n",
      "Epoch 1, GStep 8500, AvgCombinedLoss: 3.7258, LR: 6.92e-05\n",
      "  - Avg MLM Loss: 2.6988\n",
      "  - Avg DATE Loss: 0.3953\n",
      "  - Avg MATERIAL Loss: 0.3176\n",
      "  - Avg POS Loss: 0.3142\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-8500\n",
      "Epoch 1, GStep 8600, AvgCombinedLoss: 3.8319, LR: 6.90e-05\n",
      "  - Avg MLM Loss: 2.7820\n",
      "  - Avg DATE Loss: 0.4339\n",
      "  - Avg MATERIAL Loss: 0.2998\n",
      "  - Avg POS Loss: 0.3162\n",
      "Epoch 1, GStep 8700, AvgCombinedLoss: 3.6943, LR: 6.89e-05\n",
      "  - Avg MLM Loss: 2.6352\n",
      "  - Avg DATE Loss: 0.4337\n",
      "  - Avg MATERIAL Loss: 0.3029\n",
      "  - Avg POS Loss: 0.3224\n",
      "Epoch 1, GStep 8800, AvgCombinedLoss: 3.8530, LR: 6.87e-05\n",
      "  - Avg MLM Loss: 2.7917\n",
      "  - Avg DATE Loss: 0.4434\n",
      "  - Avg MATERIAL Loss: 0.3012\n",
      "  - Avg POS Loss: 0.3168\n",
      "Epoch 1, GStep 8900, AvgCombinedLoss: 3.6674, LR: 6.85e-05\n",
      "  - Avg MLM Loss: 2.6487\n",
      "  - Avg DATE Loss: 0.4106\n",
      "  - Avg MATERIAL Loss: 0.2901\n",
      "  - Avg POS Loss: 0.3180\n",
      "Epoch 1, GStep 9000, AvgCombinedLoss: 3.6333, LR: 6.84e-05\n",
      "  - Avg MLM Loss: 2.6104\n",
      "  - Avg DATE Loss: 0.4119\n",
      "  - Avg MATERIAL Loss: 0.3019\n",
      "  - Avg POS Loss: 0.3091\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-9000\n",
      "Epoch 1, GStep 9100, AvgCombinedLoss: 3.6641, LR: 6.82e-05\n",
      "  - Avg MLM Loss: 2.6268\n",
      "  - Avg DATE Loss: 0.4278\n",
      "  - Avg MATERIAL Loss: 0.3018\n",
      "  - Avg POS Loss: 0.3076\n",
      "Epoch 1, GStep 9200, AvgCombinedLoss: 3.8238, LR: 6.80e-05\n",
      "  - Avg MLM Loss: 2.7847\n",
      "  - Avg DATE Loss: 0.4193\n",
      "  - Avg MATERIAL Loss: 0.3044\n",
      "  - Avg POS Loss: 0.3155\n",
      "Epoch 1, GStep 9300, AvgCombinedLoss: 3.6486, LR: 6.79e-05\n",
      "  - Avg MLM Loss: 2.6132\n",
      "  - Avg DATE Loss: 0.4299\n",
      "  - Avg MATERIAL Loss: 0.2973\n",
      "  - Avg POS Loss: 0.3082\n",
      "Epoch 1, GStep 9400, AvgCombinedLoss: 3.6222, LR: 6.77e-05\n",
      "  - Avg MLM Loss: 2.6197\n",
      "  - Avg DATE Loss: 0.4034\n",
      "  - Avg MATERIAL Loss: 0.2928\n",
      "  - Avg POS Loss: 0.3062\n",
      "Epoch 1, GStep 9500, AvgCombinedLoss: 3.7131, LR: 6.75e-05\n",
      "  - Avg MLM Loss: 2.6953\n",
      "  - Avg DATE Loss: 0.4120\n",
      "  - Avg MATERIAL Loss: 0.2912\n",
      "  - Avg POS Loss: 0.3147\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-9500\n",
      "Epoch 1, GStep 9600, AvgCombinedLoss: 3.6933, LR: 6.74e-05\n",
      "  - Avg MLM Loss: 2.6394\n",
      "  - Avg DATE Loss: 0.4269\n",
      "  - Avg MATERIAL Loss: 0.3191\n",
      "  - Avg POS Loss: 0.3079\n",
      "Epoch 1, GStep 9700, AvgCombinedLoss: 3.6800, LR: 6.72e-05\n",
      "  - Avg MLM Loss: 2.6421\n",
      "  - Avg DATE Loss: 0.4313\n",
      "  - Avg MATERIAL Loss: 0.2955\n",
      "  - Avg POS Loss: 0.3111\n",
      "Epoch 1, GStep 9800, AvgCombinedLoss: 3.6120, LR: 6.70e-05\n",
      "  - Avg MLM Loss: 2.5951\n",
      "  - Avg DATE Loss: 0.4135\n",
      "  - Avg MATERIAL Loss: 0.2920\n",
      "  - Avg POS Loss: 0.3113\n",
      "Epoch 1, GStep 9900, AvgCombinedLoss: 3.7588, LR: 6.69e-05\n",
      "  - Avg MLM Loss: 2.7128\n",
      "  - Avg DATE Loss: 0.4188\n",
      "  - Avg MATERIAL Loss: 0.3138\n",
      "  - Avg POS Loss: 0.3134\n",
      "Epoch 1, GStep 10000, AvgCombinedLoss: 3.5775, LR: 6.67e-05\n",
      "  - Avg MLM Loss: 2.5816\n",
      "  - Avg DATE Loss: 0.4048\n",
      "  - Avg MATERIAL Loss: 0.2823\n",
      "  - Avg POS Loss: 0.3087\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-10000\n",
      "Epoch 1, GStep 10100, AvgCombinedLoss: 3.6011, LR: 6.66e-05\n",
      "  - Avg MLM Loss: 2.5674\n",
      "  - Avg DATE Loss: 0.4281\n",
      "  - Avg MATERIAL Loss: 0.3042\n",
      "  - Avg POS Loss: 0.3014\n",
      "--- Evaluation results after epoch 1 ---\n",
      "  eval_loss_combined: 3.4955\n",
      "  eval_mlm_loss: 2.4937\n",
      "  eval_date_loss: 0.8244\n",
      "  eval_material_loss: 1.4918\n",
      "  eval_pos_loss: 0.4161\n",
      "  eval_mlm_perplexity: 12.1056\n",
      "  eval_mlm_accuracy: 0.5967\n",
      "  eval_date_accuracy: 0.6928\n",
      "  eval_date_precision: 0.6846\n",
      "  eval_date_recall: 0.6928\n",
      "  eval_date_f1: 0.6727\n",
      "  eval_material_accuracy: 0.6017\n",
      "  eval_material_precision: 0.4777\n",
      "  eval_material_recall: 0.6017\n",
      "  eval_material_f1: 0.5179\n",
      "  eval_pos_accuracy: 0.8583\n",
      "  eval_pos_precision: 0.8604\n",
      "  eval_pos_recall: 0.8583\n",
      "  eval_pos_f1: 0.8558\n",
      "Saving best model with eval combined loss 3.4955 to runs/multitask_mlm_date_material_pos\\best_model\n",
      "Epoch 2, GStep 10200, AvgCombinedLoss: 3.4580, LR: 6.64e-05\n",
      "  - Avg MLM Loss: 2.4797\n",
      "  - Avg DATE Loss: 0.3980\n",
      "  - Avg MATERIAL Loss: 0.2907\n",
      "  - Avg POS Loss: 0.2896\n",
      "Epoch 2, GStep 10300, AvgCombinedLoss: 3.5089, LR: 6.62e-05\n",
      "  - Avg MLM Loss: 2.5294\n",
      "  - Avg DATE Loss: 0.3946\n",
      "  - Avg MATERIAL Loss: 0.2949\n",
      "  - Avg POS Loss: 0.2899\n",
      "Epoch 2, GStep 10400, AvgCombinedLoss: 3.4740, LR: 6.61e-05\n",
      "  - Avg MLM Loss: 2.4893\n",
      "  - Avg DATE Loss: 0.4076\n",
      "  - Avg MATERIAL Loss: 0.2776\n",
      "  - Avg POS Loss: 0.2995\n",
      "Epoch 2, GStep 10500, AvgCombinedLoss: 3.4653, LR: 6.59e-05\n",
      "  - Avg MLM Loss: 2.4939\n",
      "  - Avg DATE Loss: 0.3848\n",
      "  - Avg MATERIAL Loss: 0.2890\n",
      "  - Avg POS Loss: 0.2977\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-10500\n",
      "Epoch 2, GStep 10600, AvgCombinedLoss: 3.5682, LR: 6.57e-05\n",
      "  - Avg MLM Loss: 2.5948\n",
      "  - Avg DATE Loss: 0.3848\n",
      "  - Avg MATERIAL Loss: 0.2966\n",
      "  - Avg POS Loss: 0.2921\n",
      "Epoch 2, GStep 10700, AvgCombinedLoss: 3.4353, LR: 6.56e-05\n",
      "  - Avg MLM Loss: 2.4839\n",
      "  - Avg DATE Loss: 0.3799\n",
      "  - Avg MATERIAL Loss: 0.2786\n",
      "  - Avg POS Loss: 0.2928\n",
      "Epoch 2, GStep 10800, AvgCombinedLoss: 3.4991, LR: 6.54e-05\n",
      "  - Avg MLM Loss: 2.5286\n",
      "  - Avg DATE Loss: 0.3971\n",
      "  - Avg MATERIAL Loss: 0.2731\n",
      "  - Avg POS Loss: 0.3003\n",
      "Epoch 2, GStep 10900, AvgCombinedLoss: 3.4939, LR: 6.52e-05\n",
      "  - Avg MLM Loss: 2.5357\n",
      "  - Avg DATE Loss: 0.3907\n",
      "  - Avg MATERIAL Loss: 0.2740\n",
      "  - Avg POS Loss: 0.2935\n",
      "Epoch 2, GStep 11000, AvgCombinedLoss: 3.4924, LR: 6.51e-05\n",
      "  - Avg MLM Loss: 2.5044\n",
      "  - Avg DATE Loss: 0.3940\n",
      "  - Avg MATERIAL Loss: 0.2919\n",
      "  - Avg POS Loss: 0.3020\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-11000\n",
      "Epoch 2, GStep 11100, AvgCombinedLoss: 3.4525, LR: 6.49e-05\n",
      "  - Avg MLM Loss: 2.4678\n",
      "  - Avg DATE Loss: 0.3986\n",
      "  - Avg MATERIAL Loss: 0.2899\n",
      "  - Avg POS Loss: 0.2962\n",
      "Epoch 2, GStep 11200, AvgCombinedLoss: 3.4458, LR: 6.47e-05\n",
      "  - Avg MLM Loss: 2.4650\n",
      "  - Avg DATE Loss: 0.3908\n",
      "  - Avg MATERIAL Loss: 0.2986\n",
      "  - Avg POS Loss: 0.2914\n",
      "Epoch 2, GStep 11300, AvgCombinedLoss: 3.3993, LR: 6.46e-05\n",
      "  - Avg MLM Loss: 2.4102\n",
      "  - Avg DATE Loss: 0.4104\n",
      "  - Avg MATERIAL Loss: 0.2824\n",
      "  - Avg POS Loss: 0.2963\n",
      "Epoch 2, GStep 11400, AvgCombinedLoss: 3.4442, LR: 6.44e-05\n",
      "  - Avg MLM Loss: 2.4543\n",
      "  - Avg DATE Loss: 0.4000\n",
      "  - Avg MATERIAL Loss: 0.2900\n",
      "  - Avg POS Loss: 0.2999\n",
      "Epoch 2, GStep 11500, AvgCombinedLoss: 3.4399, LR: 6.43e-05\n",
      "  - Avg MLM Loss: 2.4807\n",
      "  - Avg DATE Loss: 0.3884\n",
      "  - Avg MATERIAL Loss: 0.2777\n",
      "  - Avg POS Loss: 0.2930\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-11500\n",
      "Epoch 2, GStep 11600, AvgCombinedLoss: 3.3557, LR: 6.41e-05\n",
      "  - Avg MLM Loss: 2.4021\n",
      "  - Avg DATE Loss: 0.3959\n",
      "  - Avg MATERIAL Loss: 0.2604\n",
      "  - Avg POS Loss: 0.2973\n",
      "Epoch 2, GStep 11700, AvgCombinedLoss: 3.3934, LR: 6.39e-05\n",
      "  - Avg MLM Loss: 2.4116\n",
      "  - Avg DATE Loss: 0.3829\n",
      "  - Avg MATERIAL Loss: 0.3038\n",
      "  - Avg POS Loss: 0.2951\n",
      "Epoch 2, GStep 11800, AvgCombinedLoss: 3.4681, LR: 6.38e-05\n",
      "  - Avg MLM Loss: 2.4876\n",
      "  - Avg DATE Loss: 0.4143\n",
      "  - Avg MATERIAL Loss: 0.2710\n",
      "  - Avg POS Loss: 0.2952\n",
      "Epoch 2, GStep 11900, AvgCombinedLoss: 3.4403, LR: 6.36e-05\n",
      "  - Avg MLM Loss: 2.4513\n",
      "  - Avg DATE Loss: 0.3905\n",
      "  - Avg MATERIAL Loss: 0.3007\n",
      "  - Avg POS Loss: 0.2978\n",
      "Epoch 2, GStep 12000, AvgCombinedLoss: 3.3968, LR: 6.34e-05\n",
      "  - Avg MLM Loss: 2.4042\n",
      "  - Avg DATE Loss: 0.4003\n",
      "  - Avg MATERIAL Loss: 0.2983\n",
      "  - Avg POS Loss: 0.2939\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-12000\n",
      "Epoch 2, GStep 12100, AvgCombinedLoss: 3.3483, LR: 6.33e-05\n",
      "  - Avg MLM Loss: 2.3650\n",
      "  - Avg DATE Loss: 0.3941\n",
      "  - Avg MATERIAL Loss: 0.2953\n",
      "  - Avg POS Loss: 0.2939\n",
      "Epoch 2, GStep 12200, AvgCombinedLoss: 3.4423, LR: 6.31e-05\n",
      "  - Avg MLM Loss: 2.4625\n",
      "  - Avg DATE Loss: 0.3982\n",
      "  - Avg MATERIAL Loss: 0.2904\n",
      "  - Avg POS Loss: 0.2912\n",
      "Epoch 2, GStep 12300, AvgCombinedLoss: 3.2667, LR: 6.29e-05\n",
      "  - Avg MLM Loss: 2.3218\n",
      "  - Avg DATE Loss: 0.3655\n",
      "  - Avg MATERIAL Loss: 0.2862\n",
      "  - Avg POS Loss: 0.2932\n",
      "Epoch 2, GStep 12400, AvgCombinedLoss: 3.2849, LR: 6.28e-05\n",
      "  - Avg MLM Loss: 2.3017\n",
      "  - Avg DATE Loss: 0.4221\n",
      "  - Avg MATERIAL Loss: 0.2793\n",
      "  - Avg POS Loss: 0.2819\n",
      "Epoch 2, GStep 12500, AvgCombinedLoss: 3.3235, LR: 6.26e-05\n",
      "  - Avg MLM Loss: 2.3457\n",
      "  - Avg DATE Loss: 0.4008\n",
      "  - Avg MATERIAL Loss: 0.2964\n",
      "  - Avg POS Loss: 0.2806\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-12500\n",
      "Epoch 2, GStep 12600, AvgCombinedLoss: 3.3689, LR: 6.25e-05\n",
      "  - Avg MLM Loss: 2.3547\n",
      "  - Avg DATE Loss: 0.4131\n",
      "  - Avg MATERIAL Loss: 0.3098\n",
      "  - Avg POS Loss: 0.2913\n",
      "Epoch 2, GStep 12700, AvgCombinedLoss: 3.2372, LR: 6.23e-05\n",
      "  - Avg MLM Loss: 2.2964\n",
      "  - Avg DATE Loss: 0.3680\n",
      "  - Avg MATERIAL Loss: 0.2907\n",
      "  - Avg POS Loss: 0.2822\n",
      "Epoch 2, GStep 12800, AvgCombinedLoss: 3.3765, LR: 6.21e-05\n",
      "  - Avg MLM Loss: 2.3996\n",
      "  - Avg DATE Loss: 0.3878\n",
      "  - Avg MATERIAL Loss: 0.2928\n",
      "  - Avg POS Loss: 0.2963\n",
      "Epoch 2, GStep 12900, AvgCombinedLoss: 3.4118, LR: 6.20e-05\n",
      "  - Avg MLM Loss: 2.4465\n",
      "  - Avg DATE Loss: 0.3826\n",
      "  - Avg MATERIAL Loss: 0.2975\n",
      "  - Avg POS Loss: 0.2852\n",
      "Epoch 2, GStep 13000, AvgCombinedLoss: 3.3418, LR: 6.18e-05\n",
      "  - Avg MLM Loss: 2.3641\n",
      "  - Avg DATE Loss: 0.3884\n",
      "  - Avg MATERIAL Loss: 0.2967\n",
      "  - Avg POS Loss: 0.2926\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-13000\n",
      "Epoch 2, GStep 13100, AvgCombinedLoss: 3.3503, LR: 6.16e-05\n",
      "  - Avg MLM Loss: 2.3978\n",
      "  - Avg DATE Loss: 0.3873\n",
      "  - Avg MATERIAL Loss: 0.2797\n",
      "  - Avg POS Loss: 0.2854\n",
      "Epoch 2, GStep 13200, AvgCombinedLoss: 3.2246, LR: 6.15e-05\n",
      "  - Avg MLM Loss: 2.2590\n",
      "  - Avg DATE Loss: 0.4050\n",
      "  - Avg MATERIAL Loss: 0.2862\n",
      "  - Avg POS Loss: 0.2743\n",
      "Epoch 2, GStep 13300, AvgCombinedLoss: 3.3413, LR: 6.13e-05\n",
      "  - Avg MLM Loss: 2.3752\n",
      "  - Avg DATE Loss: 0.3961\n",
      "  - Avg MATERIAL Loss: 0.2827\n",
      "  - Avg POS Loss: 0.2872\n",
      "Epoch 2, GStep 13400, AvgCombinedLoss: 3.2647, LR: 6.11e-05\n",
      "  - Avg MLM Loss: 2.3059\n",
      "  - Avg DATE Loss: 0.3743\n",
      "  - Avg MATERIAL Loss: 0.2963\n",
      "  - Avg POS Loss: 0.2882\n",
      "Epoch 2, GStep 13500, AvgCombinedLoss: 3.3045, LR: 6.10e-05\n",
      "  - Avg MLM Loss: 2.3558\n",
      "  - Avg DATE Loss: 0.3790\n",
      "  - Avg MATERIAL Loss: 0.2862\n",
      "  - Avg POS Loss: 0.2836\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-13500\n",
      "Epoch 2, GStep 13600, AvgCombinedLoss: 3.3181, LR: 6.08e-05\n",
      "  - Avg MLM Loss: 2.3560\n",
      "  - Avg DATE Loss: 0.3900\n",
      "  - Avg MATERIAL Loss: 0.2830\n",
      "  - Avg POS Loss: 0.2892\n",
      "Epoch 2, GStep 13700, AvgCombinedLoss: 3.3475, LR: 6.06e-05\n",
      "  - Avg MLM Loss: 2.3868\n",
      "  - Avg DATE Loss: 0.3953\n",
      "  - Avg MATERIAL Loss: 0.2791\n",
      "  - Avg POS Loss: 0.2864\n",
      "Epoch 2, GStep 13800, AvgCombinedLoss: 3.2852, LR: 6.05e-05\n",
      "  - Avg MLM Loss: 2.3040\n",
      "  - Avg DATE Loss: 0.3819\n",
      "  - Avg MATERIAL Loss: 0.3081\n",
      "  - Avg POS Loss: 0.2913\n",
      "Epoch 2, GStep 13900, AvgCombinedLoss: 3.2748, LR: 6.03e-05\n",
      "  - Avg MLM Loss: 2.3015\n",
      "  - Avg DATE Loss: 0.3824\n",
      "  - Avg MATERIAL Loss: 0.2971\n",
      "  - Avg POS Loss: 0.2938\n",
      "Epoch 2, GStep 14000, AvgCombinedLoss: 3.2458, LR: 6.02e-05\n",
      "  - Avg MLM Loss: 2.2927\n",
      "  - Avg DATE Loss: 0.3855\n",
      "  - Avg MATERIAL Loss: 0.2765\n",
      "  - Avg POS Loss: 0.2911\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-14000\n",
      "Epoch 2, GStep 14100, AvgCombinedLoss: 3.3611, LR: 6.00e-05\n",
      "  - Avg MLM Loss: 2.3894\n",
      "  - Avg DATE Loss: 0.4079\n",
      "  - Avg MATERIAL Loss: 0.2820\n",
      "  - Avg POS Loss: 0.2817\n",
      "Epoch 2, GStep 14200, AvgCombinedLoss: 3.3754, LR: 5.98e-05\n",
      "  - Avg MLM Loss: 2.4084\n",
      "  - Avg DATE Loss: 0.3913\n",
      "  - Avg MATERIAL Loss: 0.2870\n",
      "  - Avg POS Loss: 0.2886\n",
      "Epoch 2, GStep 14300, AvgCombinedLoss: 3.3062, LR: 5.97e-05\n",
      "  - Avg MLM Loss: 2.3396\n",
      "  - Avg DATE Loss: 0.3929\n",
      "  - Avg MATERIAL Loss: 0.2896\n",
      "  - Avg POS Loss: 0.2841\n",
      "Epoch 2, GStep 14400, AvgCombinedLoss: 3.1852, LR: 5.95e-05\n",
      "  - Avg MLM Loss: 2.2419\n",
      "  - Avg DATE Loss: 0.3795\n",
      "  - Avg MATERIAL Loss: 0.2780\n",
      "  - Avg POS Loss: 0.2858\n",
      "Epoch 2, GStep 14500, AvgCombinedLoss: 3.2466, LR: 5.93e-05\n",
      "  - Avg MLM Loss: 2.2874\n",
      "  - Avg DATE Loss: 0.3791\n",
      "  - Avg MATERIAL Loss: 0.2957\n",
      "  - Avg POS Loss: 0.2844\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-14500\n",
      "Epoch 2, GStep 14600, AvgCombinedLoss: 3.3059, LR: 5.92e-05\n",
      "  - Avg MLM Loss: 2.3293\n",
      "  - Avg DATE Loss: 0.4076\n",
      "  - Avg MATERIAL Loss: 0.2890\n",
      "  - Avg POS Loss: 0.2800\n",
      "Epoch 2, GStep 14700, AvgCombinedLoss: 3.1476, LR: 5.90e-05\n",
      "  - Avg MLM Loss: 2.1890\n",
      "  - Avg DATE Loss: 0.3981\n",
      "  - Avg MATERIAL Loss: 0.2864\n",
      "  - Avg POS Loss: 0.2740\n",
      "Epoch 2, GStep 14800, AvgCombinedLoss: 3.2800, LR: 5.88e-05\n",
      "  - Avg MLM Loss: 2.3289\n",
      "  - Avg DATE Loss: 0.3865\n",
      "  - Avg MATERIAL Loss: 0.2834\n",
      "  - Avg POS Loss: 0.2813\n",
      "Epoch 2, GStep 14900, AvgCombinedLoss: 3.2495, LR: 5.87e-05\n",
      "  - Avg MLM Loss: 2.2642\n",
      "  - Avg DATE Loss: 0.4013\n",
      "  - Avg MATERIAL Loss: 0.2912\n",
      "  - Avg POS Loss: 0.2928\n",
      "Epoch 2, GStep 15000, AvgCombinedLoss: 3.2942, LR: 5.85e-05\n",
      "  - Avg MLM Loss: 2.3275\n",
      "  - Avg DATE Loss: 0.3952\n",
      "  - Avg MATERIAL Loss: 0.2770\n",
      "  - Avg POS Loss: 0.2946\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-15000\n",
      "Epoch 2, GStep 15100, AvgCombinedLoss: 3.0801, LR: 5.83e-05\n",
      "  - Avg MLM Loss: 2.1473\n",
      "  - Avg DATE Loss: 0.3804\n",
      "  - Avg MATERIAL Loss: 0.2715\n",
      "  - Avg POS Loss: 0.2809\n",
      "--- Evaluation results after epoch 2 ---\n",
      "  eval_loss_combined: 3.1747\n",
      "  eval_mlm_loss: 2.2090\n",
      "  eval_date_loss: 0.8002\n",
      "  eval_material_loss: 1.4430\n",
      "  eval_pos_loss: 0.3958\n",
      "  eval_mlm_perplexity: 9.1062\n",
      "  eval_mlm_accuracy: 0.6316\n",
      "  eval_date_accuracy: 0.6937\n",
      "  eval_date_precision: 0.6718\n",
      "  eval_date_recall: 0.6937\n",
      "  eval_date_f1: 0.6720\n",
      "  eval_material_accuracy: 0.6117\n",
      "  eval_material_precision: 0.4872\n",
      "  eval_material_recall: 0.6117\n",
      "  eval_material_f1: 0.5372\n",
      "  eval_pos_accuracy: 0.8649\n",
      "  eval_pos_precision: 0.8656\n",
      "  eval_pos_recall: 0.8649\n",
      "  eval_pos_f1: 0.8631\n",
      "Saving best model with eval combined loss 3.1747 to runs/multitask_mlm_date_material_pos\\best_model\n",
      "Epoch 3, GStep 15200, AvgCombinedLoss: 3.1911, LR: 5.82e-05\n",
      "  - Avg MLM Loss: 2.2435\n",
      "  - Avg DATE Loss: 0.3807\n",
      "  - Avg MATERIAL Loss: 0.2889\n",
      "  - Avg POS Loss: 0.2780\n",
      "Epoch 3, GStep 15300, AvgCombinedLoss: 2.9999, LR: 5.80e-05\n",
      "  - Avg MLM Loss: 2.1187\n",
      "  - Avg DATE Loss: 0.3393\n",
      "  - Avg MATERIAL Loss: 0.2775\n",
      "  - Avg POS Loss: 0.2644\n",
      "Epoch 3, GStep 15400, AvgCombinedLoss: 3.1783, LR: 5.79e-05\n",
      "  - Avg MLM Loss: 2.2749\n",
      "  - Avg DATE Loss: 0.3597\n",
      "  - Avg MATERIAL Loss: 0.2697\n",
      "  - Avg POS Loss: 0.2740\n",
      "Epoch 3, GStep 15500, AvgCombinedLoss: 3.1467, LR: 5.77e-05\n",
      "  - Avg MLM Loss: 2.2471\n",
      "  - Avg DATE Loss: 0.3571\n",
      "  - Avg MATERIAL Loss: 0.2755\n",
      "  - Avg POS Loss: 0.2671\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-15500\n",
      "Epoch 3, GStep 15600, AvgCombinedLoss: 3.1295, LR: 5.75e-05\n",
      "  - Avg MLM Loss: 2.2283\n",
      "  - Avg DATE Loss: 0.3469\n",
      "  - Avg MATERIAL Loss: 0.2690\n",
      "  - Avg POS Loss: 0.2853\n",
      "Epoch 3, GStep 15700, AvgCombinedLoss: 3.0791, LR: 5.74e-05\n",
      "  - Avg MLM Loss: 2.1628\n",
      "  - Avg DATE Loss: 0.3786\n",
      "  - Avg MATERIAL Loss: 0.2786\n",
      "  - Avg POS Loss: 0.2591\n",
      "Epoch 3, GStep 15800, AvgCombinedLoss: 3.1248, LR: 5.72e-05\n",
      "  - Avg MLM Loss: 2.2072\n",
      "  - Avg DATE Loss: 0.3721\n",
      "  - Avg MATERIAL Loss: 0.2787\n",
      "  - Avg POS Loss: 0.2668\n",
      "Epoch 3, GStep 15900, AvgCombinedLoss: 3.0640, LR: 5.70e-05\n",
      "  - Avg MLM Loss: 2.1458\n",
      "  - Avg DATE Loss: 0.3704\n",
      "  - Avg MATERIAL Loss: 0.2709\n",
      "  - Avg POS Loss: 0.2769\n",
      "Epoch 3, GStep 16000, AvgCombinedLoss: 3.1307, LR: 5.69e-05\n",
      "  - Avg MLM Loss: 2.2091\n",
      "  - Avg DATE Loss: 0.3716\n",
      "  - Avg MATERIAL Loss: 0.2745\n",
      "  - Avg POS Loss: 0.2755\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-16000\n",
      "Epoch 3, GStep 16100, AvgCombinedLoss: 2.9411, LR: 5.67e-05\n",
      "  - Avg MLM Loss: 2.0606\n",
      "  - Avg DATE Loss: 0.3504\n",
      "  - Avg MATERIAL Loss: 0.2696\n",
      "  - Avg POS Loss: 0.2606\n",
      "Epoch 3, GStep 16200, AvgCombinedLoss: 3.0919, LR: 5.65e-05\n",
      "  - Avg MLM Loss: 2.1645\n",
      "  - Avg DATE Loss: 0.3573\n",
      "  - Avg MATERIAL Loss: 0.2879\n",
      "  - Avg POS Loss: 0.2823\n",
      "Epoch 3, GStep 16300, AvgCombinedLoss: 3.2082, LR: 5.64e-05\n",
      "  - Avg MLM Loss: 2.3029\n",
      "  - Avg DATE Loss: 0.3633\n",
      "  - Avg MATERIAL Loss: 0.2700\n",
      "  - Avg POS Loss: 0.2720\n",
      "Epoch 3, GStep 16400, AvgCombinedLoss: 3.1320, LR: 5.62e-05\n",
      "  - Avg MLM Loss: 2.2066\n",
      "  - Avg DATE Loss: 0.3705\n",
      "  - Avg MATERIAL Loss: 0.2801\n",
      "  - Avg POS Loss: 0.2748\n",
      "Epoch 3, GStep 16500, AvgCombinedLoss: 3.0987, LR: 5.60e-05\n",
      "  - Avg MLM Loss: 2.2172\n",
      "  - Avg DATE Loss: 0.3593\n",
      "  - Avg MATERIAL Loss: 0.2560\n",
      "  - Avg POS Loss: 0.2662\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-16500\n",
      "Epoch 3, GStep 16600, AvgCombinedLoss: 2.9517, LR: 5.59e-05\n",
      "  - Avg MLM Loss: 2.0606\n",
      "  - Avg DATE Loss: 0.3445\n",
      "  - Avg MATERIAL Loss: 0.2785\n",
      "  - Avg POS Loss: 0.2680\n",
      "Epoch 3, GStep 16700, AvgCombinedLoss: 3.1684, LR: 5.57e-05\n",
      "  - Avg MLM Loss: 2.2528\n",
      "  - Avg DATE Loss: 0.3548\n",
      "  - Avg MATERIAL Loss: 0.2933\n",
      "  - Avg POS Loss: 0.2675\n",
      "Epoch 3, GStep 16800, AvgCombinedLoss: 2.9638, LR: 5.56e-05\n",
      "  - Avg MLM Loss: 2.0584\n",
      "  - Avg DATE Loss: 0.3600\n",
      "  - Avg MATERIAL Loss: 0.2715\n",
      "  - Avg POS Loss: 0.2738\n",
      "Epoch 3, GStep 16900, AvgCombinedLoss: 3.0985, LR: 5.54e-05\n",
      "  - Avg MLM Loss: 2.1816\n",
      "  - Avg DATE Loss: 0.3747\n",
      "  - Avg MATERIAL Loss: 0.2649\n",
      "  - Avg POS Loss: 0.2772\n",
      "Epoch 3, GStep 17000, AvgCombinedLoss: 2.9594, LR: 5.52e-05\n",
      "  - Avg MLM Loss: 2.0429\n",
      "  - Avg DATE Loss: 0.3743\n",
      "  - Avg MATERIAL Loss: 0.2741\n",
      "  - Avg POS Loss: 0.2682\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-17000\n",
      "Epoch 3, GStep 17100, AvgCombinedLoss: 3.1532, LR: 5.51e-05\n",
      "  - Avg MLM Loss: 2.2456\n",
      "  - Avg DATE Loss: 0.3567\n",
      "  - Avg MATERIAL Loss: 0.2796\n",
      "  - Avg POS Loss: 0.2713\n",
      "Epoch 3, GStep 17200, AvgCombinedLoss: 3.1581, LR: 5.49e-05\n",
      "  - Avg MLM Loss: 2.2047\n",
      "  - Avg DATE Loss: 0.3844\n",
      "  - Avg MATERIAL Loss: 0.2952\n",
      "  - Avg POS Loss: 0.2738\n",
      "Epoch 3, GStep 17300, AvgCombinedLoss: 3.0035, LR: 5.47e-05\n",
      "  - Avg MLM Loss: 2.1020\n",
      "  - Avg DATE Loss: 0.3600\n",
      "  - Avg MATERIAL Loss: 0.2681\n",
      "  - Avg POS Loss: 0.2734\n",
      "Epoch 3, GStep 17400, AvgCombinedLoss: 3.0473, LR: 5.46e-05\n",
      "  - Avg MLM Loss: 2.1312\n",
      "  - Avg DATE Loss: 0.3713\n",
      "  - Avg MATERIAL Loss: 0.2728\n",
      "  - Avg POS Loss: 0.2720\n",
      "Epoch 3, GStep 17500, AvgCombinedLoss: 3.0545, LR: 5.44e-05\n",
      "  - Avg MLM Loss: 2.1311\n",
      "  - Avg DATE Loss: 0.3666\n",
      "  - Avg MATERIAL Loss: 0.2853\n",
      "  - Avg POS Loss: 0.2716\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-17500\n",
      "Epoch 3, GStep 17600, AvgCombinedLoss: 3.0764, LR: 5.42e-05\n",
      "  - Avg MLM Loss: 2.1730\n",
      "  - Avg DATE Loss: 0.3534\n",
      "  - Avg MATERIAL Loss: 0.2785\n",
      "  - Avg POS Loss: 0.2714\n",
      "Epoch 3, GStep 17700, AvgCombinedLoss: 2.9623, LR: 5.41e-05\n",
      "  - Avg MLM Loss: 2.0636\n",
      "  - Avg DATE Loss: 0.3539\n",
      "  - Avg MATERIAL Loss: 0.2736\n",
      "  - Avg POS Loss: 0.2712\n",
      "Epoch 3, GStep 17800, AvgCombinedLoss: 3.1025, LR: 5.39e-05\n",
      "  - Avg MLM Loss: 2.1700\n",
      "  - Avg DATE Loss: 0.3703\n",
      "  - Avg MATERIAL Loss: 0.2933\n",
      "  - Avg POS Loss: 0.2689\n",
      "Epoch 3, GStep 17900, AvgCombinedLoss: 3.0161, LR: 5.38e-05\n",
      "  - Avg MLM Loss: 2.1032\n",
      "  - Avg DATE Loss: 0.3737\n",
      "  - Avg MATERIAL Loss: 0.2797\n",
      "  - Avg POS Loss: 0.2595\n",
      "Epoch 3, GStep 18000, AvgCombinedLoss: 3.0883, LR: 5.36e-05\n",
      "  - Avg MLM Loss: 2.2028\n",
      "  - Avg DATE Loss: 0.3516\n",
      "  - Avg MATERIAL Loss: 0.2724\n",
      "  - Avg POS Loss: 0.2614\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-18000\n",
      "Epoch 3, GStep 18100, AvgCombinedLoss: 3.1663, LR: 5.34e-05\n",
      "  - Avg MLM Loss: 2.2176\n",
      "  - Avg DATE Loss: 0.3836\n",
      "  - Avg MATERIAL Loss: 0.2925\n",
      "  - Avg POS Loss: 0.2726\n",
      "Epoch 3, GStep 18200, AvgCombinedLoss: 3.1148, LR: 5.33e-05\n",
      "  - Avg MLM Loss: 2.2061\n",
      "  - Avg DATE Loss: 0.3679\n",
      "  - Avg MATERIAL Loss: 0.2724\n",
      "  - Avg POS Loss: 0.2685\n",
      "Epoch 3, GStep 18300, AvgCombinedLoss: 3.1185, LR: 5.31e-05\n",
      "  - Avg MLM Loss: 2.2188\n",
      "  - Avg DATE Loss: 0.3568\n",
      "  - Avg MATERIAL Loss: 0.2789\n",
      "  - Avg POS Loss: 0.2639\n",
      "Epoch 3, GStep 18400, AvgCombinedLoss: 2.9695, LR: 5.29e-05\n",
      "  - Avg MLM Loss: 2.0768\n",
      "  - Avg DATE Loss: 0.3536\n",
      "  - Avg MATERIAL Loss: 0.2682\n",
      "  - Avg POS Loss: 0.2710\n",
      "Epoch 3, GStep 18500, AvgCombinedLoss: 2.9431, LR: 5.28e-05\n",
      "  - Avg MLM Loss: 2.0516\n",
      "  - Avg DATE Loss: 0.3493\n",
      "  - Avg MATERIAL Loss: 0.2743\n",
      "  - Avg POS Loss: 0.2678\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-18500\n",
      "Epoch 3, GStep 18600, AvgCombinedLoss: 3.0409, LR: 5.26e-05\n",
      "  - Avg MLM Loss: 2.1241\n",
      "  - Avg DATE Loss: 0.3595\n",
      "  - Avg MATERIAL Loss: 0.2923\n",
      "  - Avg POS Loss: 0.2650\n",
      "Epoch 3, GStep 18700, AvgCombinedLoss: 2.9907, LR: 5.24e-05\n",
      "  - Avg MLM Loss: 2.0857\n",
      "  - Avg DATE Loss: 0.3662\n",
      "  - Avg MATERIAL Loss: 0.2707\n",
      "  - Avg POS Loss: 0.2681\n",
      "Epoch 3, GStep 18800, AvgCombinedLoss: 2.9711, LR: 5.23e-05\n",
      "  - Avg MLM Loss: 2.0681\n",
      "  - Avg DATE Loss: 0.3659\n",
      "  - Avg MATERIAL Loss: 0.2730\n",
      "  - Avg POS Loss: 0.2641\n",
      "Epoch 3, GStep 18900, AvgCombinedLoss: 2.9958, LR: 5.21e-05\n",
      "  - Avg MLM Loss: 2.0875\n",
      "  - Avg DATE Loss: 0.3835\n",
      "  - Avg MATERIAL Loss: 0.2666\n",
      "  - Avg POS Loss: 0.2582\n",
      "Epoch 3, GStep 19000, AvgCombinedLoss: 3.0307, LR: 5.19e-05\n",
      "  - Avg MLM Loss: 2.1365\n",
      "  - Avg DATE Loss: 0.3571\n",
      "  - Avg MATERIAL Loss: 0.2708\n",
      "  - Avg POS Loss: 0.2663\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-19000\n",
      "Epoch 3, GStep 19100, AvgCombinedLoss: 2.9911, LR: 5.18e-05\n",
      "  - Avg MLM Loss: 2.0804\n",
      "  - Avg DATE Loss: 0.3685\n",
      "  - Avg MATERIAL Loss: 0.2727\n",
      "  - Avg POS Loss: 0.2694\n",
      "Epoch 3, GStep 19200, AvgCombinedLoss: 3.0178, LR: 5.16e-05\n",
      "  - Avg MLM Loss: 2.1093\n",
      "  - Avg DATE Loss: 0.3654\n",
      "  - Avg MATERIAL Loss: 0.2731\n",
      "  - Avg POS Loss: 0.2700\n",
      "Epoch 3, GStep 19300, AvgCombinedLoss: 3.0279, LR: 5.15e-05\n",
      "  - Avg MLM Loss: 2.1279\n",
      "  - Avg DATE Loss: 0.3534\n",
      "  - Avg MATERIAL Loss: 0.2782\n",
      "  - Avg POS Loss: 0.2684\n",
      "Epoch 3, GStep 19400, AvgCombinedLoss: 2.9241, LR: 5.13e-05\n",
      "  - Avg MLM Loss: 2.0427\n",
      "  - Avg DATE Loss: 0.3580\n",
      "  - Avg MATERIAL Loss: 0.2611\n",
      "  - Avg POS Loss: 0.2623\n",
      "Epoch 3, GStep 19500, AvgCombinedLoss: 3.0392, LR: 5.11e-05\n",
      "  - Avg MLM Loss: 2.0996\n",
      "  - Avg DATE Loss: 0.3780\n",
      "  - Avg MATERIAL Loss: 0.2849\n",
      "  - Avg POS Loss: 0.2766\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-19500\n",
      "Epoch 3, GStep 19600, AvgCombinedLoss: 2.8391, LR: 5.10e-05\n",
      "  - Avg MLM Loss: 1.9536\n",
      "  - Avg DATE Loss: 0.3577\n",
      "  - Avg MATERIAL Loss: 0.2641\n",
      "  - Avg POS Loss: 0.2638\n",
      "Epoch 3, GStep 19700, AvgCombinedLoss: 2.9432, LR: 5.08e-05\n",
      "  - Avg MLM Loss: 2.0499\n",
      "  - Avg DATE Loss: 0.3401\n",
      "  - Avg MATERIAL Loss: 0.2753\n",
      "  - Avg POS Loss: 0.2779\n",
      "Epoch 3, GStep 19800, AvgCombinedLoss: 3.0738, LR: 5.06e-05\n",
      "  - Avg MLM Loss: 2.1699\n",
      "  - Avg DATE Loss: 0.3644\n",
      "  - Avg MATERIAL Loss: 0.2715\n",
      "  - Avg POS Loss: 0.2680\n",
      "Epoch 3, GStep 19900, AvgCombinedLoss: 2.9704, LR: 5.05e-05\n",
      "  - Avg MLM Loss: 2.0594\n",
      "  - Avg DATE Loss: 0.3612\n",
      "  - Avg MATERIAL Loss: 0.2713\n",
      "  - Avg POS Loss: 0.2785\n",
      "Epoch 3, GStep 20000, AvgCombinedLoss: 2.9570, LR: 5.03e-05\n",
      "  - Avg MLM Loss: 2.0623\n",
      "  - Avg DATE Loss: 0.3582\n",
      "  - Avg MATERIAL Loss: 0.2709\n",
      "  - Avg POS Loss: 0.2655\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-20000\n",
      "Epoch 3, GStep 20100, AvgCombinedLoss: 2.8917, LR: 5.01e-05\n",
      "  - Avg MLM Loss: 1.9879\n",
      "  - Avg DATE Loss: 0.3671\n",
      "  - Avg MATERIAL Loss: 0.2656\n",
      "  - Avg POS Loss: 0.2711\n",
      "Epoch 3, GStep 20200, AvgCombinedLoss: 2.8225, LR: 5.00e-05\n",
      "  - Avg MLM Loss: 1.9390\n",
      "  - Avg DATE Loss: 0.3504\n",
      "  - Avg MATERIAL Loss: 0.2728\n",
      "  - Avg POS Loss: 0.2603\n",
      "--- Evaluation results after epoch 3 ---\n",
      "  eval_loss_combined: 3.0137\n",
      "  eval_mlm_loss: 2.0799\n",
      "  eval_date_loss: 0.7823\n",
      "  eval_material_loss: 1.4274\n",
      "  eval_pos_loss: 0.3674\n",
      "  eval_mlm_perplexity: 8.0040\n",
      "  eval_mlm_accuracy: 0.6496\n",
      "  eval_date_accuracy: 0.7058\n",
      "  eval_date_precision: 0.6920\n",
      "  eval_date_recall: 0.7058\n",
      "  eval_date_f1: 0.6906\n",
      "  eval_material_accuracy: 0.6113\n",
      "  eval_material_precision: 0.4884\n",
      "  eval_material_recall: 0.6113\n",
      "  eval_material_f1: 0.5311\n",
      "  eval_pos_accuracy: 0.8750\n",
      "  eval_pos_precision: 0.8756\n",
      "  eval_pos_recall: 0.8750\n",
      "  eval_pos_f1: 0.8729\n",
      "Saving best model with eval combined loss 3.0137 to runs/multitask_mlm_date_material_pos\\best_model\n",
      "Epoch 4, GStep 20300, AvgCombinedLoss: 2.9055, LR: 4.98e-05\n",
      "  - Avg MLM Loss: 2.0101\n",
      "  - Avg DATE Loss: 0.3596\n",
      "  - Avg MATERIAL Loss: 0.2782\n",
      "  - Avg POS Loss: 0.2577\n",
      "Epoch 4, GStep 20400, AvgCombinedLoss: 2.9053, LR: 4.96e-05\n",
      "  - Avg MLM Loss: 2.0693\n",
      "  - Avg DATE Loss: 0.3191\n",
      "  - Avg MATERIAL Loss: 0.2544\n",
      "  - Avg POS Loss: 0.2625\n",
      "Epoch 4, GStep 20500, AvgCombinedLoss: 2.9229, LR: 4.95e-05\n",
      "  - Avg MLM Loss: 2.0835\n",
      "  - Avg DATE Loss: 0.3270\n",
      "  - Avg MATERIAL Loss: 0.2602\n",
      "  - Avg POS Loss: 0.2522\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-20500\n",
      "Epoch 4, GStep 20600, AvgCombinedLoss: 2.8455, LR: 4.93e-05\n",
      "  - Avg MLM Loss: 1.9778\n",
      "  - Avg DATE Loss: 0.3447\n",
      "  - Avg MATERIAL Loss: 0.2631\n",
      "  - Avg POS Loss: 0.2599\n",
      "Epoch 4, GStep 20700, AvgCombinedLoss: 2.8736, LR: 4.92e-05\n",
      "  - Avg MLM Loss: 2.0388\n",
      "  - Avg DATE Loss: 0.3199\n",
      "  - Avg MATERIAL Loss: 0.2668\n",
      "  - Avg POS Loss: 0.2481\n",
      "Epoch 4, GStep 20800, AvgCombinedLoss: 2.9819, LR: 4.90e-05\n",
      "  - Avg MLM Loss: 2.1158\n",
      "  - Avg DATE Loss: 0.3411\n",
      "  - Avg MATERIAL Loss: 0.2661\n",
      "  - Avg POS Loss: 0.2589\n",
      "Epoch 4, GStep 20900, AvgCombinedLoss: 2.8647, LR: 4.88e-05\n",
      "  - Avg MLM Loss: 2.0121\n",
      "  - Avg DATE Loss: 0.3336\n",
      "  - Avg MATERIAL Loss: 0.2699\n",
      "  - Avg POS Loss: 0.2492\n",
      "Epoch 4, GStep 21000, AvgCombinedLoss: 2.8371, LR: 4.87e-05\n",
      "  - Avg MLM Loss: 2.0057\n",
      "  - Avg DATE Loss: 0.3072\n",
      "  - Avg MATERIAL Loss: 0.2705\n",
      "  - Avg POS Loss: 0.2536\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-21000\n",
      "Epoch 4, GStep 21100, AvgCombinedLoss: 3.0475, LR: 4.85e-05\n",
      "  - Avg MLM Loss: 2.1872\n",
      "  - Avg DATE Loss: 0.3359\n",
      "  - Avg MATERIAL Loss: 0.2718\n",
      "  - Avg POS Loss: 0.2526\n",
      "Epoch 4, GStep 21200, AvgCombinedLoss: 2.8334, LR: 4.83e-05\n",
      "  - Avg MLM Loss: 1.9793\n",
      "  - Avg DATE Loss: 0.3364\n",
      "  - Avg MATERIAL Loss: 0.2668\n",
      "  - Avg POS Loss: 0.2509\n",
      "Epoch 4, GStep 21300, AvgCombinedLoss: 2.8395, LR: 4.82e-05\n",
      "  - Avg MLM Loss: 2.0043\n",
      "  - Avg DATE Loss: 0.3245\n",
      "  - Avg MATERIAL Loss: 0.2672\n",
      "  - Avg POS Loss: 0.2436\n",
      "Epoch 4, GStep 21400, AvgCombinedLoss: 2.8861, LR: 4.80e-05\n",
      "  - Avg MLM Loss: 2.0153\n",
      "  - Avg DATE Loss: 0.3369\n",
      "  - Avg MATERIAL Loss: 0.2759\n",
      "  - Avg POS Loss: 0.2579\n",
      "Epoch 4, GStep 21500, AvgCombinedLoss: 2.8524, LR: 4.78e-05\n",
      "  - Avg MLM Loss: 2.0070\n",
      "  - Avg DATE Loss: 0.3197\n",
      "  - Avg MATERIAL Loss: 0.2712\n",
      "  - Avg POS Loss: 0.2546\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-21500\n",
      "Epoch 4, GStep 21600, AvgCombinedLoss: 2.9207, LR: 4.77e-05\n",
      "  - Avg MLM Loss: 2.0404\n",
      "  - Avg DATE Loss: 0.3367\n",
      "  - Avg MATERIAL Loss: 0.2842\n",
      "  - Avg POS Loss: 0.2594\n",
      "Epoch 4, GStep 21700, AvgCombinedLoss: 2.8165, LR: 4.75e-05\n",
      "  - Avg MLM Loss: 1.9632\n",
      "  - Avg DATE Loss: 0.3381\n",
      "  - Avg MATERIAL Loss: 0.2632\n",
      "  - Avg POS Loss: 0.2521\n",
      "Epoch 4, GStep 21800, AvgCombinedLoss: 2.8769, LR: 4.74e-05\n",
      "  - Avg MLM Loss: 1.9971\n",
      "  - Avg DATE Loss: 0.3507\n",
      "  - Avg MATERIAL Loss: 0.2721\n",
      "  - Avg POS Loss: 0.2571\n",
      "Epoch 4, GStep 21900, AvgCombinedLoss: 3.0102, LR: 4.72e-05\n",
      "  - Avg MLM Loss: 2.1282\n",
      "  - Avg DATE Loss: 0.3433\n",
      "  - Avg MATERIAL Loss: 0.2749\n",
      "  - Avg POS Loss: 0.2638\n",
      "Epoch 4, GStep 22000, AvgCombinedLoss: 2.9623, LR: 4.70e-05\n",
      "  - Avg MLM Loss: 2.0737\n",
      "  - Avg DATE Loss: 0.3352\n",
      "  - Avg MATERIAL Loss: 0.3001\n",
      "  - Avg POS Loss: 0.2533\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-22000\n",
      "Epoch 4, GStep 22100, AvgCombinedLoss: 2.7600, LR: 4.69e-05\n",
      "  - Avg MLM Loss: 1.9222\n",
      "  - Avg DATE Loss: 0.3120\n",
      "  - Avg MATERIAL Loss: 0.2713\n",
      "  - Avg POS Loss: 0.2545\n",
      "Epoch 4, GStep 22200, AvgCombinedLoss: 2.9254, LR: 4.67e-05\n",
      "  - Avg MLM Loss: 2.0464\n",
      "  - Avg DATE Loss: 0.3375\n",
      "  - Avg MATERIAL Loss: 0.2879\n",
      "  - Avg POS Loss: 0.2537\n",
      "Epoch 4, GStep 22300, AvgCombinedLoss: 2.8598, LR: 4.65e-05\n",
      "  - Avg MLM Loss: 2.0019\n",
      "  - Avg DATE Loss: 0.3479\n",
      "  - Avg MATERIAL Loss: 0.2574\n",
      "  - Avg POS Loss: 0.2526\n",
      "Epoch 4, GStep 22400, AvgCombinedLoss: 2.8725, LR: 4.64e-05\n",
      "  - Avg MLM Loss: 2.0070\n",
      "  - Avg DATE Loss: 0.3375\n",
      "  - Avg MATERIAL Loss: 0.2712\n",
      "  - Avg POS Loss: 0.2569\n",
      "Epoch 4, GStep 22500, AvgCombinedLoss: 2.8906, LR: 4.62e-05\n",
      "  - Avg MLM Loss: 1.9989\n",
      "  - Avg DATE Loss: 0.3585\n",
      "  - Avg MATERIAL Loss: 0.2769\n",
      "  - Avg POS Loss: 0.2562\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-22500\n",
      "Epoch 4, GStep 22600, AvgCombinedLoss: 2.8305, LR: 4.60e-05\n",
      "  - Avg MLM Loss: 1.9649\n",
      "  - Avg DATE Loss: 0.3471\n",
      "  - Avg MATERIAL Loss: 0.2668\n",
      "  - Avg POS Loss: 0.2518\n",
      "Epoch 4, GStep 22700, AvgCombinedLoss: 2.8347, LR: 4.59e-05\n",
      "  - Avg MLM Loss: 1.9850\n",
      "  - Avg DATE Loss: 0.3308\n",
      "  - Avg MATERIAL Loss: 0.2653\n",
      "  - Avg POS Loss: 0.2536\n",
      "Epoch 4, GStep 22800, AvgCombinedLoss: 2.7950, LR: 4.57e-05\n",
      "  - Avg MLM Loss: 1.9381\n",
      "  - Avg DATE Loss: 0.3424\n",
      "  - Avg MATERIAL Loss: 0.2672\n",
      "  - Avg POS Loss: 0.2473\n",
      "Epoch 4, GStep 22900, AvgCombinedLoss: 2.7680, LR: 4.55e-05\n",
      "  - Avg MLM Loss: 1.9198\n",
      "  - Avg DATE Loss: 0.3341\n",
      "  - Avg MATERIAL Loss: 0.2571\n",
      "  - Avg POS Loss: 0.2569\n",
      "Epoch 4, GStep 23000, AvgCombinedLoss: 2.8044, LR: 4.54e-05\n",
      "  - Avg MLM Loss: 1.9553\n",
      "  - Avg DATE Loss: 0.3324\n",
      "  - Avg MATERIAL Loss: 0.2610\n",
      "  - Avg POS Loss: 0.2558\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-23000\n",
      "Epoch 4, GStep 23100, AvgCombinedLoss: 2.8935, LR: 4.52e-05\n",
      "  - Avg MLM Loss: 2.0270\n",
      "  - Avg DATE Loss: 0.3358\n",
      "  - Avg MATERIAL Loss: 0.2702\n",
      "  - Avg POS Loss: 0.2606\n",
      "Epoch 4, GStep 23200, AvgCombinedLoss: 2.9938, LR: 4.51e-05\n",
      "  - Avg MLM Loss: 2.1276\n",
      "  - Avg DATE Loss: 0.3451\n",
      "  - Avg MATERIAL Loss: 0.2572\n",
      "  - Avg POS Loss: 0.2639\n",
      "Epoch 4, GStep 23300, AvgCombinedLoss: 2.8462, LR: 4.49e-05\n",
      "  - Avg MLM Loss: 1.9938\n",
      "  - Avg DATE Loss: 0.3397\n",
      "  - Avg MATERIAL Loss: 0.2636\n",
      "  - Avg POS Loss: 0.2490\n",
      "Epoch 4, GStep 23400, AvgCombinedLoss: 2.8567, LR: 4.47e-05\n",
      "  - Avg MLM Loss: 2.0309\n",
      "  - Avg DATE Loss: 0.3246\n",
      "  - Avg MATERIAL Loss: 0.2574\n",
      "  - Avg POS Loss: 0.2439\n",
      "Epoch 4, GStep 23500, AvgCombinedLoss: 2.8455, LR: 4.46e-05\n",
      "  - Avg MLM Loss: 1.9785\n",
      "  - Avg DATE Loss: 0.3472\n",
      "  - Avg MATERIAL Loss: 0.2663\n",
      "  - Avg POS Loss: 0.2535\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-23500\n",
      "Epoch 4, GStep 23600, AvgCombinedLoss: 2.8929, LR: 4.44e-05\n",
      "  - Avg MLM Loss: 2.0266\n",
      "  - Avg DATE Loss: 0.3365\n",
      "  - Avg MATERIAL Loss: 0.2711\n",
      "  - Avg POS Loss: 0.2588\n",
      "Epoch 4, GStep 23700, AvgCombinedLoss: 2.7739, LR: 4.42e-05\n",
      "  - Avg MLM Loss: 1.9259\n",
      "  - Avg DATE Loss: 0.3291\n",
      "  - Avg MATERIAL Loss: 0.2712\n",
      "  - Avg POS Loss: 0.2477\n",
      "Epoch 4, GStep 23800, AvgCombinedLoss: 2.7026, LR: 4.41e-05\n",
      "  - Avg MLM Loss: 1.8716\n",
      "  - Avg DATE Loss: 0.3395\n",
      "  - Avg MATERIAL Loss: 0.2532\n",
      "  - Avg POS Loss: 0.2382\n",
      "Epoch 4, GStep 23900, AvgCombinedLoss: 2.7555, LR: 4.39e-05\n",
      "  - Avg MLM Loss: 1.9086\n",
      "  - Avg DATE Loss: 0.3319\n",
      "  - Avg MATERIAL Loss: 0.2588\n",
      "  - Avg POS Loss: 0.2562\n",
      "Epoch 4, GStep 24000, AvgCombinedLoss: 2.9014, LR: 4.37e-05\n",
      "  - Avg MLM Loss: 2.0249\n",
      "  - Avg DATE Loss: 0.3500\n",
      "  - Avg MATERIAL Loss: 0.2735\n",
      "  - Avg POS Loss: 0.2530\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-24000\n",
      "Epoch 4, GStep 24100, AvgCombinedLoss: 2.8706, LR: 4.36e-05\n",
      "  - Avg MLM Loss: 2.0346\n",
      "  - Avg DATE Loss: 0.3317\n",
      "  - Avg MATERIAL Loss: 0.2551\n",
      "  - Avg POS Loss: 0.2492\n",
      "Epoch 4, GStep 24200, AvgCombinedLoss: 2.7843, LR: 4.34e-05\n",
      "  - Avg MLM Loss: 1.9430\n",
      "  - Avg DATE Loss: 0.3307\n",
      "  - Avg MATERIAL Loss: 0.2536\n",
      "  - Avg POS Loss: 0.2570\n",
      "Epoch 4, GStep 24300, AvgCombinedLoss: 2.9263, LR: 4.32e-05\n",
      "  - Avg MLM Loss: 2.0802\n",
      "  - Avg DATE Loss: 0.3262\n",
      "  - Avg MATERIAL Loss: 0.2651\n",
      "  - Avg POS Loss: 0.2547\n",
      "Epoch 4, GStep 24400, AvgCombinedLoss: 2.8471, LR: 4.31e-05\n",
      "  - Avg MLM Loss: 2.0043\n",
      "  - Avg DATE Loss: 0.3327\n",
      "  - Avg MATERIAL Loss: 0.2560\n",
      "  - Avg POS Loss: 0.2541\n",
      "Epoch 4, GStep 24500, AvgCombinedLoss: 2.8980, LR: 4.29e-05\n",
      "  - Avg MLM Loss: 2.0563\n",
      "  - Avg DATE Loss: 0.3418\n",
      "  - Avg MATERIAL Loss: 0.2478\n",
      "  - Avg POS Loss: 0.2521\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-24500\n",
      "Epoch 4, GStep 24600, AvgCombinedLoss: 2.7966, LR: 4.28e-05\n",
      "  - Avg MLM Loss: 1.9395\n",
      "  - Avg DATE Loss: 0.3488\n",
      "  - Avg MATERIAL Loss: 0.2615\n",
      "  - Avg POS Loss: 0.2468\n",
      "Epoch 4, GStep 24700, AvgCombinedLoss: 2.6756, LR: 4.26e-05\n",
      "  - Avg MLM Loss: 1.8577\n",
      "  - Avg DATE Loss: 0.3307\n",
      "  - Avg MATERIAL Loss: 0.2461\n",
      "  - Avg POS Loss: 0.2411\n",
      "Epoch 4, GStep 24800, AvgCombinedLoss: 2.9021, LR: 4.24e-05\n",
      "  - Avg MLM Loss: 2.0307\n",
      "  - Avg DATE Loss: 0.3445\n",
      "  - Avg MATERIAL Loss: 0.2710\n",
      "  - Avg POS Loss: 0.2559\n",
      "Epoch 4, GStep 24900, AvgCombinedLoss: 2.7221, LR: 4.23e-05\n",
      "  - Avg MLM Loss: 1.9042\n",
      "  - Avg DATE Loss: 0.3210\n",
      "  - Avg MATERIAL Loss: 0.2493\n",
      "  - Avg POS Loss: 0.2477\n",
      "Epoch 4, GStep 25000, AvgCombinedLoss: 2.8934, LR: 4.21e-05\n",
      "  - Avg MLM Loss: 2.0327\n",
      "  - Avg DATE Loss: 0.3364\n",
      "  - Avg MATERIAL Loss: 0.2691\n",
      "  - Avg POS Loss: 0.2552\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-25000\n",
      "Epoch 4, GStep 25100, AvgCombinedLoss: 2.8598, LR: 4.19e-05\n",
      "  - Avg MLM Loss: 2.0130\n",
      "  - Avg DATE Loss: 0.3265\n",
      "  - Avg MATERIAL Loss: 0.2716\n",
      "  - Avg POS Loss: 0.2488\n",
      "Epoch 4, GStep 25200, AvgCombinedLoss: 2.8057, LR: 4.18e-05\n",
      "  - Avg MLM Loss: 1.9632\n",
      "  - Avg DATE Loss: 0.3194\n",
      "  - Avg MATERIAL Loss: 0.2741\n",
      "  - Avg POS Loss: 0.2491\n",
      "Epoch 4, GStep 25300, AvgCombinedLoss: 2.8544, LR: 4.16e-05\n",
      "  - Avg MLM Loss: 1.9614\n",
      "  - Avg DATE Loss: 0.3598\n",
      "  - Avg MATERIAL Loss: 0.2868\n",
      "  - Avg POS Loss: 0.2465\n",
      "--- Evaluation results after epoch 4 ---\n",
      "  eval_loss_combined: 2.9022\n",
      "  eval_mlm_loss: 1.9745\n",
      "  eval_date_loss: 0.7886\n",
      "  eval_material_loss: 1.4105\n",
      "  eval_pos_loss: 0.3590\n",
      "  eval_mlm_perplexity: 7.2030\n",
      "  eval_mlm_accuracy: 0.6631\n",
      "  eval_date_accuracy: 0.7049\n",
      "  eval_date_precision: 0.6917\n",
      "  eval_date_recall: 0.7049\n",
      "  eval_date_f1: 0.6907\n",
      "  eval_material_accuracy: 0.6188\n",
      "  eval_material_precision: 0.5117\n",
      "  eval_material_recall: 0.6188\n",
      "  eval_material_f1: 0.5392\n",
      "  eval_pos_accuracy: 0.8769\n",
      "  eval_pos_precision: 0.8773\n",
      "  eval_pos_recall: 0.8769\n",
      "  eval_pos_f1: 0.8759\n",
      "Saving best model with eval combined loss 2.9022 to runs/multitask_mlm_date_material_pos\\best_model\n",
      "Epoch 5, GStep 25400, AvgCombinedLoss: 2.6962, LR: 4.14e-05\n",
      "  - Avg MLM Loss: 1.8899\n",
      "  - Avg DATE Loss: 0.3134\n",
      "  - Avg MATERIAL Loss: 0.2465\n",
      "  - Avg POS Loss: 0.2465\n",
      "Epoch 5, GStep 25500, AvgCombinedLoss: 2.7200, LR: 4.13e-05\n",
      "  - Avg MLM Loss: 1.9046\n",
      "  - Avg DATE Loss: 0.3069\n",
      "  - Avg MATERIAL Loss: 0.2603\n",
      "  - Avg POS Loss: 0.2483\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-25500\n",
      "Epoch 5, GStep 25600, AvgCombinedLoss: 2.6828, LR: 4.11e-05\n",
      "  - Avg MLM Loss: 1.8907\n",
      "  - Avg DATE Loss: 0.3036\n",
      "  - Avg MATERIAL Loss: 0.2470\n",
      "  - Avg POS Loss: 0.2414\n",
      "Epoch 5, GStep 25700, AvgCombinedLoss: 2.7648, LR: 4.09e-05\n",
      "  - Avg MLM Loss: 1.9653\n",
      "  - Avg DATE Loss: 0.3060\n",
      "  - Avg MATERIAL Loss: 0.2520\n",
      "  - Avg POS Loss: 0.2414\n",
      "Epoch 5, GStep 25800, AvgCombinedLoss: 2.6847, LR: 4.08e-05\n",
      "  - Avg MLM Loss: 1.8881\n",
      "  - Avg DATE Loss: 0.2948\n",
      "  - Avg MATERIAL Loss: 0.2567\n",
      "  - Avg POS Loss: 0.2450\n",
      "Epoch 5, GStep 25900, AvgCombinedLoss: 2.6674, LR: 4.06e-05\n",
      "  - Avg MLM Loss: 1.8715\n",
      "  - Avg DATE Loss: 0.3025\n",
      "  - Avg MATERIAL Loss: 0.2541\n",
      "  - Avg POS Loss: 0.2394\n",
      "Epoch 5, GStep 26000, AvgCombinedLoss: 2.6981, LR: 4.05e-05\n",
      "  - Avg MLM Loss: 1.9086\n",
      "  - Avg DATE Loss: 0.3097\n",
      "  - Avg MATERIAL Loss: 0.2573\n",
      "  - Avg POS Loss: 0.2226\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-26000\n",
      "Epoch 5, GStep 26100, AvgCombinedLoss: 2.8179, LR: 4.03e-05\n",
      "  - Avg MLM Loss: 2.0060\n",
      "  - Avg DATE Loss: 0.3049\n",
      "  - Avg MATERIAL Loss: 0.2603\n",
      "  - Avg POS Loss: 0.2467\n",
      "Epoch 5, GStep 26200, AvgCombinedLoss: 2.6623, LR: 4.01e-05\n",
      "  - Avg MLM Loss: 1.8625\n",
      "  - Avg DATE Loss: 0.2999\n",
      "  - Avg MATERIAL Loss: 0.2623\n",
      "  - Avg POS Loss: 0.2375\n",
      "Epoch 5, GStep 26300, AvgCombinedLoss: 2.6026, LR: 4.00e-05\n",
      "  - Avg MLM Loss: 1.8299\n",
      "  - Avg DATE Loss: 0.2812\n",
      "  - Avg MATERIAL Loss: 0.2429\n",
      "  - Avg POS Loss: 0.2486\n",
      "Epoch 5, GStep 26400, AvgCombinedLoss: 2.8156, LR: 3.98e-05\n",
      "  - Avg MLM Loss: 2.0206\n",
      "  - Avg DATE Loss: 0.2961\n",
      "  - Avg MATERIAL Loss: 0.2491\n",
      "  - Avg POS Loss: 0.2498\n",
      "Epoch 5, GStep 26500, AvgCombinedLoss: 2.7353, LR: 3.96e-05\n",
      "  - Avg MLM Loss: 1.9355\n",
      "  - Avg DATE Loss: 0.3160\n",
      "  - Avg MATERIAL Loss: 0.2493\n",
      "  - Avg POS Loss: 0.2345\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-26500\n",
      "Epoch 5, GStep 26600, AvgCombinedLoss: 2.7475, LR: 3.95e-05\n",
      "  - Avg MLM Loss: 1.9260\n",
      "  - Avg DATE Loss: 0.3007\n",
      "  - Avg MATERIAL Loss: 0.2734\n",
      "  - Avg POS Loss: 0.2475\n",
      "Epoch 5, GStep 26700, AvgCombinedLoss: 2.7899, LR: 3.93e-05\n",
      "  - Avg MLM Loss: 1.9773\n",
      "  - Avg DATE Loss: 0.3004\n",
      "  - Avg MATERIAL Loss: 0.2772\n",
      "  - Avg POS Loss: 0.2350\n",
      "Epoch 5, GStep 26800, AvgCombinedLoss: 2.7695, LR: 3.91e-05\n",
      "  - Avg MLM Loss: 1.9555\n",
      "  - Avg DATE Loss: 0.3020\n",
      "  - Avg MATERIAL Loss: 0.2668\n",
      "  - Avg POS Loss: 0.2452\n",
      "Epoch 5, GStep 26900, AvgCombinedLoss: 2.7340, LR: 3.90e-05\n",
      "  - Avg MLM Loss: 1.9344\n",
      "  - Avg DATE Loss: 0.3005\n",
      "  - Avg MATERIAL Loss: 0.2581\n",
      "  - Avg POS Loss: 0.2410\n",
      "Epoch 5, GStep 27000, AvgCombinedLoss: 2.6801, LR: 3.88e-05\n",
      "  - Avg MLM Loss: 1.8716\n",
      "  - Avg DATE Loss: 0.3135\n",
      "  - Avg MATERIAL Loss: 0.2532\n",
      "  - Avg POS Loss: 0.2417\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-27000\n",
      "Epoch 5, GStep 27100, AvgCombinedLoss: 2.8109, LR: 3.87e-05\n",
      "  - Avg MLM Loss: 1.9996\n",
      "  - Avg DATE Loss: 0.3151\n",
      "  - Avg MATERIAL Loss: 0.2519\n",
      "  - Avg POS Loss: 0.2443\n",
      "Epoch 5, GStep 27200, AvgCombinedLoss: 2.6445, LR: 3.85e-05\n",
      "  - Avg MLM Loss: 1.8590\n",
      "  - Avg DATE Loss: 0.2976\n",
      "  - Avg MATERIAL Loss: 0.2527\n",
      "  - Avg POS Loss: 0.2352\n",
      "Epoch 5, GStep 27300, AvgCombinedLoss: 2.6440, LR: 3.83e-05\n",
      "  - Avg MLM Loss: 1.8359\n",
      "  - Avg DATE Loss: 0.3066\n",
      "  - Avg MATERIAL Loss: 0.2642\n",
      "  - Avg POS Loss: 0.2373\n",
      "Epoch 5, GStep 27400, AvgCombinedLoss: 2.7234, LR: 3.82e-05\n",
      "  - Avg MLM Loss: 1.9240\n",
      "  - Avg DATE Loss: 0.3033\n",
      "  - Avg MATERIAL Loss: 0.2561\n",
      "  - Avg POS Loss: 0.2400\n",
      "Epoch 5, GStep 27500, AvgCombinedLoss: 2.7121, LR: 3.80e-05\n",
      "  - Avg MLM Loss: 1.8832\n",
      "  - Avg DATE Loss: 0.3182\n",
      "  - Avg MATERIAL Loss: 0.2723\n",
      "  - Avg POS Loss: 0.2385\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-27500\n",
      "Epoch 5, GStep 27600, AvgCombinedLoss: 2.7028, LR: 3.78e-05\n",
      "  - Avg MLM Loss: 1.8975\n",
      "  - Avg DATE Loss: 0.3054\n",
      "  - Avg MATERIAL Loss: 0.2621\n",
      "  - Avg POS Loss: 0.2379\n",
      "Epoch 5, GStep 27700, AvgCombinedLoss: 2.7370, LR: 3.77e-05\n",
      "  - Avg MLM Loss: 1.9336\n",
      "  - Avg DATE Loss: 0.3094\n",
      "  - Avg MATERIAL Loss: 0.2563\n",
      "  - Avg POS Loss: 0.2377\n",
      "Epoch 5, GStep 27800, AvgCombinedLoss: 2.6935, LR: 3.75e-05\n",
      "  - Avg MLM Loss: 1.9088\n",
      "  - Avg DATE Loss: 0.3024\n",
      "  - Avg MATERIAL Loss: 0.2470\n",
      "  - Avg POS Loss: 0.2352\n",
      "Epoch 5, GStep 27900, AvgCombinedLoss: 2.6166, LR: 3.73e-05\n",
      "  - Avg MLM Loss: 1.8499\n",
      "  - Avg DATE Loss: 0.2835\n",
      "  - Avg MATERIAL Loss: 0.2483\n",
      "  - Avg POS Loss: 0.2350\n",
      "Epoch 5, GStep 28000, AvgCombinedLoss: 2.7562, LR: 3.72e-05\n",
      "  - Avg MLM Loss: 1.9339\n",
      "  - Avg DATE Loss: 0.3028\n",
      "  - Avg MATERIAL Loss: 0.2722\n",
      "  - Avg POS Loss: 0.2474\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-28000\n",
      "Epoch 5, GStep 28100, AvgCombinedLoss: 2.7617, LR: 3.70e-05\n",
      "  - Avg MLM Loss: 1.9396\n",
      "  - Avg DATE Loss: 0.3299\n",
      "  - Avg MATERIAL Loss: 0.2556\n",
      "  - Avg POS Loss: 0.2366\n",
      "Epoch 5, GStep 28200, AvgCombinedLoss: 2.6329, LR: 3.68e-05\n",
      "  - Avg MLM Loss: 1.8269\n",
      "  - Avg DATE Loss: 0.3029\n",
      "  - Avg MATERIAL Loss: 0.2700\n",
      "  - Avg POS Loss: 0.2331\n",
      "Epoch 5, GStep 28300, AvgCombinedLoss: 2.7027, LR: 3.67e-05\n",
      "  - Avg MLM Loss: 1.8946\n",
      "  - Avg DATE Loss: 0.3081\n",
      "  - Avg MATERIAL Loss: 0.2615\n",
      "  - Avg POS Loss: 0.2385\n",
      "Epoch 5, GStep 28400, AvgCombinedLoss: 2.8426, LR: 3.65e-05\n",
      "  - Avg MLM Loss: 2.0065\n",
      "  - Avg DATE Loss: 0.3225\n",
      "  - Avg MATERIAL Loss: 0.2664\n",
      "  - Avg POS Loss: 0.2471\n",
      "Epoch 5, GStep 28500, AvgCombinedLoss: 2.6435, LR: 3.64e-05\n",
      "  - Avg MLM Loss: 1.8224\n",
      "  - Avg DATE Loss: 0.3249\n",
      "  - Avg MATERIAL Loss: 0.2641\n",
      "  - Avg POS Loss: 0.2321\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-28500\n",
      "Epoch 5, GStep 28600, AvgCombinedLoss: 2.7318, LR: 3.62e-05\n",
      "  - Avg MLM Loss: 1.9106\n",
      "  - Avg DATE Loss: 0.3415\n",
      "  - Avg MATERIAL Loss: 0.2445\n",
      "  - Avg POS Loss: 0.2352\n",
      "Epoch 5, GStep 28700, AvgCombinedLoss: 2.6374, LR: 3.60e-05\n",
      "  - Avg MLM Loss: 1.8621\n",
      "  - Avg DATE Loss: 0.2974\n",
      "  - Avg MATERIAL Loss: 0.2458\n",
      "  - Avg POS Loss: 0.2322\n",
      "Epoch 5, GStep 28800, AvgCombinedLoss: 2.6092, LR: 3.59e-05\n",
      "  - Avg MLM Loss: 1.8088\n",
      "  - Avg DATE Loss: 0.3056\n",
      "  - Avg MATERIAL Loss: 0.2604\n",
      "  - Avg POS Loss: 0.2345\n",
      "Epoch 5, GStep 28900, AvgCombinedLoss: 2.8260, LR: 3.57e-05\n",
      "  - Avg MLM Loss: 2.0012\n",
      "  - Avg DATE Loss: 0.3070\n",
      "  - Avg MATERIAL Loss: 0.2716\n",
      "  - Avg POS Loss: 0.2462\n",
      "Epoch 5, GStep 29000, AvgCombinedLoss: 2.8542, LR: 3.55e-05\n",
      "  - Avg MLM Loss: 2.0266\n",
      "  - Avg DATE Loss: 0.3201\n",
      "  - Avg MATERIAL Loss: 0.2639\n",
      "  - Avg POS Loss: 0.2436\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-29000\n",
      "Epoch 5, GStep 29100, AvgCombinedLoss: 2.6910, LR: 3.54e-05\n",
      "  - Avg MLM Loss: 1.8770\n",
      "  - Avg DATE Loss: 0.3102\n",
      "  - Avg MATERIAL Loss: 0.2604\n",
      "  - Avg POS Loss: 0.2435\n",
      "Epoch 5, GStep 29200, AvgCombinedLoss: 2.7370, LR: 3.52e-05\n",
      "  - Avg MLM Loss: 1.9119\n",
      "  - Avg DATE Loss: 0.3262\n",
      "  - Avg MATERIAL Loss: 0.2577\n",
      "  - Avg POS Loss: 0.2412\n",
      "Epoch 5, GStep 29300, AvgCombinedLoss: 2.7183, LR: 3.50e-05\n",
      "  - Avg MLM Loss: 1.9414\n",
      "  - Avg DATE Loss: 0.3052\n",
      "  - Avg MATERIAL Loss: 0.2328\n",
      "  - Avg POS Loss: 0.2389\n",
      "Epoch 5, GStep 29400, AvgCombinedLoss: 2.7161, LR: 3.49e-05\n",
      "  - Avg MLM Loss: 1.8748\n",
      "  - Avg DATE Loss: 0.3285\n",
      "  - Avg MATERIAL Loss: 0.2721\n",
      "  - Avg POS Loss: 0.2406\n",
      "Epoch 5, GStep 29500, AvgCombinedLoss: 2.6272, LR: 3.47e-05\n",
      "  - Avg MLM Loss: 1.8057\n",
      "  - Avg DATE Loss: 0.3197\n",
      "  - Avg MATERIAL Loss: 0.2621\n",
      "  - Avg POS Loss: 0.2397\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-29500\n",
      "Epoch 5, GStep 29600, AvgCombinedLoss: 2.7353, LR: 3.45e-05\n",
      "  - Avg MLM Loss: 1.9431\n",
      "  - Avg DATE Loss: 0.3117\n",
      "  - Avg MATERIAL Loss: 0.2445\n",
      "  - Avg POS Loss: 0.2360\n",
      "Epoch 5, GStep 29700, AvgCombinedLoss: 2.8498, LR: 3.44e-05\n",
      "  - Avg MLM Loss: 2.0304\n",
      "  - Avg DATE Loss: 0.3235\n",
      "  - Avg MATERIAL Loss: 0.2560\n",
      "  - Avg POS Loss: 0.2399\n",
      "Epoch 5, GStep 29800, AvgCombinedLoss: 2.7220, LR: 3.42e-05\n",
      "  - Avg MLM Loss: 1.8901\n",
      "  - Avg DATE Loss: 0.3262\n",
      "  - Avg MATERIAL Loss: 0.2711\n",
      "  - Avg POS Loss: 0.2346\n",
      "Epoch 5, GStep 29900, AvgCombinedLoss: 2.6766, LR: 3.41e-05\n",
      "  - Avg MLM Loss: 1.8663\n",
      "  - Avg DATE Loss: 0.3126\n",
      "  - Avg MATERIAL Loss: 0.2551\n",
      "  - Avg POS Loss: 0.2427\n",
      "Epoch 5, GStep 30000, AvgCombinedLoss: 2.6160, LR: 3.39e-05\n",
      "  - Avg MLM Loss: 1.8296\n",
      "  - Avg DATE Loss: 0.2882\n",
      "  - Avg MATERIAL Loss: 0.2498\n",
      "  - Avg POS Loss: 0.2484\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-30000\n",
      "Epoch 5, GStep 30100, AvgCombinedLoss: 2.6433, LR: 3.37e-05\n",
      "  - Avg MLM Loss: 1.8231\n",
      "  - Avg DATE Loss: 0.3217\n",
      "  - Avg MATERIAL Loss: 0.2589\n",
      "  - Avg POS Loss: 0.2396\n",
      "Epoch 5, GStep 30200, AvgCombinedLoss: 2.5799, LR: 3.36e-05\n",
      "  - Avg MLM Loss: 1.7559\n",
      "  - Avg DATE Loss: 0.3280\n",
      "  - Avg MATERIAL Loss: 0.2598\n",
      "  - Avg POS Loss: 0.2362\n",
      "Epoch 5, GStep 30300, AvgCombinedLoss: 2.8199, LR: 3.34e-05\n",
      "  - Avg MLM Loss: 2.0135\n",
      "  - Avg DATE Loss: 0.3081\n",
      "  - Avg MATERIAL Loss: 0.2599\n",
      "  - Avg POS Loss: 0.2384\n",
      "--- Evaluation results after epoch 5 ---\n",
      "  eval_loss_combined: 2.8385\n",
      "  eval_mlm_loss: 1.9012\n",
      "  eval_date_loss: 0.8265\n",
      "  eval_material_loss: 1.3956\n",
      "  eval_pos_loss: 0.3499\n",
      "  eval_mlm_perplexity: 6.6937\n",
      "  eval_mlm_accuracy: 0.6735\n",
      "  eval_date_accuracy: 0.6960\n",
      "  eval_date_precision: 0.6886\n",
      "  eval_date_recall: 0.6960\n",
      "  eval_date_f1: 0.6903\n",
      "  eval_material_accuracy: 0.6214\n",
      "  eval_material_precision: 0.5255\n",
      "  eval_material_recall: 0.6214\n",
      "  eval_material_f1: 0.5495\n",
      "  eval_pos_accuracy: 0.8809\n",
      "  eval_pos_precision: 0.8810\n",
      "  eval_pos_recall: 0.8809\n",
      "  eval_pos_f1: 0.8793\n",
      "Saving best model with eval combined loss 2.8385 to runs/multitask_mlm_date_material_pos\\best_model\n",
      "Epoch 6, GStep 30400, AvgCombinedLoss: 2.7051, LR: 3.32e-05\n",
      "  - Avg MLM Loss: 1.9117\n",
      "  - Avg DATE Loss: 0.3064\n",
      "  - Avg MATERIAL Loss: 0.2502\n",
      "  - Avg POS Loss: 0.2367\n",
      "Epoch 6, GStep 30500, AvgCombinedLoss: 2.5209, LR: 3.31e-05\n",
      "  - Avg MLM Loss: 1.7606\n",
      "  - Avg DATE Loss: 0.2911\n",
      "  - Avg MATERIAL Loss: 0.2489\n",
      "  - Avg POS Loss: 0.2203\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-30500\n",
      "Epoch 6, GStep 30600, AvgCombinedLoss: 2.5851, LR: 3.29e-05\n",
      "  - Avg MLM Loss: 1.8178\n",
      "  - Avg DATE Loss: 0.2909\n",
      "  - Avg MATERIAL Loss: 0.2485\n",
      "  - Avg POS Loss: 0.2278\n",
      "Epoch 6, GStep 30700, AvgCombinedLoss: 2.5654, LR: 3.27e-05\n",
      "  - Avg MLM Loss: 1.8211\n",
      "  - Avg DATE Loss: 0.2702\n",
      "  - Avg MATERIAL Loss: 0.2524\n",
      "  - Avg POS Loss: 0.2218\n",
      "Epoch 6, GStep 30800, AvgCombinedLoss: 2.5601, LR: 3.26e-05\n",
      "  - Avg MLM Loss: 1.8165\n",
      "  - Avg DATE Loss: 0.2676\n",
      "  - Avg MATERIAL Loss: 0.2417\n",
      "  - Avg POS Loss: 0.2343\n",
      "Epoch 6, GStep 30900, AvgCombinedLoss: 2.6247, LR: 3.24e-05\n",
      "  - Avg MLM Loss: 1.8685\n",
      "  - Avg DATE Loss: 0.2746\n",
      "  - Avg MATERIAL Loss: 0.2471\n",
      "  - Avg POS Loss: 0.2345\n",
      "Epoch 6, GStep 31000, AvgCombinedLoss: 2.6204, LR: 3.23e-05\n",
      "  - Avg MLM Loss: 1.8930\n",
      "  - Avg DATE Loss: 0.2613\n",
      "  - Avg MATERIAL Loss: 0.2389\n",
      "  - Avg POS Loss: 0.2272\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-31000\n",
      "Epoch 6, GStep 31100, AvgCombinedLoss: 2.6710, LR: 3.21e-05\n",
      "  - Avg MLM Loss: 1.8987\n",
      "  - Avg DATE Loss: 0.2792\n",
      "  - Avg MATERIAL Loss: 0.2566\n",
      "  - Avg POS Loss: 0.2366\n",
      "Epoch 6, GStep 31200, AvgCombinedLoss: 2.5218, LR: 3.19e-05\n",
      "  - Avg MLM Loss: 1.7737\n",
      "  - Avg DATE Loss: 0.2743\n",
      "  - Avg MATERIAL Loss: 0.2434\n",
      "  - Avg POS Loss: 0.2304\n",
      "Epoch 6, GStep 31300, AvgCombinedLoss: 2.5510, LR: 3.18e-05\n",
      "  - Avg MLM Loss: 1.7928\n",
      "  - Avg DATE Loss: 0.2837\n",
      "  - Avg MATERIAL Loss: 0.2450\n",
      "  - Avg POS Loss: 0.2296\n",
      "Epoch 6, GStep 31400, AvgCombinedLoss: 2.6392, LR: 3.16e-05\n",
      "  - Avg MLM Loss: 1.8794\n",
      "  - Avg DATE Loss: 0.2802\n",
      "  - Avg MATERIAL Loss: 0.2499\n",
      "  - Avg POS Loss: 0.2297\n",
      "Epoch 6, GStep 31500, AvgCombinedLoss: 2.6517, LR: 3.14e-05\n",
      "  - Avg MLM Loss: 1.8719\n",
      "  - Avg DATE Loss: 0.2838\n",
      "  - Avg MATERIAL Loss: 0.2572\n",
      "  - Avg POS Loss: 0.2387\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-31500\n",
      "Epoch 6, GStep 31600, AvgCombinedLoss: 2.5613, LR: 3.13e-05\n",
      "  - Avg MLM Loss: 1.7892\n",
      "  - Avg DATE Loss: 0.2965\n",
      "  - Avg MATERIAL Loss: 0.2468\n",
      "  - Avg POS Loss: 0.2288\n",
      "Epoch 6, GStep 31700, AvgCombinedLoss: 2.5505, LR: 3.11e-05\n",
      "  - Avg MLM Loss: 1.7940\n",
      "  - Avg DATE Loss: 0.2693\n",
      "  - Avg MATERIAL Loss: 0.2537\n",
      "  - Avg POS Loss: 0.2335\n",
      "Epoch 6, GStep 31800, AvgCombinedLoss: 2.6578, LR: 3.09e-05\n",
      "  - Avg MLM Loss: 1.8957\n",
      "  - Avg DATE Loss: 0.2829\n",
      "  - Avg MATERIAL Loss: 0.2511\n",
      "  - Avg POS Loss: 0.2281\n",
      "Epoch 6, GStep 31900, AvgCombinedLoss: 2.6026, LR: 3.08e-05\n",
      "  - Avg MLM Loss: 1.8348\n",
      "  - Avg DATE Loss: 0.2953\n",
      "  - Avg MATERIAL Loss: 0.2505\n",
      "  - Avg POS Loss: 0.2221\n",
      "Epoch 6, GStep 32000, AvgCombinedLoss: 2.5482, LR: 3.06e-05\n",
      "  - Avg MLM Loss: 1.7898\n",
      "  - Avg DATE Loss: 0.2719\n",
      "  - Avg MATERIAL Loss: 0.2540\n",
      "  - Avg POS Loss: 0.2326\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-32000\n",
      "Epoch 6, GStep 32100, AvgCombinedLoss: 2.5842, LR: 3.04e-05\n",
      "  - Avg MLM Loss: 1.8408\n",
      "  - Avg DATE Loss: 0.2711\n",
      "  - Avg MATERIAL Loss: 0.2494\n",
      "  - Avg POS Loss: 0.2229\n",
      "Epoch 6, GStep 32200, AvgCombinedLoss: 2.6297, LR: 3.03e-05\n",
      "  - Avg MLM Loss: 1.8881\n",
      "  - Avg DATE Loss: 0.2652\n",
      "  - Avg MATERIAL Loss: 0.2456\n",
      "  - Avg POS Loss: 0.2309\n",
      "Epoch 6, GStep 32300, AvgCombinedLoss: 2.6141, LR: 3.01e-05\n",
      "  - Avg MLM Loss: 1.8523\n",
      "  - Avg DATE Loss: 0.2624\n",
      "  - Avg MATERIAL Loss: 0.2613\n",
      "  - Avg POS Loss: 0.2381\n",
      "Epoch 6, GStep 32400, AvgCombinedLoss: 2.5509, LR: 3.00e-05\n",
      "  - Avg MLM Loss: 1.7849\n",
      "  - Avg DATE Loss: 0.2802\n",
      "  - Avg MATERIAL Loss: 0.2643\n",
      "  - Avg POS Loss: 0.2214\n",
      "Epoch 6, GStep 32500, AvgCombinedLoss: 2.6367, LR: 2.98e-05\n",
      "  - Avg MLM Loss: 1.8697\n",
      "  - Avg DATE Loss: 0.2767\n",
      "  - Avg MATERIAL Loss: 0.2493\n",
      "  - Avg POS Loss: 0.2410\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-32500\n",
      "Epoch 6, GStep 32600, AvgCombinedLoss: 2.5227, LR: 2.96e-05\n",
      "  - Avg MLM Loss: 1.7676\n",
      "  - Avg DATE Loss: 0.2634\n",
      "  - Avg MATERIAL Loss: 0.2563\n",
      "  - Avg POS Loss: 0.2354\n",
      "Epoch 6, GStep 32700, AvgCombinedLoss: 2.5224, LR: 2.95e-05\n",
      "  - Avg MLM Loss: 1.7582\n",
      "  - Avg DATE Loss: 0.2781\n",
      "  - Avg MATERIAL Loss: 0.2609\n",
      "  - Avg POS Loss: 0.2253\n",
      "Epoch 6, GStep 32800, AvgCombinedLoss: 2.5104, LR: 2.93e-05\n",
      "  - Avg MLM Loss: 1.7488\n",
      "  - Avg DATE Loss: 0.2775\n",
      "  - Avg MATERIAL Loss: 0.2541\n",
      "  - Avg POS Loss: 0.2300\n",
      "Epoch 6, GStep 32900, AvgCombinedLoss: 2.5641, LR: 2.91e-05\n",
      "  - Avg MLM Loss: 1.8104\n",
      "  - Avg DATE Loss: 0.2843\n",
      "  - Avg MATERIAL Loss: 0.2463\n",
      "  - Avg POS Loss: 0.2231\n",
      "Epoch 6, GStep 33000, AvgCombinedLoss: 2.5644, LR: 2.90e-05\n",
      "  - Avg MLM Loss: 1.7952\n",
      "  - Avg DATE Loss: 0.2891\n",
      "  - Avg MATERIAL Loss: 0.2587\n",
      "  - Avg POS Loss: 0.2213\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-33000\n",
      "Epoch 6, GStep 33100, AvgCombinedLoss: 2.4869, LR: 2.88e-05\n",
      "  - Avg MLM Loss: 1.7341\n",
      "  - Avg DATE Loss: 0.2783\n",
      "  - Avg MATERIAL Loss: 0.2436\n",
      "  - Avg POS Loss: 0.2308\n",
      "Epoch 6, GStep 33200, AvgCombinedLoss: 2.4897, LR: 2.86e-05\n",
      "  - Avg MLM Loss: 1.7658\n",
      "  - Avg DATE Loss: 0.2625\n",
      "  - Avg MATERIAL Loss: 0.2435\n",
      "  - Avg POS Loss: 0.2179\n",
      "Epoch 6, GStep 33300, AvgCombinedLoss: 2.5591, LR: 2.85e-05\n",
      "  - Avg MLM Loss: 1.8086\n",
      "  - Avg DATE Loss: 0.2785\n",
      "  - Avg MATERIAL Loss: 0.2452\n",
      "  - Avg POS Loss: 0.2269\n",
      "Epoch 6, GStep 33400, AvgCombinedLoss: 2.6781, LR: 2.83e-05\n",
      "  - Avg MLM Loss: 1.9142\n",
      "  - Avg DATE Loss: 0.2920\n",
      "  - Avg MATERIAL Loss: 0.2435\n",
      "  - Avg POS Loss: 0.2283\n",
      "Epoch 6, GStep 33500, AvgCombinedLoss: 2.5790, LR: 2.81e-05\n",
      "  - Avg MLM Loss: 1.7981\n",
      "  - Avg DATE Loss: 0.2877\n",
      "  - Avg MATERIAL Loss: 0.2577\n",
      "  - Avg POS Loss: 0.2355\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-33500\n",
      "Epoch 6, GStep 33600, AvgCombinedLoss: 2.5869, LR: 2.80e-05\n",
      "  - Avg MLM Loss: 1.8117\n",
      "  - Avg DATE Loss: 0.2941\n",
      "  - Avg MATERIAL Loss: 0.2508\n",
      "  - Avg POS Loss: 0.2303\n",
      "Epoch 6, GStep 33700, AvgCombinedLoss: 2.5531, LR: 2.78e-05\n",
      "  - Avg MLM Loss: 1.8304\n",
      "  - Avg DATE Loss: 0.2750\n",
      "  - Avg MATERIAL Loss: 0.2253\n",
      "  - Avg POS Loss: 0.2224\n",
      "Epoch 6, GStep 33800, AvgCombinedLoss: 2.5835, LR: 2.77e-05\n",
      "  - Avg MLM Loss: 1.8289\n",
      "  - Avg DATE Loss: 0.2882\n",
      "  - Avg MATERIAL Loss: 0.2443\n",
      "  - Avg POS Loss: 0.2221\n",
      "Epoch 6, GStep 33900, AvgCombinedLoss: 2.6877, LR: 2.75e-05\n",
      "  - Avg MLM Loss: 1.9189\n",
      "  - Avg DATE Loss: 0.2794\n",
      "  - Avg MATERIAL Loss: 0.2567\n",
      "  - Avg POS Loss: 0.2327\n",
      "Epoch 6, GStep 34000, AvgCombinedLoss: 2.6273, LR: 2.73e-05\n",
      "  - Avg MLM Loss: 1.8438\n",
      "  - Avg DATE Loss: 0.2861\n",
      "  - Avg MATERIAL Loss: 0.2671\n",
      "  - Avg POS Loss: 0.2302\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-34000\n",
      "Epoch 6, GStep 34100, AvgCombinedLoss: 2.5670, LR: 2.72e-05\n",
      "  - Avg MLM Loss: 1.8215\n",
      "  - Avg DATE Loss: 0.2618\n",
      "  - Avg MATERIAL Loss: 0.2532\n",
      "  - Avg POS Loss: 0.2305\n",
      "Epoch 6, GStep 34200, AvgCombinedLoss: 2.5010, LR: 2.70e-05\n",
      "  - Avg MLM Loss: 1.7451\n",
      "  - Avg DATE Loss: 0.2809\n",
      "  - Avg MATERIAL Loss: 0.2480\n",
      "  - Avg POS Loss: 0.2270\n",
      "Epoch 6, GStep 34300, AvgCombinedLoss: 2.6650, LR: 2.68e-05\n",
      "  - Avg MLM Loss: 1.8654\n",
      "  - Avg DATE Loss: 0.3037\n",
      "  - Avg MATERIAL Loss: 0.2620\n",
      "  - Avg POS Loss: 0.2338\n",
      "Epoch 6, GStep 34400, AvgCombinedLoss: 2.6727, LR: 2.67e-05\n",
      "  - Avg MLM Loss: 1.9080\n",
      "  - Avg DATE Loss: 0.2861\n",
      "  - Avg MATERIAL Loss: 0.2466\n",
      "  - Avg POS Loss: 0.2319\n",
      "Epoch 6, GStep 34500, AvgCombinedLoss: 2.5189, LR: 2.65e-05\n",
      "  - Avg MLM Loss: 1.7660\n",
      "  - Avg DATE Loss: 0.2784\n",
      "  - Avg MATERIAL Loss: 0.2450\n",
      "  - Avg POS Loss: 0.2295\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-34500\n",
      "Epoch 6, GStep 34600, AvgCombinedLoss: 2.4956, LR: 2.63e-05\n",
      "  - Avg MLM Loss: 1.7538\n",
      "  - Avg DATE Loss: 0.2810\n",
      "  - Avg MATERIAL Loss: 0.2425\n",
      "  - Avg POS Loss: 0.2183\n",
      "Epoch 6, GStep 34700, AvgCombinedLoss: 2.5767, LR: 2.62e-05\n",
      "  - Avg MLM Loss: 1.8440\n",
      "  - Avg DATE Loss: 0.2742\n",
      "  - Avg MATERIAL Loss: 0.2343\n",
      "  - Avg POS Loss: 0.2242\n",
      "Epoch 6, GStep 34800, AvgCombinedLoss: 2.5324, LR: 2.60e-05\n",
      "  - Avg MLM Loss: 1.8041\n",
      "  - Avg DATE Loss: 0.2605\n",
      "  - Avg MATERIAL Loss: 0.2441\n",
      "  - Avg POS Loss: 0.2238\n",
      "Epoch 6, GStep 34900, AvgCombinedLoss: 2.5652, LR: 2.58e-05\n",
      "  - Avg MLM Loss: 1.7899\n",
      "  - Avg DATE Loss: 0.2923\n",
      "  - Avg MATERIAL Loss: 0.2543\n",
      "  - Avg POS Loss: 0.2287\n",
      "Epoch 6, GStep 35000, AvgCombinedLoss: 2.6608, LR: 2.57e-05\n",
      "  - Avg MLM Loss: 1.9084\n",
      "  - Avg DATE Loss: 0.2614\n",
      "  - Avg MATERIAL Loss: 0.2622\n",
      "  - Avg POS Loss: 0.2289\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-35000\n",
      "Epoch 6, GStep 35100, AvgCombinedLoss: 2.5982, LR: 2.55e-05\n",
      "  - Avg MLM Loss: 1.8470\n",
      "  - Avg DATE Loss: 0.2706\n",
      "  - Avg MATERIAL Loss: 0.2427\n",
      "  - Avg POS Loss: 0.2379\n",
      "Epoch 6, GStep 35200, AvgCombinedLoss: 2.5506, LR: 2.54e-05\n",
      "  - Avg MLM Loss: 1.7776\n",
      "  - Avg DATE Loss: 0.2891\n",
      "  - Avg MATERIAL Loss: 0.2667\n",
      "  - Avg POS Loss: 0.2171\n",
      "Epoch 6, GStep 35300, AvgCombinedLoss: 2.5081, LR: 2.52e-05\n",
      "  - Avg MLM Loss: 1.7707\n",
      "  - Avg DATE Loss: 0.2864\n",
      "  - Avg MATERIAL Loss: 0.2351\n",
      "  - Avg POS Loss: 0.2160\n",
      "Epoch 6, GStep 35400, AvgCombinedLoss: 2.5133, LR: 2.50e-05\n",
      "  - Avg MLM Loss: 1.7485\n",
      "  - Avg DATE Loss: 0.2846\n",
      "  - Avg MATERIAL Loss: 0.2515\n",
      "  - Avg POS Loss: 0.2286\n",
      "--- Evaluation results after epoch 6 ---\n",
      "  eval_loss_combined: 2.7831\n",
      "  eval_mlm_loss: 1.8462\n",
      "  eval_date_loss: 0.8412\n",
      "  eval_material_loss: 1.3886\n",
      "  eval_pos_loss: 0.3408\n",
      "  eval_mlm_perplexity: 6.3359\n",
      "  eval_mlm_accuracy: 0.6800\n",
      "  eval_date_accuracy: 0.7077\n",
      "  eval_date_precision: 0.6961\n",
      "  eval_date_recall: 0.7077\n",
      "  eval_date_f1: 0.6939\n",
      "  eval_material_accuracy: 0.6254\n",
      "  eval_material_precision: 0.5365\n",
      "  eval_material_recall: 0.6254\n",
      "  eval_material_f1: 0.5532\n",
      "  eval_pos_accuracy: 0.8839\n",
      "  eval_pos_precision: 0.8840\n",
      "  eval_pos_recall: 0.8839\n",
      "  eval_pos_f1: 0.8827\n",
      "Saving best model with eval combined loss 2.7831 to runs/multitask_mlm_date_material_pos\\best_model\n",
      "Epoch 7, GStep 35500, AvgCombinedLoss: 2.4491, LR: 2.49e-05\n",
      "  - Avg MLM Loss: 1.7386\n",
      "  - Avg DATE Loss: 0.2545\n",
      "  - Avg MATERIAL Loss: 0.2455\n",
      "  - Avg POS Loss: 0.2105\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-35500\n",
      "Epoch 7, GStep 35600, AvgCombinedLoss: 2.4621, LR: 2.47e-05\n",
      "  - Avg MLM Loss: 1.7768\n",
      "  - Avg DATE Loss: 0.2358\n",
      "  - Avg MATERIAL Loss: 0.2354\n",
      "  - Avg POS Loss: 0.2141\n",
      "Epoch 7, GStep 35700, AvgCombinedLoss: 2.5149, LR: 2.45e-05\n",
      "  - Avg MLM Loss: 1.7992\n",
      "  - Avg DATE Loss: 0.2407\n",
      "  - Avg MATERIAL Loss: 0.2523\n",
      "  - Avg POS Loss: 0.2227\n",
      "Epoch 7, GStep 35800, AvgCombinedLoss: 2.6610, LR: 2.44e-05\n",
      "  - Avg MLM Loss: 1.9421\n",
      "  - Avg DATE Loss: 0.2486\n",
      "  - Avg MATERIAL Loss: 0.2508\n",
      "  - Avg POS Loss: 0.2195\n",
      "Epoch 7, GStep 35900, AvgCombinedLoss: 2.5083, LR: 2.42e-05\n",
      "  - Avg MLM Loss: 1.8007\n",
      "  - Avg DATE Loss: 0.2439\n",
      "  - Avg MATERIAL Loss: 0.2375\n",
      "  - Avg POS Loss: 0.2261\n",
      "Epoch 7, GStep 36000, AvgCombinedLoss: 2.4579, LR: 2.40e-05\n",
      "  - Avg MLM Loss: 1.7660\n",
      "  - Avg DATE Loss: 0.2353\n",
      "  - Avg MATERIAL Loss: 0.2397\n",
      "  - Avg POS Loss: 0.2169\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-36000\n",
      "Epoch 7, GStep 36100, AvgCombinedLoss: 2.4995, LR: 2.39e-05\n",
      "  - Avg MLM Loss: 1.7894\n",
      "  - Avg DATE Loss: 0.2486\n",
      "  - Avg MATERIAL Loss: 0.2476\n",
      "  - Avg POS Loss: 0.2140\n",
      "Epoch 7, GStep 36200, AvgCombinedLoss: 2.4392, LR: 2.37e-05\n",
      "  - Avg MLM Loss: 1.7344\n",
      "  - Avg DATE Loss: 0.2531\n",
      "  - Avg MATERIAL Loss: 0.2369\n",
      "  - Avg POS Loss: 0.2147\n",
      "Epoch 7, GStep 36300, AvgCombinedLoss: 2.4315, LR: 2.36e-05\n",
      "  - Avg MLM Loss: 1.7395\n",
      "  - Avg DATE Loss: 0.2370\n",
      "  - Avg MATERIAL Loss: 0.2427\n",
      "  - Avg POS Loss: 0.2123\n",
      "Epoch 7, GStep 36400, AvgCombinedLoss: 2.5094, LR: 2.34e-05\n",
      "  - Avg MLM Loss: 1.8066\n",
      "  - Avg DATE Loss: 0.2425\n",
      "  - Avg MATERIAL Loss: 0.2371\n",
      "  - Avg POS Loss: 0.2232\n",
      "Epoch 7, GStep 36500, AvgCombinedLoss: 2.5195, LR: 2.32e-05\n",
      "  - Avg MLM Loss: 1.7920\n",
      "  - Avg DATE Loss: 0.2503\n",
      "  - Avg MATERIAL Loss: 0.2597\n",
      "  - Avg POS Loss: 0.2176\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-36500\n",
      "Epoch 7, GStep 36600, AvgCombinedLoss: 2.4529, LR: 2.31e-05\n",
      "  - Avg MLM Loss: 1.7524\n",
      "  - Avg DATE Loss: 0.2307\n",
      "  - Avg MATERIAL Loss: 0.2497\n",
      "  - Avg POS Loss: 0.2201\n",
      "Epoch 7, GStep 36700, AvgCombinedLoss: 2.3639, LR: 2.29e-05\n",
      "  - Avg MLM Loss: 1.6722\n",
      "  - Avg DATE Loss: 0.2409\n",
      "  - Avg MATERIAL Loss: 0.2383\n",
      "  - Avg POS Loss: 0.2124\n",
      "Epoch 7, GStep 36800, AvgCombinedLoss: 2.5642, LR: 2.27e-05\n",
      "  - Avg MLM Loss: 1.8468\n",
      "  - Avg DATE Loss: 0.2526\n",
      "  - Avg MATERIAL Loss: 0.2416\n",
      "  - Avg POS Loss: 0.2232\n",
      "Epoch 7, GStep 36900, AvgCombinedLoss: 2.4894, LR: 2.26e-05\n",
      "  - Avg MLM Loss: 1.7851\n",
      "  - Avg DATE Loss: 0.2418\n",
      "  - Avg MATERIAL Loss: 0.2370\n",
      "  - Avg POS Loss: 0.2255\n",
      "Epoch 7, GStep 37000, AvgCombinedLoss: 2.4895, LR: 2.24e-05\n",
      "  - Avg MLM Loss: 1.7857\n",
      "  - Avg DATE Loss: 0.2349\n",
      "  - Avg MATERIAL Loss: 0.2452\n",
      "  - Avg POS Loss: 0.2237\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-37000\n",
      "Epoch 7, GStep 37100, AvgCombinedLoss: 2.4859, LR: 2.22e-05\n",
      "  - Avg MLM Loss: 1.7929\n",
      "  - Avg DATE Loss: 0.2555\n",
      "  - Avg MATERIAL Loss: 0.2257\n",
      "  - Avg POS Loss: 0.2117\n",
      "Epoch 7, GStep 37200, AvgCombinedLoss: 2.4982, LR: 2.21e-05\n",
      "  - Avg MLM Loss: 1.7767\n",
      "  - Avg DATE Loss: 0.2550\n",
      "  - Avg MATERIAL Loss: 0.2475\n",
      "  - Avg POS Loss: 0.2191\n",
      "Epoch 7, GStep 37300, AvgCombinedLoss: 2.5602, LR: 2.19e-05\n",
      "  - Avg MLM Loss: 1.8664\n",
      "  - Avg DATE Loss: 0.2515\n",
      "  - Avg MATERIAL Loss: 0.2295\n",
      "  - Avg POS Loss: 0.2128\n",
      "Epoch 7, GStep 37400, AvgCombinedLoss: 2.4975, LR: 2.17e-05\n",
      "  - Avg MLM Loss: 1.7668\n",
      "  - Avg DATE Loss: 0.2589\n",
      "  - Avg MATERIAL Loss: 0.2529\n",
      "  - Avg POS Loss: 0.2189\n",
      "Epoch 7, GStep 37500, AvgCombinedLoss: 2.5285, LR: 2.16e-05\n",
      "  - Avg MLM Loss: 1.8290\n",
      "  - Avg DATE Loss: 0.2311\n",
      "  - Avg MATERIAL Loss: 0.2421\n",
      "  - Avg POS Loss: 0.2262\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-37500\n",
      "Epoch 7, GStep 37600, AvgCombinedLoss: 2.4508, LR: 2.14e-05\n",
      "  - Avg MLM Loss: 1.7157\n",
      "  - Avg DATE Loss: 0.2671\n",
      "  - Avg MATERIAL Loss: 0.2547\n",
      "  - Avg POS Loss: 0.2132\n",
      "Epoch 7, GStep 37700, AvgCombinedLoss: 2.4673, LR: 2.13e-05\n",
      "  - Avg MLM Loss: 1.7663\n",
      "  - Avg DATE Loss: 0.2392\n",
      "  - Avg MATERIAL Loss: 0.2480\n",
      "  - Avg POS Loss: 0.2138\n",
      "Epoch 7, GStep 37800, AvgCombinedLoss: 2.4467, LR: 2.11e-05\n",
      "  - Avg MLM Loss: 1.7401\n",
      "  - Avg DATE Loss: 0.2531\n",
      "  - Avg MATERIAL Loss: 0.2280\n",
      "  - Avg POS Loss: 0.2254\n",
      "Epoch 7, GStep 37900, AvgCombinedLoss: 2.5253, LR: 2.09e-05\n",
      "  - Avg MLM Loss: 1.8132\n",
      "  - Avg DATE Loss: 0.2516\n",
      "  - Avg MATERIAL Loss: 0.2447\n",
      "  - Avg POS Loss: 0.2157\n",
      "Epoch 7, GStep 38000, AvgCombinedLoss: 2.4242, LR: 2.08e-05\n",
      "  - Avg MLM Loss: 1.7197\n",
      "  - Avg DATE Loss: 0.2428\n",
      "  - Avg MATERIAL Loss: 0.2386\n",
      "  - Avg POS Loss: 0.2232\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-38000\n",
      "Epoch 7, GStep 38100, AvgCombinedLoss: 2.4725, LR: 2.06e-05\n",
      "  - Avg MLM Loss: 1.7816\n",
      "  - Avg DATE Loss: 0.2344\n",
      "  - Avg MATERIAL Loss: 0.2338\n",
      "  - Avg POS Loss: 0.2227\n",
      "Epoch 7, GStep 38200, AvgCombinedLoss: 2.5048, LR: 2.04e-05\n",
      "  - Avg MLM Loss: 1.7962\n",
      "  - Avg DATE Loss: 0.2594\n",
      "  - Avg MATERIAL Loss: 0.2342\n",
      "  - Avg POS Loss: 0.2150\n",
      "Epoch 7, GStep 38300, AvgCombinedLoss: 2.4276, LR: 2.03e-05\n",
      "  - Avg MLM Loss: 1.7331\n",
      "  - Avg DATE Loss: 0.2374\n",
      "  - Avg MATERIAL Loss: 0.2424\n",
      "  - Avg POS Loss: 0.2146\n",
      "Epoch 7, GStep 38400, AvgCombinedLoss: 2.4513, LR: 2.01e-05\n",
      "  - Avg MLM Loss: 1.7384\n",
      "  - Avg DATE Loss: 0.2627\n",
      "  - Avg MATERIAL Loss: 0.2356\n",
      "  - Avg POS Loss: 0.2146\n",
      "Epoch 7, GStep 38500, AvgCombinedLoss: 2.4685, LR: 1.99e-05\n",
      "  - Avg MLM Loss: 1.7717\n",
      "  - Avg DATE Loss: 0.2402\n",
      "  - Avg MATERIAL Loss: 0.2355\n",
      "  - Avg POS Loss: 0.2211\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-38500\n",
      "Epoch 7, GStep 38600, AvgCombinedLoss: 2.3914, LR: 1.98e-05\n",
      "  - Avg MLM Loss: 1.6964\n",
      "  - Avg DATE Loss: 0.2320\n",
      "  - Avg MATERIAL Loss: 0.2478\n",
      "  - Avg POS Loss: 0.2151\n",
      "Epoch 7, GStep 38700, AvgCombinedLoss: 2.4286, LR: 1.96e-05\n",
      "  - Avg MLM Loss: 1.7244\n",
      "  - Avg DATE Loss: 0.2500\n",
      "  - Avg MATERIAL Loss: 0.2373\n",
      "  - Avg POS Loss: 0.2168\n",
      "Epoch 7, GStep 38800, AvgCombinedLoss: 2.5184, LR: 1.94e-05\n",
      "  - Avg MLM Loss: 1.8240\n",
      "  - Avg DATE Loss: 0.2496\n",
      "  - Avg MATERIAL Loss: 0.2290\n",
      "  - Avg POS Loss: 0.2158\n",
      "Epoch 7, GStep 38900, AvgCombinedLoss: 2.3500, LR: 1.93e-05\n",
      "  - Avg MLM Loss: 1.6679\n",
      "  - Avg DATE Loss: 0.2332\n",
      "  - Avg MATERIAL Loss: 0.2283\n",
      "  - Avg POS Loss: 0.2206\n",
      "Epoch 7, GStep 39000, AvgCombinedLoss: 2.3902, LR: 1.91e-05\n",
      "  - Avg MLM Loss: 1.6751\n",
      "  - Avg DATE Loss: 0.2507\n",
      "  - Avg MATERIAL Loss: 0.2402\n",
      "  - Avg POS Loss: 0.2243\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-39000\n",
      "Epoch 7, GStep 39100, AvgCombinedLoss: 2.4747, LR: 1.90e-05\n",
      "  - Avg MLM Loss: 1.7509\n",
      "  - Avg DATE Loss: 0.2640\n",
      "  - Avg MATERIAL Loss: 0.2391\n",
      "  - Avg POS Loss: 0.2207\n",
      "Epoch 7, GStep 39200, AvgCombinedLoss: 2.4832, LR: 1.88e-05\n",
      "  - Avg MLM Loss: 1.7682\n",
      "  - Avg DATE Loss: 0.2523\n",
      "  - Avg MATERIAL Loss: 0.2417\n",
      "  - Avg POS Loss: 0.2211\n",
      "Epoch 7, GStep 39300, AvgCombinedLoss: 2.4249, LR: 1.86e-05\n",
      "  - Avg MLM Loss: 1.7187\n",
      "  - Avg DATE Loss: 0.2396\n",
      "  - Avg MATERIAL Loss: 0.2486\n",
      "  - Avg POS Loss: 0.2181\n",
      "Epoch 7, GStep 39400, AvgCombinedLoss: 2.5688, LR: 1.85e-05\n",
      "  - Avg MLM Loss: 1.8310\n",
      "  - Avg DATE Loss: 0.2555\n",
      "  - Avg MATERIAL Loss: 0.2565\n",
      "  - Avg POS Loss: 0.2258\n",
      "Epoch 7, GStep 39500, AvgCombinedLoss: 2.3950, LR: 1.83e-05\n",
      "  - Avg MLM Loss: 1.6836\n",
      "  - Avg DATE Loss: 0.2456\n",
      "  - Avg MATERIAL Loss: 0.2458\n",
      "  - Avg POS Loss: 0.2200\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-39500\n",
      "Epoch 7, GStep 39600, AvgCombinedLoss: 2.4242, LR: 1.81e-05\n",
      "  - Avg MLM Loss: 1.7340\n",
      "  - Avg DATE Loss: 0.2337\n",
      "  - Avg MATERIAL Loss: 0.2432\n",
      "  - Avg POS Loss: 0.2133\n",
      "Epoch 7, GStep 39700, AvgCombinedLoss: 2.4066, LR: 1.80e-05\n",
      "  - Avg MLM Loss: 1.7222\n",
      "  - Avg DATE Loss: 0.2427\n",
      "  - Avg MATERIAL Loss: 0.2298\n",
      "  - Avg POS Loss: 0.2119\n",
      "Epoch 7, GStep 39800, AvgCombinedLoss: 2.4734, LR: 1.78e-05\n",
      "  - Avg MLM Loss: 1.7462\n",
      "  - Avg DATE Loss: 0.2547\n",
      "  - Avg MATERIAL Loss: 0.2479\n",
      "  - Avg POS Loss: 0.2246\n",
      "Epoch 7, GStep 39900, AvgCombinedLoss: 2.3467, LR: 1.76e-05\n",
      "  - Avg MLM Loss: 1.6618\n",
      "  - Avg DATE Loss: 0.2331\n",
      "  - Avg MATERIAL Loss: 0.2413\n",
      "  - Avg POS Loss: 0.2106\n",
      "Epoch 7, GStep 40000, AvgCombinedLoss: 2.4552, LR: 1.75e-05\n",
      "  - Avg MLM Loss: 1.7435\n",
      "  - Avg DATE Loss: 0.2444\n",
      "  - Avg MATERIAL Loss: 0.2501\n",
      "  - Avg POS Loss: 0.2172\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-40000\n",
      "Epoch 7, GStep 40100, AvgCombinedLoss: 2.4890, LR: 1.73e-05\n",
      "  - Avg MLM Loss: 1.7718\n",
      "  - Avg DATE Loss: 0.2494\n",
      "  - Avg MATERIAL Loss: 0.2460\n",
      "  - Avg POS Loss: 0.2219\n",
      "Epoch 7, GStep 40200, AvgCombinedLoss: 2.4765, LR: 1.72e-05\n",
      "  - Avg MLM Loss: 1.7414\n",
      "  - Avg DATE Loss: 0.2621\n",
      "  - Avg MATERIAL Loss: 0.2552\n",
      "  - Avg POS Loss: 0.2178\n",
      "Epoch 7, GStep 40300, AvgCombinedLoss: 2.3725, LR: 1.70e-05\n",
      "  - Avg MLM Loss: 1.6698\n",
      "  - Avg DATE Loss: 0.2414\n",
      "  - Avg MATERIAL Loss: 0.2437\n",
      "  - Avg POS Loss: 0.2176\n",
      "Epoch 7, GStep 40400, AvgCombinedLoss: 2.4798, LR: 1.68e-05\n",
      "  - Avg MLM Loss: 1.7714\n",
      "  - Avg DATE Loss: 0.2530\n",
      "  - Avg MATERIAL Loss: 0.2426\n",
      "  - Avg POS Loss: 0.2129\n",
      "Epoch 7, GStep 40500, AvgCombinedLoss: 2.4571, LR: 1.67e-05\n",
      "  - Avg MLM Loss: 1.7459\n",
      "  - Avg DATE Loss: 0.2448\n",
      "  - Avg MATERIAL Loss: 0.2462\n",
      "  - Avg POS Loss: 0.2202\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-40500\n",
      "--- Evaluation results after epoch 7 ---\n",
      "  eval_loss_combined: 2.7979\n",
      "  eval_mlm_loss: 1.8396\n",
      "  eval_date_loss: 0.8827\n",
      "  eval_material_loss: 1.3951\n",
      "  eval_pos_loss: 0.3398\n",
      "  eval_mlm_perplexity: 6.2940\n",
      "  eval_mlm_accuracy: 0.6854\n",
      "  eval_date_accuracy: 0.7024\n",
      "  eval_date_precision: 0.6923\n",
      "  eval_date_recall: 0.7024\n",
      "  eval_date_f1: 0.6935\n",
      "  eval_material_accuracy: 0.6207\n",
      "  eval_material_precision: 0.5253\n",
      "  eval_material_recall: 0.6207\n",
      "  eval_material_f1: 0.5509\n",
      "  eval_pos_accuracy: 0.8844\n",
      "  eval_pos_precision: 0.8843\n",
      "  eval_pos_recall: 0.8844\n",
      "  eval_pos_f1: 0.8836\n",
      "Epoch 8, GStep 40600, AvgCombinedLoss: 2.3749, LR: 1.65e-05\n",
      "  - Avg MLM Loss: 1.7000\n",
      "  - Avg DATE Loss: 0.2283\n",
      "  - Avg MATERIAL Loss: 0.2294\n",
      "  - Avg POS Loss: 0.2172\n",
      "Epoch 8, GStep 40700, AvgCombinedLoss: 2.4169, LR: 1.63e-05\n",
      "  - Avg MLM Loss: 1.7513\n",
      "  - Avg DATE Loss: 0.2185\n",
      "  - Avg MATERIAL Loss: 0.2399\n",
      "  - Avg POS Loss: 0.2072\n",
      "Epoch 8, GStep 40800, AvgCombinedLoss: 2.3683, LR: 1.62e-05\n",
      "  - Avg MLM Loss: 1.7245\n",
      "  - Avg DATE Loss: 0.2044\n",
      "  - Avg MATERIAL Loss: 0.2342\n",
      "  - Avg POS Loss: 0.2052\n",
      "Epoch 8, GStep 40900, AvgCombinedLoss: 2.3819, LR: 1.60e-05\n",
      "  - Avg MLM Loss: 1.7138\n",
      "  - Avg DATE Loss: 0.2267\n",
      "  - Avg MATERIAL Loss: 0.2358\n",
      "  - Avg POS Loss: 0.2056\n",
      "Epoch 8, GStep 41000, AvgCombinedLoss: 2.3672, LR: 1.58e-05\n",
      "  - Avg MLM Loss: 1.7151\n",
      "  - Avg DATE Loss: 0.2061\n",
      "  - Avg MATERIAL Loss: 0.2359\n",
      "  - Avg POS Loss: 0.2101\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-41000\n",
      "Epoch 8, GStep 41100, AvgCombinedLoss: 2.3447, LR: 1.57e-05\n",
      "  - Avg MLM Loss: 1.6984\n",
      "  - Avg DATE Loss: 0.2069\n",
      "  - Avg MATERIAL Loss: 0.2342\n",
      "  - Avg POS Loss: 0.2053\n",
      "Epoch 8, GStep 41200, AvgCombinedLoss: 2.3635, LR: 1.55e-05\n",
      "  - Avg MLM Loss: 1.6793\n",
      "  - Avg DATE Loss: 0.2268\n",
      "  - Avg MATERIAL Loss: 0.2503\n",
      "  - Avg POS Loss: 0.2071\n",
      "Epoch 8, GStep 41300, AvgCombinedLoss: 2.2908, LR: 1.53e-05\n",
      "  - Avg MLM Loss: 1.6307\n",
      "  - Avg DATE Loss: 0.2057\n",
      "  - Avg MATERIAL Loss: 0.2438\n",
      "  - Avg POS Loss: 0.2106\n",
      "Epoch 8, GStep 41400, AvgCombinedLoss: 2.3865, LR: 1.52e-05\n",
      "  - Avg MLM Loss: 1.6990\n",
      "  - Avg DATE Loss: 0.2338\n",
      "  - Avg MATERIAL Loss: 0.2457\n",
      "  - Avg POS Loss: 0.2080\n",
      "Epoch 8, GStep 41500, AvgCombinedLoss: 2.4313, LR: 1.50e-05\n",
      "  - Avg MLM Loss: 1.7651\n",
      "  - Avg DATE Loss: 0.2055\n",
      "  - Avg MATERIAL Loss: 0.2426\n",
      "  - Avg POS Loss: 0.2181\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-41500\n",
      "Epoch 8, GStep 41600, AvgCombinedLoss: 2.4384, LR: 1.49e-05\n",
      "  - Avg MLM Loss: 1.7475\n",
      "  - Avg DATE Loss: 0.2335\n",
      "  - Avg MATERIAL Loss: 0.2399\n",
      "  - Avg POS Loss: 0.2175\n",
      "Epoch 8, GStep 41700, AvgCombinedLoss: 2.2940, LR: 1.47e-05\n",
      "  - Avg MLM Loss: 1.6337\n",
      "  - Avg DATE Loss: 0.2129\n",
      "  - Avg MATERIAL Loss: 0.2374\n",
      "  - Avg POS Loss: 0.2100\n",
      "Epoch 8, GStep 41800, AvgCombinedLoss: 2.2833, LR: 1.45e-05\n",
      "  - Avg MLM Loss: 1.6304\n",
      "  - Avg DATE Loss: 0.2191\n",
      "  - Avg MATERIAL Loss: 0.2270\n",
      "  - Avg POS Loss: 0.2067\n",
      "Epoch 8, GStep 41900, AvgCombinedLoss: 2.4178, LR: 1.44e-05\n",
      "  - Avg MLM Loss: 1.7496\n",
      "  - Avg DATE Loss: 0.2131\n",
      "  - Avg MATERIAL Loss: 0.2405\n",
      "  - Avg POS Loss: 0.2145\n",
      "Epoch 8, GStep 42000, AvgCombinedLoss: 2.3832, LR: 1.42e-05\n",
      "  - Avg MLM Loss: 1.7373\n",
      "  - Avg DATE Loss: 0.2097\n",
      "  - Avg MATERIAL Loss: 0.2316\n",
      "  - Avg POS Loss: 0.2046\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-42000\n",
      "Epoch 8, GStep 42100, AvgCombinedLoss: 2.3191, LR: 1.40e-05\n",
      "  - Avg MLM Loss: 1.6760\n",
      "  - Avg DATE Loss: 0.2165\n",
      "  - Avg MATERIAL Loss: 0.2210\n",
      "  - Avg POS Loss: 0.2056\n",
      "Epoch 8, GStep 42200, AvgCombinedLoss: 2.4475, LR: 1.39e-05\n",
      "  - Avg MLM Loss: 1.7822\n",
      "  - Avg DATE Loss: 0.2138\n",
      "  - Avg MATERIAL Loss: 0.2312\n",
      "  - Avg POS Loss: 0.2203\n",
      "Epoch 8, GStep 42300, AvgCombinedLoss: 2.3974, LR: 1.37e-05\n",
      "  - Avg MLM Loss: 1.7490\n",
      "  - Avg DATE Loss: 0.2194\n",
      "  - Avg MATERIAL Loss: 0.2253\n",
      "  - Avg POS Loss: 0.2037\n",
      "Epoch 8, GStep 42400, AvgCombinedLoss: 2.3178, LR: 1.35e-05\n",
      "  - Avg MLM Loss: 1.6391\n",
      "  - Avg DATE Loss: 0.2184\n",
      "  - Avg MATERIAL Loss: 0.2504\n",
      "  - Avg POS Loss: 0.2098\n",
      "Epoch 8, GStep 42500, AvgCombinedLoss: 2.3641, LR: 1.34e-05\n",
      "  - Avg MLM Loss: 1.7097\n",
      "  - Avg DATE Loss: 0.2155\n",
      "  - Avg MATERIAL Loss: 0.2325\n",
      "  - Avg POS Loss: 0.2064\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-42500\n",
      "Epoch 8, GStep 42600, AvgCombinedLoss: 2.3071, LR: 1.32e-05\n",
      "  - Avg MLM Loss: 1.6347\n",
      "  - Avg DATE Loss: 0.2337\n",
      "  - Avg MATERIAL Loss: 0.2250\n",
      "  - Avg POS Loss: 0.2137\n",
      "Epoch 8, GStep 42700, AvgCombinedLoss: 2.3672, LR: 1.30e-05\n",
      "  - Avg MLM Loss: 1.7006\n",
      "  - Avg DATE Loss: 0.2320\n",
      "  - Avg MATERIAL Loss: 0.2248\n",
      "  - Avg POS Loss: 0.2097\n",
      "Epoch 8, GStep 42800, AvgCombinedLoss: 2.3977, LR: 1.29e-05\n",
      "  - Avg MLM Loss: 1.7451\n",
      "  - Avg DATE Loss: 0.2136\n",
      "  - Avg MATERIAL Loss: 0.2293\n",
      "  - Avg POS Loss: 0.2097\n",
      "Epoch 8, GStep 42900, AvgCombinedLoss: 2.3632, LR: 1.27e-05\n",
      "  - Avg MLM Loss: 1.7202\n",
      "  - Avg DATE Loss: 0.2053\n",
      "  - Avg MATERIAL Loss: 0.2342\n",
      "  - Avg POS Loss: 0.2036\n",
      "Epoch 8, GStep 43000, AvgCombinedLoss: 2.3879, LR: 1.26e-05\n",
      "  - Avg MLM Loss: 1.7297\n",
      "  - Avg DATE Loss: 0.2165\n",
      "  - Avg MATERIAL Loss: 0.2335\n",
      "  - Avg POS Loss: 0.2083\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-43000\n",
      "Epoch 8, GStep 43100, AvgCombinedLoss: 2.3981, LR: 1.24e-05\n",
      "  - Avg MLM Loss: 1.7330\n",
      "  - Avg DATE Loss: 0.2185\n",
      "  - Avg MATERIAL Loss: 0.2365\n",
      "  - Avg POS Loss: 0.2101\n",
      "Epoch 8, GStep 43200, AvgCombinedLoss: 2.3997, LR: 1.22e-05\n",
      "  - Avg MLM Loss: 1.7250\n",
      "  - Avg DATE Loss: 0.2211\n",
      "  - Avg MATERIAL Loss: 0.2352\n",
      "  - Avg POS Loss: 0.2185\n",
      "Epoch 8, GStep 43300, AvgCombinedLoss: 2.4442, LR: 1.21e-05\n",
      "  - Avg MLM Loss: 1.7874\n",
      "  - Avg DATE Loss: 0.2102\n",
      "  - Avg MATERIAL Loss: 0.2349\n",
      "  - Avg POS Loss: 0.2117\n",
      "Epoch 8, GStep 43400, AvgCombinedLoss: 2.3660, LR: 1.19e-05\n",
      "  - Avg MLM Loss: 1.6869\n",
      "  - Avg DATE Loss: 0.2147\n",
      "  - Avg MATERIAL Loss: 0.2512\n",
      "  - Avg POS Loss: 0.2132\n",
      "Epoch 8, GStep 43500, AvgCombinedLoss: 2.3806, LR: 1.17e-05\n",
      "  - Avg MLM Loss: 1.7039\n",
      "  - Avg DATE Loss: 0.2212\n",
      "  - Avg MATERIAL Loss: 0.2448\n",
      "  - Avg POS Loss: 0.2107\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-43500\n",
      "Epoch 8, GStep 43600, AvgCombinedLoss: 2.3995, LR: 1.16e-05\n",
      "  - Avg MLM Loss: 1.7432\n",
      "  - Avg DATE Loss: 0.2224\n",
      "  - Avg MATERIAL Loss: 0.2299\n",
      "  - Avg POS Loss: 0.2040\n",
      "Epoch 8, GStep 43700, AvgCombinedLoss: 2.4136, LR: 1.14e-05\n",
      "  - Avg MLM Loss: 1.7664\n",
      "  - Avg DATE Loss: 0.2135\n",
      "  - Avg MATERIAL Loss: 0.2251\n",
      "  - Avg POS Loss: 0.2086\n",
      "Epoch 8, GStep 43800, AvgCombinedLoss: 2.5187, LR: 1.12e-05\n",
      "  - Avg MLM Loss: 1.8500\n",
      "  - Avg DATE Loss: 0.2274\n",
      "  - Avg MATERIAL Loss: 0.2233\n",
      "  - Avg POS Loss: 0.2181\n",
      "Epoch 8, GStep 43900, AvgCombinedLoss: 2.4159, LR: 1.11e-05\n",
      "  - Avg MLM Loss: 1.7348\n",
      "  - Avg DATE Loss: 0.2397\n",
      "  - Avg MATERIAL Loss: 0.2407\n",
      "  - Avg POS Loss: 0.2007\n",
      "Epoch 8, GStep 44000, AvgCombinedLoss: 2.3847, LR: 1.09e-05\n",
      "  - Avg MLM Loss: 1.7201\n",
      "  - Avg DATE Loss: 0.2124\n",
      "  - Avg MATERIAL Loss: 0.2339\n",
      "  - Avg POS Loss: 0.2184\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-44000\n",
      "Epoch 8, GStep 44100, AvgCombinedLoss: 2.2999, LR: 1.08e-05\n",
      "  - Avg MLM Loss: 1.6435\n",
      "  - Avg DATE Loss: 0.2157\n",
      "  - Avg MATERIAL Loss: 0.2335\n",
      "  - Avg POS Loss: 0.2071\n",
      "Epoch 8, GStep 44200, AvgCombinedLoss: 2.3479, LR: 1.06e-05\n",
      "  - Avg MLM Loss: 1.6527\n",
      "  - Avg DATE Loss: 0.2552\n",
      "  - Avg MATERIAL Loss: 0.2371\n",
      "  - Avg POS Loss: 0.2028\n",
      "Epoch 8, GStep 44300, AvgCombinedLoss: 2.3864, LR: 1.04e-05\n",
      "  - Avg MLM Loss: 1.7094\n",
      "  - Avg DATE Loss: 0.2183\n",
      "  - Avg MATERIAL Loss: 0.2486\n",
      "  - Avg POS Loss: 0.2100\n",
      "Epoch 8, GStep 44400, AvgCombinedLoss: 2.3899, LR: 1.03e-05\n",
      "  - Avg MLM Loss: 1.7279\n",
      "  - Avg DATE Loss: 0.2180\n",
      "  - Avg MATERIAL Loss: 0.2350\n",
      "  - Avg POS Loss: 0.2090\n",
      "Epoch 8, GStep 44500, AvgCombinedLoss: 2.4352, LR: 1.01e-05\n",
      "  - Avg MLM Loss: 1.7483\n",
      "  - Avg DATE Loss: 0.2334\n",
      "  - Avg MATERIAL Loss: 0.2381\n",
      "  - Avg POS Loss: 0.2154\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-44500\n",
      "Epoch 8, GStep 44600, AvgCombinedLoss: 2.3022, LR: 9.93e-06\n",
      "  - Avg MLM Loss: 1.6354\n",
      "  - Avg DATE Loss: 0.2169\n",
      "  - Avg MATERIAL Loss: 0.2378\n",
      "  - Avg POS Loss: 0.2121\n",
      "Epoch 8, GStep 44700, AvgCombinedLoss: 2.3251, LR: 9.77e-06\n",
      "  - Avg MLM Loss: 1.6649\n",
      "  - Avg DATE Loss: 0.2085\n",
      "  - Avg MATERIAL Loss: 0.2412\n",
      "  - Avg POS Loss: 0.2105\n",
      "Epoch 8, GStep 44800, AvgCombinedLoss: 2.3411, LR: 9.60e-06\n",
      "  - Avg MLM Loss: 1.6718\n",
      "  - Avg DATE Loss: 0.2194\n",
      "  - Avg MATERIAL Loss: 0.2337\n",
      "  - Avg POS Loss: 0.2162\n",
      "Epoch 8, GStep 44900, AvgCombinedLoss: 2.3034, LR: 9.44e-06\n",
      "  - Avg MLM Loss: 1.6530\n",
      "  - Avg DATE Loss: 0.2121\n",
      "  - Avg MATERIAL Loss: 0.2356\n",
      "  - Avg POS Loss: 0.2027\n",
      "Epoch 8, GStep 45000, AvgCombinedLoss: 2.4215, LR: 9.27e-06\n",
      "  - Avg MLM Loss: 1.7705\n",
      "  - Avg DATE Loss: 0.2112\n",
      "  - Avg MATERIAL Loss: 0.2334\n",
      "  - Avg POS Loss: 0.2064\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-45000\n",
      "Epoch 8, GStep 45100, AvgCombinedLoss: 2.4417, LR: 9.11e-06\n",
      "  - Avg MLM Loss: 1.7661\n",
      "  - Avg DATE Loss: 0.2282\n",
      "  - Avg MATERIAL Loss: 0.2387\n",
      "  - Avg POS Loss: 0.2087\n",
      "Epoch 8, GStep 45200, AvgCombinedLoss: 2.3049, LR: 8.94e-06\n",
      "  - Avg MLM Loss: 1.6408\n",
      "  - Avg DATE Loss: 0.2230\n",
      "  - Avg MATERIAL Loss: 0.2344\n",
      "  - Avg POS Loss: 0.2067\n",
      "Epoch 8, GStep 45300, AvgCombinedLoss: 2.3606, LR: 8.78e-06\n",
      "  - Avg MLM Loss: 1.6939\n",
      "  - Avg DATE Loss: 0.2201\n",
      "  - Avg MATERIAL Loss: 0.2387\n",
      "  - Avg POS Loss: 0.2079\n",
      "Epoch 8, GStep 45400, AvgCombinedLoss: 2.3385, LR: 8.62e-06\n",
      "  - Avg MLM Loss: 1.6752\n",
      "  - Avg DATE Loss: 0.2186\n",
      "  - Avg MATERIAL Loss: 0.2349\n",
      "  - Avg POS Loss: 0.2098\n",
      "Epoch 8, GStep 45500, AvgCombinedLoss: 2.3725, LR: 8.45e-06\n",
      "  - Avg MLM Loss: 1.7073\n",
      "  - Avg DATE Loss: 0.2219\n",
      "  - Avg MATERIAL Loss: 0.2309\n",
      "  - Avg POS Loss: 0.2125\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-45500\n",
      "--- Evaluation results after epoch 8 ---\n",
      "  eval_loss_combined: 2.7050\n",
      "  eval_mlm_loss: 1.7313\n",
      "  eval_date_loss: 0.9269\n",
      "  eval_material_loss: 1.3870\n",
      "  eval_pos_loss: 0.3327\n",
      "  eval_mlm_perplexity: 5.6479\n",
      "  eval_mlm_accuracy: 0.6982\n",
      "  eval_date_accuracy: 0.7026\n",
      "  eval_date_precision: 0.6907\n",
      "  eval_date_recall: 0.7026\n",
      "  eval_date_f1: 0.6921\n",
      "  eval_material_accuracy: 0.6225\n",
      "  eval_material_precision: 0.5378\n",
      "  eval_material_recall: 0.6225\n",
      "  eval_material_f1: 0.5589\n",
      "  eval_pos_accuracy: 0.8871\n",
      "  eval_pos_precision: 0.8871\n",
      "  eval_pos_recall: 0.8871\n",
      "  eval_pos_f1: 0.8862\n",
      "Saving best model with eval combined loss 2.7050 to runs/multitask_mlm_date_material_pos\\best_model\n",
      "Epoch 9, GStep 45600, AvgCombinedLoss: 2.3348, LR: 8.29e-06\n",
      "  - Avg MLM Loss: 1.6783\n",
      "  - Avg DATE Loss: 0.2242\n",
      "  - Avg MATERIAL Loss: 0.2274\n",
      "  - Avg POS Loss: 0.2049\n",
      "Epoch 9, GStep 45700, AvgCombinedLoss: 2.3994, LR: 8.12e-06\n",
      "  - Avg MLM Loss: 1.7605\n",
      "  - Avg DATE Loss: 0.1930\n",
      "  - Avg MATERIAL Loss: 0.2411\n",
      "  - Avg POS Loss: 0.2048\n",
      "Epoch 9, GStep 45800, AvgCombinedLoss: 2.2704, LR: 7.96e-06\n",
      "  - Avg MLM Loss: 1.6545\n",
      "  - Avg DATE Loss: 0.1974\n",
      "  - Avg MATERIAL Loss: 0.2191\n",
      "  - Avg POS Loss: 0.1994\n",
      "Epoch 9, GStep 45900, AvgCombinedLoss: 2.2773, LR: 7.80e-06\n",
      "  - Avg MLM Loss: 1.6768\n",
      "  - Avg DATE Loss: 0.1769\n",
      "  - Avg MATERIAL Loss: 0.2130\n",
      "  - Avg POS Loss: 0.2106\n",
      "Epoch 9, GStep 46000, AvgCombinedLoss: 2.4469, LR: 7.63e-06\n",
      "  - Avg MLM Loss: 1.7933\n",
      "  - Avg DATE Loss: 0.1999\n",
      "  - Avg MATERIAL Loss: 0.2422\n",
      "  - Avg POS Loss: 0.2115\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-46000\n",
      "Epoch 9, GStep 46100, AvgCombinedLoss: 2.2957, LR: 7.47e-06\n",
      "  - Avg MLM Loss: 1.6648\n",
      "  - Avg DATE Loss: 0.2025\n",
      "  - Avg MATERIAL Loss: 0.2237\n",
      "  - Avg POS Loss: 0.2047\n",
      "Epoch 9, GStep 46200, AvgCombinedLoss: 2.3629, LR: 7.30e-06\n",
      "  - Avg MLM Loss: 1.7366\n",
      "  - Avg DATE Loss: 0.1928\n",
      "  - Avg MATERIAL Loss: 0.2271\n",
      "  - Avg POS Loss: 0.2064\n",
      "Epoch 9, GStep 46300, AvgCombinedLoss: 2.3420, LR: 7.14e-06\n",
      "  - Avg MLM Loss: 1.7221\n",
      "  - Avg DATE Loss: 0.1974\n",
      "  - Avg MATERIAL Loss: 0.2173\n",
      "  - Avg POS Loss: 0.2052\n",
      "Epoch 9, GStep 46400, AvgCombinedLoss: 2.3049, LR: 6.98e-06\n",
      "  - Avg MLM Loss: 1.6894\n",
      "  - Avg DATE Loss: 0.1911\n",
      "  - Avg MATERIAL Loss: 0.2198\n",
      "  - Avg POS Loss: 0.2046\n",
      "Epoch 9, GStep 46500, AvgCombinedLoss: 2.4011, LR: 6.81e-06\n",
      "  - Avg MLM Loss: 1.7753\n",
      "  - Avg DATE Loss: 0.1953\n",
      "  - Avg MATERIAL Loss: 0.2296\n",
      "  - Avg POS Loss: 0.2009\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-46500\n",
      "Epoch 9, GStep 46600, AvgCombinedLoss: 2.2880, LR: 6.65e-06\n",
      "  - Avg MLM Loss: 1.6927\n",
      "  - Avg DATE Loss: 0.1735\n",
      "  - Avg MATERIAL Loss: 0.2209\n",
      "  - Avg POS Loss: 0.2009\n",
      "Epoch 9, GStep 46700, AvgCombinedLoss: 2.2639, LR: 6.48e-06\n",
      "  - Avg MLM Loss: 1.6479\n",
      "  - Avg DATE Loss: 0.1939\n",
      "  - Avg MATERIAL Loss: 0.2131\n",
      "  - Avg POS Loss: 0.2090\n",
      "Epoch 9, GStep 46800, AvgCombinedLoss: 2.2646, LR: 6.32e-06\n",
      "  - Avg MLM Loss: 1.6569\n",
      "  - Avg DATE Loss: 0.1869\n",
      "  - Avg MATERIAL Loss: 0.2132\n",
      "  - Avg POS Loss: 0.2075\n",
      "Epoch 9, GStep 46900, AvgCombinedLoss: 2.2828, LR: 6.15e-06\n",
      "  - Avg MLM Loss: 1.6442\n",
      "  - Avg DATE Loss: 0.2006\n",
      "  - Avg MATERIAL Loss: 0.2413\n",
      "  - Avg POS Loss: 0.1966\n",
      "Epoch 9, GStep 47000, AvgCombinedLoss: 2.2603, LR: 5.99e-06\n",
      "  - Avg MLM Loss: 1.6478\n",
      "  - Avg DATE Loss: 0.1889\n",
      "  - Avg MATERIAL Loss: 0.2242\n",
      "  - Avg POS Loss: 0.1993\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-47000\n",
      "Epoch 9, GStep 47100, AvgCombinedLoss: 2.2605, LR: 5.83e-06\n",
      "  - Avg MLM Loss: 1.6538\n",
      "  - Avg DATE Loss: 0.1928\n",
      "  - Avg MATERIAL Loss: 0.2160\n",
      "  - Avg POS Loss: 0.1979\n",
      "Epoch 9, GStep 47200, AvgCombinedLoss: 2.3508, LR: 5.66e-06\n",
      "  - Avg MLM Loss: 1.7090\n",
      "  - Avg DATE Loss: 0.2089\n",
      "  - Avg MATERIAL Loss: 0.2267\n",
      "  - Avg POS Loss: 0.2062\n",
      "Epoch 9, GStep 47300, AvgCombinedLoss: 2.3813, LR: 5.50e-06\n",
      "  - Avg MLM Loss: 1.7580\n",
      "  - Avg DATE Loss: 0.1870\n",
      "  - Avg MATERIAL Loss: 0.2246\n",
      "  - Avg POS Loss: 0.2116\n",
      "Epoch 9, GStep 47400, AvgCombinedLoss: 2.3569, LR: 5.33e-06\n",
      "  - Avg MLM Loss: 1.7211\n",
      "  - Avg DATE Loss: 0.2011\n",
      "  - Avg MATERIAL Loss: 0.2327\n",
      "  - Avg POS Loss: 0.2021\n",
      "Epoch 9, GStep 47500, AvgCombinedLoss: 2.3473, LR: 5.17e-06\n",
      "  - Avg MLM Loss: 1.7435\n",
      "  - Avg DATE Loss: 0.1732\n",
      "  - Avg MATERIAL Loss: 0.2277\n",
      "  - Avg POS Loss: 0.2030\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-47500\n",
      "Epoch 9, GStep 47600, AvgCombinedLoss: 2.2631, LR: 5.01e-06\n",
      "  - Avg MLM Loss: 1.6261\n",
      "  - Avg DATE Loss: 0.2001\n",
      "  - Avg MATERIAL Loss: 0.2345\n",
      "  - Avg POS Loss: 0.2025\n",
      "Epoch 9, GStep 47700, AvgCombinedLoss: 2.2703, LR: 4.84e-06\n",
      "  - Avg MLM Loss: 1.6493\n",
      "  - Avg DATE Loss: 0.1970\n",
      "  - Avg MATERIAL Loss: 0.2168\n",
      "  - Avg POS Loss: 0.2072\n",
      "Epoch 9, GStep 47800, AvgCombinedLoss: 2.3520, LR: 4.68e-06\n",
      "  - Avg MLM Loss: 1.6979\n",
      "  - Avg DATE Loss: 0.2077\n",
      "  - Avg MATERIAL Loss: 0.2400\n",
      "  - Avg POS Loss: 0.2064\n",
      "Epoch 9, GStep 47900, AvgCombinedLoss: 2.2989, LR: 4.51e-06\n",
      "  - Avg MLM Loss: 1.6745\n",
      "  - Avg DATE Loss: 0.1980\n",
      "  - Avg MATERIAL Loss: 0.2276\n",
      "  - Avg POS Loss: 0.1987\n",
      "Epoch 9, GStep 48000, AvgCombinedLoss: 2.2904, LR: 4.35e-06\n",
      "  - Avg MLM Loss: 1.6665\n",
      "  - Avg DATE Loss: 0.2068\n",
      "  - Avg MATERIAL Loss: 0.2242\n",
      "  - Avg POS Loss: 0.1928\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-48000\n",
      "Epoch 9, GStep 48100, AvgCombinedLoss: 2.3068, LR: 4.19e-06\n",
      "  - Avg MLM Loss: 1.6781\n",
      "  - Avg DATE Loss: 0.1851\n",
      "  - Avg MATERIAL Loss: 0.2439\n",
      "  - Avg POS Loss: 0.1997\n",
      "Epoch 9, GStep 48200, AvgCombinedLoss: 2.2131, LR: 4.02e-06\n",
      "  - Avg MLM Loss: 1.5895\n",
      "  - Avg DATE Loss: 0.1943\n",
      "  - Avg MATERIAL Loss: 0.2319\n",
      "  - Avg POS Loss: 0.1974\n",
      "Epoch 9, GStep 48300, AvgCombinedLoss: 2.3760, LR: 3.86e-06\n",
      "  - Avg MLM Loss: 1.7245\n",
      "  - Avg DATE Loss: 0.2094\n",
      "  - Avg MATERIAL Loss: 0.2366\n",
      "  - Avg POS Loss: 0.2056\n",
      "Epoch 9, GStep 48400, AvgCombinedLoss: 2.2267, LR: 3.69e-06\n",
      "  - Avg MLM Loss: 1.6073\n",
      "  - Avg DATE Loss: 0.1842\n",
      "  - Avg MATERIAL Loss: 0.2327\n",
      "  - Avg POS Loss: 0.2026\n",
      "Epoch 9, GStep 48500, AvgCombinedLoss: 2.2838, LR: 3.53e-06\n",
      "  - Avg MLM Loss: 1.6558\n",
      "  - Avg DATE Loss: 0.1959\n",
      "  - Avg MATERIAL Loss: 0.2317\n",
      "  - Avg POS Loss: 0.2005\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-48500\n",
      "Epoch 9, GStep 48600, AvgCombinedLoss: 2.3150, LR: 3.36e-06\n",
      "  - Avg MLM Loss: 1.6789\n",
      "  - Avg DATE Loss: 0.2099\n",
      "  - Avg MATERIAL Loss: 0.2212\n",
      "  - Avg POS Loss: 0.2050\n",
      "Epoch 9, GStep 48700, AvgCombinedLoss: 2.3324, LR: 3.20e-06\n",
      "  - Avg MLM Loss: 1.6898\n",
      "  - Avg DATE Loss: 0.2004\n",
      "  - Avg MATERIAL Loss: 0.2324\n",
      "  - Avg POS Loss: 0.2099\n",
      "Epoch 9, GStep 48800, AvgCombinedLoss: 2.1773, LR: 3.04e-06\n",
      "  - Avg MLM Loss: 1.5499\n",
      "  - Avg DATE Loss: 0.1973\n",
      "  - Avg MATERIAL Loss: 0.2317\n",
      "  - Avg POS Loss: 0.1984\n",
      "Epoch 9, GStep 48900, AvgCombinedLoss: 2.3547, LR: 2.87e-06\n",
      "  - Avg MLM Loss: 1.7049\n",
      "  - Avg DATE Loss: 0.2089\n",
      "  - Avg MATERIAL Loss: 0.2375\n",
      "  - Avg POS Loss: 0.2034\n",
      "Epoch 9, GStep 49000, AvgCombinedLoss: 2.2603, LR: 2.71e-06\n",
      "  - Avg MLM Loss: 1.6516\n",
      "  - Avg DATE Loss: 0.1810\n",
      "  - Avg MATERIAL Loss: 0.2303\n",
      "  - Avg POS Loss: 0.1974\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-49000\n",
      "Epoch 9, GStep 49100, AvgCombinedLoss: 2.1852, LR: 2.54e-06\n",
      "  - Avg MLM Loss: 1.5649\n",
      "  - Avg DATE Loss: 0.1955\n",
      "  - Avg MATERIAL Loss: 0.2266\n",
      "  - Avg POS Loss: 0.1983\n",
      "Epoch 9, GStep 49200, AvgCombinedLoss: 2.2905, LR: 2.38e-06\n",
      "  - Avg MLM Loss: 1.6774\n",
      "  - Avg DATE Loss: 0.1885\n",
      "  - Avg MATERIAL Loss: 0.2267\n",
      "  - Avg POS Loss: 0.1979\n",
      "Epoch 9, GStep 49300, AvgCombinedLoss: 2.3417, LR: 2.22e-06\n",
      "  - Avg MLM Loss: 1.7277\n",
      "  - Avg DATE Loss: 0.1895\n",
      "  - Avg MATERIAL Loss: 0.2256\n",
      "  - Avg POS Loss: 0.1990\n",
      "Epoch 9, GStep 49400, AvgCombinedLoss: 2.3038, LR: 2.05e-06\n",
      "  - Avg MLM Loss: 1.6507\n",
      "  - Avg DATE Loss: 0.2030\n",
      "  - Avg MATERIAL Loss: 0.2460\n",
      "  - Avg POS Loss: 0.2042\n",
      "Epoch 9, GStep 49500, AvgCombinedLoss: 2.3691, LR: 1.89e-06\n",
      "  - Avg MLM Loss: 1.7219\n",
      "  - Avg DATE Loss: 0.2025\n",
      "  - Avg MATERIAL Loss: 0.2343\n",
      "  - Avg POS Loss: 0.2104\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-49500\n",
      "Epoch 9, GStep 49600, AvgCombinedLoss: 2.2494, LR: 1.72e-06\n",
      "  - Avg MLM Loss: 1.6302\n",
      "  - Avg DATE Loss: 0.1926\n",
      "  - Avg MATERIAL Loss: 0.2235\n",
      "  - Avg POS Loss: 0.2031\n",
      "Epoch 9, GStep 49700, AvgCombinedLoss: 2.3626, LR: 1.56e-06\n",
      "  - Avg MLM Loss: 1.7273\n",
      "  - Avg DATE Loss: 0.1899\n",
      "  - Avg MATERIAL Loss: 0.2368\n",
      "  - Avg POS Loss: 0.2085\n",
      "Epoch 9, GStep 49800, AvgCombinedLoss: 2.3085, LR: 1.40e-06\n",
      "  - Avg MLM Loss: 1.6701\n",
      "  - Avg DATE Loss: 0.1995\n",
      "  - Avg MATERIAL Loss: 0.2392\n",
      "  - Avg POS Loss: 0.1997\n",
      "Epoch 9, GStep 49900, AvgCombinedLoss: 2.3370, LR: 1.23e-06\n",
      "  - Avg MLM Loss: 1.7194\n",
      "  - Avg DATE Loss: 0.1852\n",
      "  - Avg MATERIAL Loss: 0.2287\n",
      "  - Avg POS Loss: 0.2037\n",
      "Epoch 9, GStep 50000, AvgCombinedLoss: 2.3178, LR: 1.07e-06\n",
      "  - Avg MLM Loss: 1.7056\n",
      "  - Avg DATE Loss: 0.1832\n",
      "  - Avg MATERIAL Loss: 0.2335\n",
      "  - Avg POS Loss: 0.1956\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-50000\n",
      "Epoch 9, GStep 50100, AvgCombinedLoss: 2.3865, LR: 9.03e-07\n",
      "  - Avg MLM Loss: 1.7515\n",
      "  - Avg DATE Loss: 0.2057\n",
      "  - Avg MATERIAL Loss: 0.2278\n",
      "  - Avg POS Loss: 0.2015\n",
      "Epoch 9, GStep 50200, AvgCombinedLoss: 2.2179, LR: 7.39e-07\n",
      "  - Avg MLM Loss: 1.5886\n",
      "  - Avg DATE Loss: 0.1871\n",
      "  - Avg MATERIAL Loss: 0.2472\n",
      "  - Avg POS Loss: 0.1950\n",
      "Epoch 9, GStep 50300, AvgCombinedLoss: 2.2171, LR: 5.74e-07\n",
      "  - Avg MLM Loss: 1.6171\n",
      "  - Avg DATE Loss: 0.1823\n",
      "  - Avg MATERIAL Loss: 0.2154\n",
      "  - Avg POS Loss: 0.2023\n",
      "Epoch 9, GStep 50400, AvgCombinedLoss: 2.2296, LR: 4.10e-07\n",
      "  - Avg MLM Loss: 1.6171\n",
      "  - Avg DATE Loss: 0.1825\n",
      "  - Avg MATERIAL Loss: 0.2241\n",
      "  - Avg POS Loss: 0.2059\n",
      "Epoch 9, GStep 50500, AvgCombinedLoss: 2.2708, LR: 2.46e-07\n",
      "  - Avg MLM Loss: 1.6394\n",
      "  - Avg DATE Loss: 0.1955\n",
      "  - Avg MATERIAL Loss: 0.2379\n",
      "  - Avg POS Loss: 0.1980\n",
      "Saving model checkpoint to runs/multitask_mlm_date_material_pos\\checkpoint-50500\n",
      "Epoch 9, GStep 50600, AvgCombinedLoss: 2.3118, LR: 8.21e-08\n",
      "  - Avg MLM Loss: 1.6996\n",
      "  - Avg DATE Loss: 0.1785\n",
      "  - Avg MATERIAL Loss: 0.2296\n",
      "  - Avg POS Loss: 0.2042\n",
      "--- Evaluation results after epoch 9 ---\n",
      "  eval_loss_combined: 2.7470\n",
      "  eval_mlm_loss: 1.7521\n",
      "  eval_date_loss: 0.9691\n",
      "  eval_material_loss: 1.3898\n",
      "  eval_pos_loss: 0.3320\n",
      "  eval_mlm_perplexity: 5.7667\n",
      "  eval_mlm_accuracy: 0.6970\n",
      "  eval_date_accuracy: 0.6971\n",
      "  eval_date_precision: 0.6864\n",
      "  eval_date_recall: 0.6971\n",
      "  eval_date_f1: 0.6896\n",
      "  eval_material_accuracy: 0.6212\n",
      "  eval_material_precision: 0.5284\n",
      "  eval_material_recall: 0.6212\n",
      "  eval_material_f1: 0.5554\n",
      "  eval_pos_accuracy: 0.8874\n",
      "  eval_pos_precision: 0.8872\n",
      "  eval_pos_recall: 0.8874\n",
      "  eval_pos_f1: 0.8866\n",
      "Closing training examples log file.\n",
      "Saving final model to runs/multitask_mlm_date_material_pos\\final_model\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "def create_vocabs_from_data(data_file_path):\n",
    "    print(f\"Scanning {data_file_path} to build vocabularies for Material and POS...\")\n",
    "    with open(data_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    material_set = set()\n",
    "    pos_tag_set = set()\n",
    "    \n",
    "    for record in data:\n",
    "        material = record.get(\"material\")\n",
    "        if material and isinstance(material, str):\n",
    "            material_set.add(material.lower().strip())\n",
    "        \n",
    "        pos_tags = record.get(\"pos_tags\")\n",
    "        if pos_tags and isinstance(pos_tags, list):\n",
    "            for _, tag in pos_tags:\n",
    "                if tag and isinstance(tag, str):\n",
    "                    pos_tag_set.add(tag)\n",
    "    \n",
    "    material_vocab = {name: i for i, name in enumerate(sorted(list(material_set)))}\n",
    "    if \"Unknown\" not in material_vocab:\n",
    "        material_vocab[\"Unknown\"] = len(material_vocab)\n",
    "\n",
    "    pos_vocab = {name: i for i, name in enumerate(sorted(list(pos_tag_set)))}\n",
    "    if \"Unknown\" not in pos_vocab:\n",
    "        pos_vocab[\"Unknown\"] = len(pos_vocab)\n",
    "        \n",
    "    print(f\"Found {len(material_vocab)} unique materials.\")\n",
    "    print(f\"Found {len(pos_vocab)} unique POS tags.\")\n",
    "    return material_vocab, pos_vocab\n",
    "\n",
    "random.seed(args[\"seed\"])\n",
    "np.random.seed(args[\"seed\"])\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args[\"seed\"])\n",
    "\n",
    "material_to_id, pos_to_id = create_vocabs_from_data(args[\"source_data_file\"])\n",
    "args[\"num_material_labels\"] = len(material_to_id)\n",
    "args[\"num_pos_labels\"] = len(pos_to_id)\n",
    "\n",
    "material_class_names = [name for name, _ in sorted(material_to_id.items(), key=lambda item: item[1])]\n",
    "pos_class_names = [name for name, _ in sorted(pos_to_id.items(), key=lambda item: item[1])]\n",
    "\n",
    "encoder = text_encoder.SubwordTextEncoder(args[\"tokenizer_path\"])\n",
    "tokenizer = LatinTokenizer(encoder)\n",
    "print(f\"Vocabulary size (for MLM): {tokenizer.get_vocab_size()} tokens\")\n",
    "\n",
    "if args.get(\"predict\", False):\n",
    "    print(f\"\\nRunning in PREDICTION mode for text: '{args['predict_text']}'\")\n",
    "    predict_model_path = os.path.join(args[\"output_dir\"], \"best_model\")\n",
    "    if not os.path.exists(os.path.join(predict_model_path, \"pytorch_model.bin\")):\n",
    "        print(f\"Warning: Fine-tuned model not found at {predict_model_path}. Using base model.\")\n",
    "        predict_model_path = args[\"bert_path\"]\n",
    "\n",
    "    model = LatinBERTForMultiTask(\n",
    "        bert_path=predict_model_path,\n",
    "        num_date_labels=args[\"num_date_labels\"],\n",
    "        num_material_labels=args[\"num_material_labels\"],\n",
    "        num_pos_labels=args[\"num_pos_labels\"]\n",
    "    )\n",
    "    if os.path.exists(os.path.join(predict_model_path, \"pytorch_model.bin\")):\n",
    "        model.load_state_dict(torch.load(os.path.join(predict_model_path, \"pytorch_model.bin\"), map_location=device))\n",
    "    model.to(device)\n",
    "    \n",
    "    prediction_results = predict_missing_words(model, tokenizer, args[\"predict_text\"])\n",
    "    print(\"\\nMLM Predictions:\")\n",
    "    print(json.dumps(prediction_results, indent=2))\n",
    "\n",
    "else:\n",
    "    print(\"\\nRunning in TRAINING mode.\")\n",
    "    model = LatinBERTForMultiTask(\n",
    "        bert_path=args[\"bert_path\"],\n",
    "        num_date_labels=args[\"num_date_labels\"],\n",
    "        num_material_labels=args[\"num_material_labels\"],\n",
    "        num_pos_labels=args[\"num_pos_labels\"]\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    full_dataset = EpigraphDataset(\n",
    "        source_data_file=args[\"source_data_file\"],\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=args[\"max_seq_length\"],\n",
    "        max_unk_percentage=args[\"max_unk_percentage\"],\n",
    "        material_to_id=material_to_id,\n",
    "        pos_to_id=pos_to_id\n",
    "    )\n",
    "\n",
    "    if len(full_dataset) == 0:\n",
    "        raise ValueError(\"Dataset is empty after filtering. Check data source and filtering criteria.\")\n",
    "\n",
    "    eval_size = int(len(full_dataset) * args[\"eval_split\"])\n",
    "    if eval_size == 0 and len(full_dataset) > 1: eval_size = 1\n",
    "    train_size = len(full_dataset) - eval_size\n",
    "\n",
    "    if train_size <= 0:\n",
    "        raise ValueError(f\"Train size is not positive. Total: {len(full_dataset)}, Eval: {eval_size}.\")\n",
    "\n",
    "    train_dataset, eval_dataset = random_split(full_dataset, [train_size, eval_size], generator=torch.Generator().manual_seed(args[\"seed\"]))\n",
    "    print(f\"Training on {train_size} examples, evaluating on {eval_size} examples.\")\n",
    "\n",
    "    train(args, model, train_dataset, eval_dataset, tokenizer, pos_class_names, material_class_names)\n",
    "    print(\"\\nTraining complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23261c2c",
   "metadata": {},
   "source": [
    "## Top-K Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6031f-17b8-4242-8633-2775b101aa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Re-evaluating the best model for Top-K MLM Accuracy ---\n",
      "Loading best model from: runs/multitask_mlm_date_material_pos\\best_model\n",
      "Re-creating the evaluation dataset to ensure consistency...\n",
      "Scanning data/final_results.json to build vocabularies for Material and POS...\n",
      "Found 497 unique materials.\n",
      "Found 16 unique POS tags.\n",
      "Reading and processing data from data/final_results.json...\n",
      "Total records read: 103542\n",
      "  Skipped (no ID/text): 821\n",
      "  Records before filtering: 102721\n",
      "  Filtered (UNK > 50.0%): 114\n",
      "  Filtered (Unknown date): 12567\n",
      "  Records with missing/malformed 'pos_tags' field: 0\n",
      "Loaded 90040 examples after all filtering.\n",
      "Recreated evaluation dataset with 9004 examples.\n",
      "\n",
      "--- MLM Top-K Accuracy Results ---\n",
      "Total Masked Tokens Evaluated: 30148\n",
      "  Accuracy@1:  68.60%\n",
      "  Accuracy@5:  80.70%\n",
      "  Accuracy@10: 84.19%\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Re-evaluating the best model for Top-K MLM Accuracy ---\")\n",
    "\n",
    "best_model_path = os.path.join(args[\"output_dir\"], \"best_model\")\n",
    "model_weights_path = os.path.join(best_model_path, \"pytorch_model.bin\")\n",
    "\n",
    "if not os.path.exists(model_weights_path):\n",
    "    print(f\"Error: Best model not found at {model_weights_path}. Cannot proceed.\")\n",
    "else:\n",
    "    print(f\"Loading best model from: {best_model_path}\")\n",
    "    model_to_evaluate = LatinBERTForMultiTask(\n",
    "        bert_path=args[\"bert_path\"],\n",
    "        num_date_labels=args[\"num_date_labels\"],\n",
    "        num_material_labels=args[\"num_material_labels\"],\n",
    "        num_pos_labels=args[\"num_pos_labels\"]\n",
    "    )\n",
    "    model_to_evaluate.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
    "    model_to_evaluate.to(device)\n",
    "    model_to_evaluate.eval()\n",
    "\n",
    "    print(\"Re-creating the evaluation dataset to ensure consistency...\")\n",
    "    material_to_id_reeval, pos_to_id_reeval = create_vocabs_from_data(args[\"source_data_file\"])\n",
    "\n",
    "    full_dataset_reeval = EpigraphDataset(\n",
    "        source_data_file=args[\"source_data_file\"],\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=args[\"max_seq_length\"],\n",
    "        max_unk_percentage=args[\"max_unk_percentage\"],\n",
    "        material_to_id=material_to_id_reeval,\n",
    "        pos_to_id=pos_to_id_reeval\n",
    "    )\n",
    "\n",
    "    eval_size = int(len(full_dataset_reeval) * args[\"eval_split\"])\n",
    "    if eval_size == 0 and len(full_dataset_reeval) > 1: eval_size = 1\n",
    "    train_size = len(full_dataset_reeval) - eval_size\n",
    "\n",
    "    _, eval_dataset_reeval = random_split(\n",
    "        full_dataset_reeval, [train_size, eval_size],\n",
    "        generator=torch.Generator().manual_seed(args[\"seed\"])\n",
    "    )\n",
    "\n",
    "    eval_dataloader_reeval = DataLoader(\n",
    "        eval_dataset_reeval,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=False, \n",
    "        collate_fn=LatinDataCollatorForMultiTask(tokenizer, mlm_probability=args[\"mlm_probability\"])\n",
    "    )\n",
    "    print(f\"Recreated evaluation dataset with {len(eval_dataset_reeval)} examples.\")\n",
    "\n",
    "    mlm_total_masked_tokens = 0\n",
    "    mlm_correct_top1 = 0\n",
    "    mlm_correct_top5 = 0\n",
    "    mlm_correct_top10 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader_reeval:\n",
    "            for key, value in batch.items():\n",
    "                batch[key] = value.to(device)\n",
    "            \n",
    "            outputs = model_to_evaluate(**batch)\n",
    "\n",
    "            if outputs.get(\"mlm_logits\") is not None:\n",
    "                mlm_logits = outputs[\"mlm_logits\"]\n",
    "                mlm_labels = batch[\"masked_lm_labels\"]\n",
    "                mask = (mlm_labels != -100)\n",
    "                \n",
    "                batch_masked_count = mask.sum().item()\n",
    "                if batch_masked_count > 0:\n",
    "                    mlm_total_masked_tokens += batch_masked_count\n",
    "                    _, top_10_indices = torch.topk(mlm_logits, k=10, dim=-1)\n",
    "                    expanded_labels = mlm_labels.unsqueeze(-1)\n",
    "                    masked_top_k_matches = (expanded_labels == top_10_indices) & mask.unsqueeze(-1)\n",
    "                    \n",
    "                    mlm_correct_top1 += masked_top_k_matches[:, :, 0].sum().item()\n",
    "                    mlm_correct_top5 += masked_top_k_matches[:, :, :5].any(dim=-1).sum().item()\n",
    "                    mlm_correct_top10 += masked_top_k_matches.any(dim=-1).sum().item()\n",
    "\n",
    "    print(\"\\n--- MLM Top-K Accuracy Results ---\")\n",
    "    if mlm_total_masked_tokens > 0:\n",
    "        acc1 = (mlm_correct_top1 / mlm_total_masked_tokens) * 100\n",
    "        acc5 = (mlm_correct_top5 / mlm_total_masked_tokens) * 100\n",
    "        acc10 = (mlm_correct_top10 / mlm_total_masked_tokens) * 100\n",
    "        \n",
    "        print(f\"Total Masked Tokens Evaluated: {mlm_total_masked_tokens}\")\n",
    "        print(f\"  Accuracy@1:  {acc1:.2f}%\")\n",
    "        print(f\"  Accuracy@5:  {acc5:.2f}%\")\n",
    "        print(f\"  Accuracy@10: {acc10:.2f}%\")\n",
    "    else:\n",
    "        print(\"No masked tokens were found in the evaluation set to calculate accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108dd80",
   "metadata": {},
   "source": [
    "## Generation experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f986cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting T5 Fine-Tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\digil\\Anaconda3\\envs\\edr\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/final_results.json for T5 fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\digil\\Anaconda3\\envs\\edr\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- T5 Epoch 1/3 ---\n",
      "  Batch 500/12841, Loss: 2.9455\n",
      "  Batch 1000/12841, Loss: 2.7264\n",
      "  Batch 1500/12841, Loss: 2.1127\n",
      "  Batch 2000/12841, Loss: 2.1828\n",
      "  Batch 2500/12841, Loss: 2.0874\n",
      "  Batch 3000/12841, Loss: 2.1929\n",
      "  Batch 3500/12841, Loss: 2.4971\n",
      "  Batch 4000/12841, Loss: 1.6540\n",
      "  Batch 4500/12841, Loss: 1.7418\n",
      "  Batch 5000/12841, Loss: 2.0527\n",
      "  Batch 5500/12841, Loss: 1.9899\n",
      "  Batch 6000/12841, Loss: 2.5501\n",
      "  Batch 6500/12841, Loss: 2.2010\n",
      "  Batch 7000/12841, Loss: 2.0770\n",
      "  Batch 7500/12841, Loss: 2.0720\n",
      "  Batch 8000/12841, Loss: 2.2403\n",
      "  Batch 8500/12841, Loss: 1.8319\n",
      "  Batch 9000/12841, Loss: 1.7869\n",
      "  Batch 9500/12841, Loss: 1.9508\n",
      "  Batch 10000/12841, Loss: 1.3510\n",
      "  Batch 10500/12841, Loss: 1.9181\n",
      "  Batch 11000/12841, Loss: 1.3844\n",
      "  Batch 11500/12841, Loss: 2.1971\n",
      "  Batch 12000/12841, Loss: 1.6450\n",
      "  Batch 12500/12841, Loss: 1.7421\n",
      "--- T5 Epoch 2/3 ---\n",
      "  Batch 500/12841, Loss: 2.2259\n",
      "  Batch 1000/12841, Loss: 1.9186\n",
      "  Batch 1500/12841, Loss: 1.4745\n",
      "  Batch 2000/12841, Loss: 1.8552\n",
      "  Batch 2500/12841, Loss: 2.2826\n",
      "  Batch 3000/12841, Loss: 1.7451\n",
      "  Batch 3500/12841, Loss: 1.4526\n",
      "  Batch 4000/12841, Loss: 1.5657\n",
      "  Batch 4500/12841, Loss: 1.5454\n",
      "  Batch 5000/12841, Loss: 1.3508\n",
      "  Batch 5500/12841, Loss: 1.3413\n",
      "  Batch 6000/12841, Loss: 1.8783\n",
      "  Batch 6500/12841, Loss: 1.3109\n",
      "  Batch 7000/12841, Loss: 1.3181\n",
      "  Batch 7500/12841, Loss: 2.1728\n",
      "  Batch 8000/12841, Loss: 1.6086\n",
      "  Batch 8500/12841, Loss: 1.2668\n",
      "  Batch 9000/12841, Loss: 1.4638\n",
      "  Batch 9500/12841, Loss: 1.9395\n",
      "  Batch 10000/12841, Loss: 1.9566\n",
      "  Batch 10500/12841, Loss: 1.1849\n",
      "  Batch 11000/12841, Loss: 1.1455\n",
      "  Batch 11500/12841, Loss: 1.4482\n",
      "  Batch 12000/12841, Loss: 1.2588\n",
      "  Batch 12500/12841, Loss: 1.3132\n",
      "--- T5 Epoch 3/3 ---\n",
      "  Batch 500/12841, Loss: 1.1033\n",
      "  Batch 1000/12841, Loss: 1.3664\n",
      "  Batch 1500/12841, Loss: 1.1536\n",
      "  Batch 2000/12841, Loss: 2.6022\n",
      "  Batch 2500/12841, Loss: 1.3421\n",
      "  Batch 3000/12841, Loss: 1.8002\n",
      "  Batch 3500/12841, Loss: 2.1493\n",
      "  Batch 4000/12841, Loss: 0.5917\n",
      "  Batch 4500/12841, Loss: 1.3048\n",
      "  Batch 5000/12841, Loss: 1.7072\n",
      "  Batch 5500/12841, Loss: 1.5295\n",
      "  Batch 6000/12841, Loss: 1.7965\n",
      "  Batch 6500/12841, Loss: 1.7452\n",
      "  Batch 7000/12841, Loss: 1.8152\n",
      "  Batch 7500/12841, Loss: 1.4055\n",
      "  Batch 8000/12841, Loss: 1.3064\n",
      "  Batch 8500/12841, Loss: 1.1603\n",
      "  Batch 9000/12841, Loss: 1.2459\n",
      "  Batch 9500/12841, Loss: 1.4706\n",
      "  Batch 10000/12841, Loss: 1.5391\n",
      "  Batch 10500/12841, Loss: 1.5510\n",
      "  Batch 11000/12841, Loss: 1.6434\n",
      "  Batch 11500/12841, Loss: 1.0727\n",
      "  Batch 12000/12841, Loss: 0.8762\n",
      "  Batch 12500/12841, Loss: 1.4999\n",
      "T5 training complete. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 model saved to t5_epigraph_generator/\n",
      "\n",
      "--- T5 Generation Examples ---\n",
      "Fine-tuned T5 model loaded successfully.\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Input:  'dis manibus <extra_id_0> et sibi'\n",
      "Output: 'sibi'\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Input:  'aurelius <extra_id_0> vixit annis <extra_id_1>'\n",
      "Output: 'saturninus xxxv.'\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Input:  '<extra_id_0> coniugi karissimo'\n",
      "Output: ''\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "\n",
    "T5_MODEL_NAME = \"t5-small\"\n",
    "T5_OUTPUT_DIR = \"t5_epigraph_generator/\"\n",
    "T5_NUM_EPOCHS = 3\n",
    "T5_BATCH_SIZE = 8\n",
    "T5_LEARNING_RATE = 5e-5\n",
    "\n",
    "\n",
    "class T5DenoisingDataset(Dataset):\n",
    "    def __init__(self, tokenizer, source_data_file, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        print(f\"Loading data from {source_data_file} for T5 fine-tuning...\")\n",
    "        with open(source_data_file, 'r', encoding='utf-8') as f:\n",
    "            all_data = json.load(f)\n",
    "            \n",
    "        for record in all_data:\n",
    "            text = record.get(\"parsed_field\")\n",
    "            if text and text.strip():\n",
    "                input_text, target_text = self._create_denoising_example(text)\n",
    "                self.inputs.append(input_text)\n",
    "                self.targets.append(target_text)\n",
    "\n",
    "    def _create_denoising_example(self, text, corruption_rate=0.25):\n",
    "        words = text.split()\n",
    "        num_words_to_corrupt = int(len(words) * corruption_rate)\n",
    "        if num_words_to_corrupt == 0 and len(words) > 0:\n",
    "            num_words_to_corrupt = 1\n",
    "\n",
    "        corrupted_indices = sorted(random.sample(range(len(words)), k=num_words_to_corrupt))\n",
    "        \n",
    "        input_parts = []\n",
    "        target_parts = []\n",
    "        current_mask_id = 0\n",
    "        last_index = -1\n",
    "\n",
    "        for i in corrupted_indices:\n",
    "            if i > last_index + 1:\n",
    "                input_parts.append(\" \".join(words[last_index+1:i]))\n",
    "            \n",
    "            input_parts.append(f\"<extra_id_{current_mask_id}>\")\n",
    "            \n",
    "            target_parts.append(f\"<extra_id_{current_mask_id}>\")\n",
    "            target_parts.append(words[i])\n",
    "            \n",
    "            current_mask_id += 1\n",
    "            last_index = i\n",
    "        \n",
    "        if last_index < len(words) - 1:\n",
    "            input_parts.append(\" \".join(words[last_index+1:]))\n",
    "\n",
    "        target_parts.append(f\"<extra_id_{current_mask_id}>\")\n",
    "\n",
    "        input_text = \"infill: \" + \" \".join(input_parts)\n",
    "        target_text = \" \".join(target_parts)\n",
    "        \n",
    "        return input_text, target_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [self.inputs[idx]], max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [self.targets[idx]], max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "\n",
    "        return {\"input_ids\": source_ids, \"attention_mask\": source_mask, \"labels\": target_ids}\n",
    "\n",
    "\n",
    "print(\"\\n--- Starting T5 Fine-Tuning ---\")\n",
    "\n",
    "if os.path.exists(os.path.join(T5_OUTPUT_DIR, 'pytorch_model.bin')):\n",
    "    print(f\"Fine-tuned T5 model already found in {T5_OUTPUT_DIR}. Skipping training.\")\n",
    "else:\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_NAME)\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(T5_MODEL_NAME)\n",
    "    t5_model.to(device)\n",
    "\n",
    "    train_dataset_t5 = T5DenoisingDataset(t5_tokenizer, args[\"source_data_file\"])\n",
    "    train_dataloader_t5 = DataLoader(train_dataset_t5, batch_size=T5_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    optimizer_t5 = AdamW(t5_model.parameters(), lr=T5_LEARNING_RATE)\n",
    "\n",
    "    t5_model.train()\n",
    "    for epoch in range(T5_NUM_EPOCHS):\n",
    "        print(f\"--- T5 Epoch {epoch+1}/{T5_NUM_EPOCHS} ---\")\n",
    "        for i, batch in enumerate(train_dataloader_t5):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            labels[labels == t5_tokenizer.pad_token_id] = -100\n",
    "\n",
    "            outputs = t5_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"  Batch {i+1}/{len(train_dataloader_t5)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_t5.step()\n",
    "            optimizer_t5.zero_grad()\n",
    "\n",
    "    print(\"T5 training complete. Saving model...\")\n",
    "    os.makedirs(T5_OUTPUT_DIR, exist_ok=True)\n",
    "    t5_model.save_pretrained(T5_OUTPUT_DIR)\n",
    "    t5_tokenizer.save_pretrained(T5_OUTPUT_DIR)\n",
    "    print(f\"T5 model saved to {T5_OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- T5 Generation Examples ---\")\n",
    "\n",
    "try:\n",
    "    t5_tokenizer_ft = T5Tokenizer.from_pretrained(T5_OUTPUT_DIR)\n",
    "    t5_model_ft = T5ForConditionalGeneration.from_pretrained(T5_OUTPUT_DIR)\n",
    "    t5_model_ft.to(device)\n",
    "    t5_model_ft.eval()\n",
    "    print(\"Fine-tuned T5 model loaded successfully.\")\n",
    "\n",
    "    def generate_epigraph(prompt_text, num_beams=5, max_length=50):\n",
    "        input_text = f\"infill: {prompt_text}\"\n",
    "        \n",
    "        input_ids = t5_tokenizer_ft.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        outputs = t5_model_ft.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        generated_text = t5_tokenizer_ft.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        return generated_text\n",
    "\n",
    "    prompt1 = \"dis manibus <extra_id_0> et sibi\"\n",
    "    prompt2 = \"aurelius <extra_id_0> vixit annis <extra_id_1>\"\n",
    "    prompt3 = \"<extra_id_0> coniugi karissimo\"\n",
    "\n",
    "    print(\"\\n--- Prompt 1 ---\")\n",
    "    print(f\"Input:  '{prompt1}'\")\n",
    "    print(f\"Output: '{generate_epigraph(prompt1)}'\")\n",
    "\n",
    "    print(\"\\n--- Prompt 2 ---\")\n",
    "    print(f\"Input:  '{prompt2}'\")\n",
    "    print(f\"Output: '{generate_epigraph(prompt2)}'\")\n",
    "\n",
    "    print(\"\\n--- Prompt 3 ---\")\n",
    "    print(f\"Input:  '{prompt3}'\")\n",
    "    print(f\"Output: '{generate_epigraph(prompt3)}'\")\n",
    "\n",
    "except OSError:\n",
    "    print(f\"Could not load fine-tuned T5 model from {T5_OUTPUT_DIR}. Please run the training block first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc341a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating 20-token Epigraphs (with anti-looping) ---\n",
      "\n",
      "--- Example 1 ---\n",
      "Starting with: 'dis manibus'\n",
      "Forbidding generation of '<unk>' token (ID: 2).\n",
      "Iteration 1: (Length: 3) -> dis manibus antonino.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\digil\\Anaconda3\\envs\\edr\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: (Length: 4) -> dis manibus antonino. antonino.\n",
      "Iteration 3: (Length: 5) -> dis manibus antonino. antonino. sacrum.\n",
      "Iteration 4: (Length: 6) -> dis manibus antonino. antonino. sacrum. rhoda.\n",
      "Iteration 5: (Length: 7) -> dis manibus antonino. antonino. sacrum. rhoda. sile\n",
      "Iteration 6: (Length: 8) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit.\n",
      "Iteration 7: (Length: 9) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae.\n",
      "Iteration 8: (Length: 10) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche.\n",
      "Iteration 9: (Length: 11) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit.\n",
      "Iteration 10: (Length: 12) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic.\n",
      "Iteration 11: (Length: 13) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic. fecit.\n",
      "Iteration 12: (Length: 14) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic. fecit. hic.\n",
      "Iteration 13: (Length: 15) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic. fecit. hic. fecit.\n",
      "Iteration 14: (Length: 16) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic. fecit. hic. fecit. hic.\n",
      "Iteration 15: (Length: 17) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic. fecit. hic. fecit. hic. fecit.\n",
      "Iteration 16: (Length: 18) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic. fecit. hic. fecit. hic. fecit. hic.\n",
      "Iteration 17: (Length: 19) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic. fecit. hic. fecit. hic. fecit. hic. fecit.\n",
      "Iteration 18: (Length: 20) -> dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic. fecit. hic. fecit. hic. fecit. hic. fecit. hic.\n",
      "Target length of 20 reached. Stopping.\n",
      "\n",
      "--- FINAL GENERATED EPIGRAPH ---\n",
      "dis manibus antonino. antonino. sacrum. rhoda. sile fecit. hermiae. tyche. fecit. hic. fecit. hic. fecit. hic. fecit. hic. fecit. hic.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Starting with: 'hic iacet'\n",
      "Forbidding generation of '<unk>' token (ID: 2).\n",
      "Iteration 1: (Length: 3) -> hic iacet amoros.\n",
      "Iteration 2: (Length: 4) -> hic iacet amoros. sympho.\n",
      "Iteration 3: (Length: 5) -> hic iacet amoros. sympho. ther.\n",
      "Iteration 4: (Length: 6) -> hic iacet amoros. sympho. ther. ev.\n",
      "Iteration 5: (Length: 7) -> hic iacet amoros. sympho. ther. ev. sevir\n",
      "Iteration 6: (Length: 8) -> hic iacet amoros. sympho. ther. ev. sevir pecunia.\n",
      "Iteration 7: (Length: 9) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia.\n",
      "Iteration 8: (Length: 10) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater.\n",
      "Iteration 9: (Length: 11) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic\n",
      "Iteration 10: (Length: 12) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet\n",
      "Iteration 11: (Length: 13) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet amoros.\n",
      "Iteration 12: (Length: 14) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet amoros. sympho.\n",
      "Iteration 13: (Length: 15) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet amoros. sympho. sympho.\n",
      "Iteration 14: (Length: 16) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet amoros. sympho. sympho. sympho.\n",
      "Iteration 15: (Length: 17) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet amoros. sympho. sympho. sympho. sympho.\n",
      "Iteration 16: (Length: 18) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet amoros. sympho. sympho. sympho. sympho. sympho.\n",
      "Iteration 17: (Length: 19) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet amoros. sympho. sympho. sympho. sympho. sympho. sympho.\n",
      "Iteration 18: (Length: 20) -> hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet amoros. sympho. sympho. sympho. sympho. sympho. sympho. sympho.\n",
      "Target length of 20 reached. Stopping.\n",
      "\n",
      "--- FINAL GENERATED EPIGRAPH ---\n",
      "hic iacet amoros. sympho. ther. ev. sevir pecunia. iulia. pater. hic iacet amoros. sympho. sympho. sympho. sympho. sympho. sympho. sympho.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "Starting with: 'iulia filia'\n",
      "Forbidding generation of '<unk>' token (ID: 2).\n",
      "Iteration 1: (Length: 3) -> iulia filia cai\n",
      "Iteration 2: (Length: 4) -> iulia filia cai filia.\n",
      "Iteration 3: (Length: 5) -> iulia filia cai filia. posuit.\n",
      "Iteration 4: (Length: 6) -> iulia filia cai filia. posuit. secundus.\n",
      "Iteration 5: (Length: 7) -> iulia filia cai filia. posuit. secundus. cnaei.\n",
      "Iteration 6: (Length: 8) -> iulia filia cai filia. posuit. secundus. cnaei. suae.\n",
      "Iteration 7: (Length: 9) -> iulia filia cai filia. posuit. secundus. cnaei. suae. vn.\n",
      "Model generated an empty or punctuation-only chunk. Stopping.\n",
      "\n",
      "--- FINAL GENERATED EPIGRAPH ---\n",
      "iulia filia cai filia. posuit. secundus. cnaei. suae. vn.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--- Example 4 ---\n",
      "Starting with: 'imperatori caesari'\n",
      "Forbidding generation of '<unk>' token (ID: 2).\n",
      "Iteration 1: (Length: 3) -> imperatori caesari augusti.\n",
      "Iteration 2: (Length: 4) -> imperatori caesari augusti. p\n",
      "Iteration 3: (Length: 5) -> imperatori caesari augusti. p pecunia\n",
      "Iteration 4: (Length: 6) -> imperatori caesari augusti. p pecunia liberta.\n",
      "Iteration 5: (Length: 7) -> imperatori caesari augusti. p pecunia liberta. g.\n",
      "Iteration 6: (Length: 8) -> imperatori caesari augusti. p pecunia liberta. g. xx.\n",
      "Iteration 7: (Length: 9) -> imperatori caesari augusti. p pecunia liberta. g. xx. f.\n",
      "Iteration 8: (Length: 10) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i.\n",
      "Iteration 9: (Length: 11) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c.\n",
      "Iteration 10: (Length: 12) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x.\n",
      "Iteration 11: (Length: 13) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x. c.\n",
      "Iteration 12: (Length: 14) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x. c. xx.\n",
      "Iteration 13: (Length: 15) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x. c. xx. f.\n",
      "Iteration 14: (Length: 16) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x. c. xx. f. auli.\n",
      "Iteration 15: (Length: 17) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x. c. xx. f. auli. v.\n",
      "Iteration 16: (Length: 18) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x. c. xx. f. auli. v. viii.\n",
      "Iteration 17: (Length: 19) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x. c. xx. f. auli. v. viii. x.\n",
      "Iteration 18: (Length: 20) -> imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x. c. xx. f. auli. v. viii. x. c.\n",
      "Target length of 20 reached. Stopping.\n",
      "\n",
      "--- FINAL GENERATED EPIGRAPH ---\n",
      "imperatori caesari augusti. p pecunia liberta. g. xx. f. i. c. x. c. xx. f. auli. v. viii. x. c.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--- Example 5 ---\n",
      "Starting with: 'in hoc tumulo'\n",
      "Forbidding generation of '<unk>' token (ID: 2).\n",
      "Iteration 1: (Length: 4) -> in hoc tumulo claudio.\n",
      "Iteration 2: (Length: 5) -> in hoc tumulo claudio. q\n",
      "Iteration 3: (Length: 6) -> in hoc tumulo claudio. q vixit.\n",
      "Iteration 4: (Length: 7) -> in hoc tumulo claudio. q vixit. secunda.\n",
      "Iteration 5: (Length: 8) -> in hoc tumulo claudio. q vixit. secunda. iussit.\n",
      "Iteration 6: (Length: 9) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic\n",
      "Iteration 7: (Length: 10) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita.\n",
      "Iteration 8: (Length: 11) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa.\n",
      "Iteration 9: (Length: 12) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv\n",
      "Iteration 10: (Length: 13) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv pctv\n",
      "Iteration 11: (Length: 14) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv pctv psv.\n",
      "Iteration 12: (Length: 15) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv pctv psv. pater.\n",
      "Iteration 13: (Length: 16) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv pctv psv. pater. xv.\n",
      "Iteration 14: (Length: 17) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv pctv psv. pater. xv. flv.\n",
      "Iteration 15: (Length: 18) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv pctv psv. pater. xv. flv. qcl.\n",
      "Iteration 16: (Length: 19) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv pctv psv. pater. xv. flv. qcl. c.\n",
      "Iteration 17: (Length: 20) -> in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv pctv psv. pater. xv. flv. qcl. c. i\n",
      "Target length of 20 reached. Stopping.\n",
      "\n",
      "--- FINAL GENERATED EPIGRAPH ---\n",
      "in hoc tumulo claudio. q vixit. secunda. iussit. hic sita. ossa. psv pctv psv. pater. xv. flv. qcl. c. i\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--- Example 6 ---\n",
      "Starting with: 'valeria marci et liberta'\n",
      "Forbidding generation of '<unk>' token (ID: 2).\n",
      "Iteration 1: (Length: 5) -> valeria marci et liberta liberta.\n",
      "Iteration 2: (Length: 6) -> valeria marci et liberta liberta. cnaeus.\n",
      "Iteration 3: (Length: 7) -> valeria marci et liberta liberta. cnaeus. severus.\n",
      "Iteration 4: (Length: 8) -> valeria marci et liberta liberta. cnaeus. severus. lxxi\n",
      "Iteration 5: (Length: 9) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t\n",
      "Iteration 6: (Length: 10) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi.\n",
      "Iteration 7: (Length: 11) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv.\n",
      "Iteration 8: (Length: 12) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i.\n",
      "Iteration 9: (Length: 13) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i. cv.\n",
      "Iteration 10: (Length: 14) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i. cv. lxi\n",
      "Iteration 11: (Length: 15) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i. cv. lxi i\n",
      "Iteration 12: (Length: 16) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i. cv. lxi i fvii.\n",
      "Iteration 13: (Length: 17) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i. cv. lxi i fvii. xxi.\n",
      "Iteration 14: (Length: 18) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i. cv. lxi i fvii. xxi. cd.\n",
      "Iteration 15: (Length: 19) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i. cv. lxi i fvii. xxi. cd. vc.\n",
      "Iteration 16: (Length: 20) -> valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i. cv. lxi i fvii. xxi. cd. vc. cnaeus.\n",
      "Target length of 20 reached. Stopping.\n",
      "\n",
      "--- FINAL GENERATED EPIGRAPH ---\n",
      "valeria marci et liberta liberta. cnaeus. severus. lxxi t fvi. iv. i. cv. lxi i fvii. xxi. cd. vc. cnaeus.\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TARGET_TOKEN_LENGTH = 20\n",
    "MAX_ITERATIONS = 25\n",
    "\n",
    "\n",
    "try:\n",
    "    if 't5_model_ft' not in locals() or 't5_tokenizer_ft' not in locals():\n",
    "        print(\"Loading fine-tuned T5 model for long-form generation...\")\n",
    "        T5_OUTPUT_DIR = \"t5_epigraph_generator/\" \n",
    "        t5_tokenizer_ft = T5Tokenizer.from_pretrained(T5_OUTPUT_DIR)\n",
    "        t5_model_ft = T5ForConditionalGeneration.from_pretrained(T5_OUTPUT_DIR)\n",
    "        t5_model_ft.to(device)\n",
    "        t5_model_ft.eval()\n",
    "        print(\"Model loaded successfully.\")\n",
    "except NameError:\n",
    "    print(\"Error: T5_OUTPUT_DIR is not defined. Please run the T5 training/loading cell first.\")\n",
    "except OSError:\n",
    "    print(f\"Error: Could not load model from {T5_OUTPUT_DIR}. Please ensure the model was trained and saved.\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_long_epigraph(start_prompt, target_length, model, tokenizer):\n",
    "    current_text = start_prompt.strip()\n",
    "    \n",
    "    unk_token_id = tokenizer.unk_token_id\n",
    "    bad_words_ids = [[unk_token_id]] if unk_token_id is not None else None\n",
    "    \n",
    "    if bad_words_ids:\n",
    "        print(f\"Forbidding generation of '<unk>' token (ID: {unk_token_id}).\")\n",
    "\n",
    "    for i in range(MAX_ITERATIONS):\n",
    "        current_token_count = len(current_text.split())\n",
    "        if current_token_count >= target_length:\n",
    "            print(f\"Target length of {target_length} reached. Stopping.\")\n",
    "            break\n",
    "            \n",
    "        input_prompt = f\"infill: {current_text} <extra_id_0>\"\n",
    "        input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=True,            \n",
    "            top_k=50,                   \n",
    "            top_p=0.95,                 \n",
    "            repetition_penalty=1.2,     \n",
    "            early_stopping=True,\n",
    "            bad_words_ids=bad_words_ids \n",
    "        )\n",
    "        \n",
    "        decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "        match = re.search(r\"<extra_id_0>(.*?)<extra_id_1>\", decoded_output)\n",
    "        \n",
    "        if match:\n",
    "            new_chunk = match.group(1).strip()\n",
    "            if not new_chunk or new_chunk in [\".\", \",\"]:\n",
    "                print(\"Model generated an empty or punctuation-only chunk. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            new_words = new_chunk.split()\n",
    "            current_text += \" \" + \" \".join(new_words[:2])\n",
    "        else:\n",
    "            print(\"Could not parse model output. Stopping.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Iteration {i+1}: (Length: {len(current_text.split())}) -> {current_text}\")\n",
    "\n",
    "    return \" \".join(current_text.split())\n",
    "\n",
    "\n",
    "if 't5_model_ft' in locals():\n",
    "    print(f\"\\n--- Generating {TARGET_TOKEN_LENGTH}-token Epigraphs (with anti-looping) ---\")\n",
    "    \n",
    "    start_prompts = [\n",
    "        \"dis manibus\",\n",
    "        \"hic iacet\",\n",
    "        \"iulia filia\",\n",
    "        \"imperatori caesari\",\n",
    "        \"in hoc tumulo\",\n",
    "        \"valeria marci et liberta\"\n",
    "    ]\n",
    "    \n",
    "    for i, prompt in enumerate(start_prompts):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(f\"Starting with: '{prompt}'\")\n",
    "        \n",
    "        final_epigraph = generate_long_epigraph(\n",
    "            start_prompt=prompt,\n",
    "            target_length=TARGET_TOKEN_LENGTH,\n",
    "            model=t5_model_ft,\n",
    "            tokenizer=t5_tokenizer_ft\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- FINAL GENERATED EPIGRAPH ---\")\n",
    "        print(final_epigraph)\n",
    "        print(\"--------------------------------\\n\")\n",
    "else:\n",
    "    print(\"\\nSkipping long-form generation because the T5 model is not loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
